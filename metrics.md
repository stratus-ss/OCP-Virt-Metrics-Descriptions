| Name | Description & Example | Example Query | Severity |
|------|----------------------|--------------|----------|
| `active_argocd_instances_total` | Shows the number of Argo CD instances currently managed across the cluster. Unexpected changes may indicate issues with Argo CD operator. For example, if this drops to 0 when you expect active instances, it indicates Argo CD instances are not being properly tracked. | `active_argocd_instances_total == 0` | High |
| `aggregator_unavailable_apiservice` | Number of APIService endpoints that are unavailable. Unavailable APIs can block VM management operations or integration with external platforms. | `aggregator_unavailable_apiservice > 0` | High |
| `alertmanager_cluster_health_score` | Health score of the Alertmanager cluster. Lower values are better, and zero means "totally healthy". Higher values indicate degraded Alertmanager functionality, which could lead to missing or delayed alert notifications. If the score rises above 1, it may indicate network issues between Alertmanager instances or other cluster health problems. | `alertmanager_cluster_health_score > 1` | High |
| `alertmanager_cluster_members` | Number of Alertmanager members in the cluster. Should match expected size. In OpenShift a size of 2 is expected. Lower values indicate degraded alerting HA, risking loss of VM/infrastructure alert delivery. | `alertmanager_cluster_members < 2` | Medium |
| `alertmanager_cluster_refresh_join_failed_total` | Total number of failed attempts to join the Alertmanager cluster. High values indicate cluster instability or misconfiguration, risking alerting HA. | `increase(alertmanager_cluster_refresh_join_failed_total[15m]) > 0` | High |
| `alertmanager_dispatcher_alert_processing_duration_seconds_count` | Number of alert events processed by the dispatcher per second. Used to monitor alert processing throughput; a sudden increase indicates an increasing number of problems observed in the platform | `rate(alertmanager_dispatcher_alert_processing_duration_seconds_count[5m]) == 0` | High |
| `alertmanager_http_concurrency_limit_exceeded_total` | Total HTTP requests rejected due to concurrency limits. Indicates Alertmanager is overloaded, risking dropped or delayed alerts for VM failures. | `increase(alertmanager_http_concurrency_limit_exceeded_total[15m]) > 0` | High |
| `alertmanager_http_request_duration_seconds_count` | 	Cumulative count of HTTP requests handled by Alertmanager, as tracked by the request duration histogram. Impact: This metric is a counter that increases with every HTTP request processed by Alertmanager. A sudden drop to zero in the rate of increase indicates that Alertmanager is not receiving or processing HTTP requests, which could mean the service is down, unreachable, or stalled. | `rate(alertmanager_http_request_duration_seconds_count[5m]) == 0` | Medium |
| `alertmanager_http_requests_in_flight` | Current number of concurrent HTTP requests being processed by Alertmanager. In most clusters, this value is typically very low, reflecting the fact that Alertmanager's API is lightly used except during bursts of alert delivery or configuration reloads. | `alertmanager_http_requests_in_flight > 20` | Medium |
| `alertmanager_marked_alerts` | Number of alerts marked by state (active, suppressed, unprocessed). Abnormal values may indicate alerts not being properly processed. A high number of unprocessed alerts could indicate that Alertmanager is falling behind in processing alerts. | `sum(alertmanager_alerts{state="unprocessed"}) > 10` | Medium |
| `alertmanager_nflog_gossip_messages_propagated_total` | Cumulative count of notification log ("nflog") gossip messages propagated by this Alertmanager instance. This metric tracks the number of notification log updates (such as sent notifications and silences) that have been gossiped to peers in an Alertmanager HA cluster. | `rate(alertmanager_nflog_gossip_messages_propagated_total[5m]) > 0` | High |
| `alertmanager_nflog_query_errors_total` | Counter for the number of notification log queries that failed. Increasing values indicate problems with the notification log system, potentially causing alerts to be lost or duplicated. If this counter increases rapidly, it may indicate issues with the notification log storage or retrieval mechanism. | `rate(alertmanager_nflog_query_errors_total[5m]) > 0.1` | Medium |
| `alertmanager_nflog_query_errors_total` | Total number of notification log query errors. High values indicate issues querying alert state, risking incorrect alert deduplication or suppression. | `increase(alertmanager_nflog_query_errors_total[15m]) > 0` | High |
| `alertmanager_nflog_snapshot_duration_seconds_count` | Cumulative count of notification log (nflog) snapshot operations in Alertmanager. This metric tracks how many times Alertmanager has taken a snapshot of its notification log, which is used for persisting alert state and supporting crash recovery and HA. | `rate(alertmanager_nflog_snapshot_duration_seconds_count[5m]) == 0` | Medium |
| `alertmanager_notification_latency_seconds_bucket` | Histogram of notification delivery latency. High values indicate delays in sending alerts for VM or infra failures, risking slow incident response. | `histogram_quantile(0.95, sum(rate(alertmanager_notification_latency_seconds_bucket[5m])) by (le)) > 5` | High |
| `alertmanager_notification_requests_failed_total` | The total number of failed notification requests. Increasing values indicate that alert notifications are failing to be delivered to configured receivers. If email notifications are failing, this metric would increase with the "integration=email" label. | `rate(alertmanager_notification_requests_failed_total{integration="email"}[5m]) > 0` | High |
| `alertmanager_notifications_failed_total` | Counter showing how many notifications have failed in total. Indicates problems with notification delivery to specific integrations. For example, if Slack notifications are failing, this would increase with the "integration=slack" label. | `increase(alertmanager_notifications_failed_total{integration="slack"}[1h]) > 0` | High |
| `alertmanager_silences_query_errors_total` | Counter for errors encountered when querying the silences database. Increasing values indicate problems with silence management, potentially causing alerts to fire when they should be silenced. If this counter increases, it may indicate corruption in the silences database or other storage issues. | `increase(alertmanager_silences_query_errors_total[1h]) > 0` | Medium |
| `apiextensions_apiserver_validation_ratcheting_seconds_sum` | Cumulative sum of seconds spent by the API server on ratcheting validation for CustomResourceDefinitions (CRDs). Ratcheting validation refers to the process where the API server enforces stricter schema validation rules as CRDs evolve between versions (e.g., from v1beta1 to v1). Excessive time spent here may indicate slow schema upgrades or problematic CRD changes, which can delay or disrupt VM CRD rollouts and upgrades in OpenShift Virtualization. | `increase(apiextensions_apiserver_validation_ratcheting_seconds_sum[15m]) > 5` | Low |
| `apiserver_admission_controller_admission_duration_seconds_bucket` | Histogram of admission controller latencies. High latencies can slow down all API operations, affecting cluster performance. If the 95th percentile latency exceeds 1 second, API operations will be noticeably slower. | `histogram_quantile(0.95, sum(rate(apiserver_admission_controller_admission_duration_seconds_bucket[5m])) by (le))` | Medium |
| `apiserver_admission_webhook_rejection_count` | Alert for high rate of admission controller errors. Indicates problems with admission controllers, potentially preventing resource creation. For example, if ValidatingAdmissionWebhooks are failing, custom resources may not be created properly. | `sum(rate(apiserver_admission_webhook_rejection_count[5m])) / sum(rate(apiserver_admission_webhook_request_total[5m])) > 0.05` | High |
| `apiserver_admission_webhook_rejection_count` | Alert for high rate of admission controller errors. Indicates problems with admission controllers, potentially preventing resource creation. If ValidatingAdmissionWebhooks are failing, custom resources may not be created properly. | `sum(rate(apiserver_admission_webhook_rejection_count[5m])) / sum(rate(apiserver_admission_webhook_request_total[5m])) > 0.05` | High |
| `apiserver_admission_webhook_rejection_count` | Count of admission webhook rejections, labeled by webhook, operation, and error type.  Frequent rejections can block VM creation, migration, or updates. | `increase(apiserver_admission_webhook_rejection_count[15m]) > 0` | Medium |
| `apiserver_audit_requests_rejected_total` | Counter of rejected audit requests. Increasing values indicate audit logging issues, potentially affecting compliance requirements. If this counter increases rapidly, it could indicate audit backend problems or configuration issues. | `rate(apiserver_audit_requests_rejected_total[5m]) > 0` | Medium |
| `apiserver_authorization_decisions_total` | Total number of authorization decisions (allow/forbid). Spikes in denied requests may indicate RBAC misconfiguration, blocking VM operations. | `sum(rate(apiserver_authorization_decisions_total{decision="forbid"}[5m])) > 0` | Medium |
| `apiserver_cache_list_total` | Total cache list operations. High values may indicate heavy API usage, possibly from VM controllers or monitoring. | `rate(apiserver_cache_list_total[5m]) > 100` | Low |
| `apiserver_client_certificate_expiration_seconds_bucket` | Histogram of client certificate expiration times. Imminent certificate expiry can break API access for VM controllers/operators. If you have less than 14 days until expiry and you manage the certs yourself, you will need to take action. | `histogram_quantile(0.01, sum(rate(apiserver_client_certificate_expiration_seconds_bucket[5m])) by (le)) < 1209600` | High |
| `apiserver_crd_conversion_webhook_duration_seconds_bucket` | Histogram of response times (in seconds) for CRD conversion webhooks. High latency in conversion webhooks can delay or block CRD object operations, including those for VM custom resources in OpenShift Virtualization. This can slow down VM creation, updates, or migrations if the conversion webhook is slow or unresponsive. | `histogram_quantile(0.95, sum(rate(apiserver_crd_conversion_webhook_duration_seconds_bucket[5m])) by (le)) > 1` | Medium |
| `apiserver_current_inflight_requests` | Current number of requests being processed. High values indicate API server overload, potentially causing timeouts and degraded performance. If this exceeds the configured max-in-flight limit, new requests will be rejected. | `sum(apiserver_current_inflight_requests) by (request_kind) > 100` | High |
| `apiserver_current_inqueue_requests` | Number of API requests currently queued. High values indicate API server overload, risking delays in VM operations. | `apiserver_current_inqueue_requests > 10` | High |
| `apiserver_delegated_authn_request_duration_seconds_bucket` | Histogram of durations (in seconds) for delegated authentication requests made by the Kubernetes API server. High latency in delegated authentication can delay all API calls that require authentication—including VM lifecycle operations in OpenShift Virtualization.| `histogram_quantile(0.95, sum(rate(apiserver_delegated_authn_request_duration_seconds_bucket[5m])) by (le)) > 1` | Medium |
| `apiserver_delegated_authz_request_duration_seconds_count` | Cumulative count of delegated authorization requests handled by the Kubernetes API server. This metric is most useful when combined with its companion sum metric to calculate average authorization latency. Spikes may signal RBAC or webhook bottlenecks that could delay or block VM operations in OpenShift Virtualization. | `rate(apiserver_delegated_authz_request_duration_seconds_count[5m]) > 100` | Low |
| `apiserver_delegated_authz_request_duration_seconds_sum` | Total time spent on delegated authorization. Used to compute average latency for authorization. | `rate(apiserver_delegated_authz_request_duration_seconds_sum[5m]) / rate(apiserver_delegated_authz_request_duration_seconds_count[5m]) > 1` | Medium |
| `apiserver_flowcontrol_current_executing_requests` | Number of requests currently being executed. High values may indicate API priority and fairness (APF) issues. If a non-critical priority level is consuming too many resources, it could starve critical requests. | `sum(apiserver_flowcontrol_current_executing_requests) by (priorityLevel) > 50` | Medium |
| `apiserver_flowcontrol_current_executing_seats` | Number of seats currently occupied by executing requests. Relates to API server concurrency control. If this approaches the upper limit, it indicates the API server is at maximum capacity. | `sum(apiserver_flowcontrol_current_executing_seats) / sum(apiserver_flowcontrol_upper_limit_seats) > 0.8` | Medium |
| `apiserver_flowcontrol_current_inqueue_requests` | **Current number of API requests waiting in the APF queue (not yet executing), labeled by `flow_schema` and `priority_level`.** High values indicate API server congestion, which can delay VM operations (creation, migration, deletion) as requests wait for available concurrency slots. | `apiserver_flowcontrol_current_inqueue_requests > 10` | High |
| `apiserver_flowcontrol_current_inqueue_seats` | Number of seats occupied by requests currently waiting in queues. Represents the resource consumption of queued requests. High values indicate that queued requests are consuming significant "seat" resources, which can lead to request throttling and delays in VM operations. Each request consumes one or more seats based on its resource requirements. | `apiserver_flowcontrol_current_inqueue_seats > 10` | Medium |
| `apiserver_flowcontrol_demand_seats_average` | Time-weighted average seat demand during the last concurrency borrowing adjustment period. Part of APF's dynamic concurrency borrowing algorithm. Values represent average demand pressure on the API server. | `apiserver_flowcontrol_demand_seats_average > 5` | Medium |
| `apiserver_flowcontrol_demand_seats_high_watermark` | High watermark of seat demand. Indicates peak API pressure, useful for capacity planning. | `apiserver_flowcontrol_demand_seats_high_watermark` | Medium |
| `apiserver_flowcontrol_demand_seats_smoothed` | Smoothed value of seat demand over time. **Impact**: Tracks ongoing API pressure affecting VM operations. | `apiserver_flowcontrol_demand_seats_smoothed > 2` | Medium |
| `apiserver_flowcontrol_demand_seats_sum` | Sum of seat demand across requests. **Impact**: High values indicate API server is under heavy load, risking delays for VM lifecycle events. | `apiserver_flowcontrol_demand_seats_sum > 10` | Medium |
| `apiserver_flowcontrol_priority_level_request_utilization_count` | Indicates request concurrency utilization by priority level. High values may indicate certain priority levels are overloaded. If the "system-node" priority level is highly utilized, node-related operations may be affected. | `apiserver_flowcontrol_priority_level_request_utilization_count{priorityLevel="system-node"} / apiserver_flowcontrol_upper_limit_seats{priorityLevel="system-node"} > 0.9` | Medium |
| `apiserver_flowcontrol_priority_level_seat_utilization_count` | Indicates seat concurrency utilization by priority level. High values may indicate certain priority levels are consuming too many resources. If a workload priority level is using most available seats, it could impact other operations. | `apiserver_flowcontrol_priority_level_seat_utilization_count{priorityLevel="workload-high"} / apiserver_flowcontrol_upper_limit_seats{priorityLevel="workload-high"} > 0.9` | Medium |
| `apiserver_flowcontrol_request_dispatch_no_accommodation_total` | Total number of requests that could not be accommodated due to concurrency limits. Increasing values indicate requests being rejected due to overload. If this increases for critical priority levels, important operations may be failing. | `increase(apiserver_flowcontrol_request_dispatch_no_accommodation_total{priorityLevel="system-cluster-critical"}[5m]) > 0` | High |
| `apiserver_flowcontrol_request_wait_duration_seconds_bucket` | Histogram of request waiting times. High wait times indicate API server congestion. If the 95th percentile wait time exceeds 1 second, API operations will be noticeably delayed. | `histogram_quantile(0.95, sum(rate(apiserver_flowcontrol_request_wait_duration_seconds_bucket[5m])) by (le, priorityLevel))` | Medium |
| `apiserver_flowcontrol_upper_limit_seats` | Maximum number of seats available per priority level. This is a configuration limit that affects concurrency. This is typically a static value but is important for calculating utilization percentages. | `apiserver_flowcontrol_upper_limit_seats` | Low |
| `apiserver_kube_aggregator_x509_insecure_sha1_total` | Count of insecure SHA1 certificates used by aggregated APIs. Insecure certificates in the aggregator can compromise security for VM API extensions and custom resources. SHA1 certificates are deprecated and pose security risks for VM management APIs. | `increase(apiserver_kube_aggregator_x509_insecure_sha1_total[15m]) > 0` | High |
| `apiserver_list_watch_request_success_total:rate:sum` | Rate of successful list/watch API requests aggregated across the cluster. Impact: Critical for VM status monitoring and real-time updates. Failed list/watch operations can cause VM controllers to miss state changes, affecting migration monitoring and status updates. | `apiserver_list_watch_request_success_total:rate:sum` | Low |
| `apiserver_request_aborts_total` | Total count of API requests that were aborted during processing. Request aborts occur when the API server cannot complete request processing due to timeouts, client disconnections, or internal errors. High abort rates indicate API server instability or resource exhaustion. | `rate(apiserver_request_aborts_total[5m]) > 0.1` | High |
| `apiserver_request:burnrate1d` | API request burn rate over 1 day, measured by taking the failed API requests divided by the total number of requests. High values indicate sustained API server load. If this exceeds SLO thresholds, it indicates the API server has been overloaded for an extended period. | `apiserver_request:burnrate1d > 2` | Medium |
| `apiserver_request:burnrate1h` | Error budget burn rate for API server SLO over 1 hour.  High burn rates indicate API server reliability issues that directly affect VM operations reliability and SLA compliance for virtualization workloads. | `sum(apiserver_request:burnrate1h) > (14.4 * 0.01)` | Critical |
| `apiserver_request:burnrate30m` | API request burn rate over 30 minutes, measured by taking the failed API requests divided by the total number of requests. High values indicate medium-term API server load. If this exceeds SLO thresholds, it indicates the API server has been overloaded recently. | `apiserver_request:burnrate30m > 5` | High |
| `apiserver_request:burnrate5m` | API request burn rate over 5 minutes, measured by taking the failed API requests divided by the total number of requests. High values indicate short-term API server load spikes. If this exceeds SLO thresholds, it indicates the API server is currently overloaded. | `apiserver_request:burnrate5m > 10` | Critical |
| `apiserver_request_duration_seconds_bucket` | Histogram of API request durations in seconds. High latency affects all VM operations including creation, migration, and status updates. Slow API responses can cause VM operation timeouts and degraded user experience. | `histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket[5m])) by (le)) > 1` | High |
| `apiserver_request_post_timeout_total` | Total number of POST requests that have timed out. Increasing values indicate API server performance issues. If this increases for create operations, new resources may fail to be created. | `increase(apiserver_request_post_timeout_total{verb="create"}[5m]) > 0` | High |
| `apiserver_request_sli_duration_seconds_bucket` | Histogram for Service Level Indicator (SLI) request durations. Used for SLO tracking of API performance. Poor SLI metrics indicate degraded service quality affecting VM operation reliability. The threshold in the query is just an example!| `histogram_quantile(0.99, sum(rate(apiserver_request_sli_duration_seconds_bucket[5m])) by (le)) > 0.5` | High |
| `apiserver_request_sli_duration_seconds_count` | Count of requests used for SLI (Service Level Indicator) calculations. Used for SLO monitoring. This is primarily used for calculating error rates and latencies against SLOs. | `sum(rate(apiserver_request_sli_duration_seconds_count{code=~"5.."}[5m])) / sum(rate(apiserver_request_sli_duration_seconds_count[5m]))` | Medium |
| `apiserver_request_slo_duration_seconds_bucket` | Histogram for Service Level Objective (SLO) request durations. Tracks API performance against defined SLOs. SLO violations indicate service degradation that affects VM operation guarantees. The threshold in the query is just an example!| `histogram_quantile(0.99, sum(rate(apiserver_request_slo_duration_seconds_bucket[5m])) by (le)) > 0.6` | High |
| `apiserver_request_slo_duration_seconds_count` | Count of requests used for SLO (Service Level Objective) calculations. Used for SLO monitoring. Similar to SLI metric, used for calculating compliance with SLOs. | `sum(rate(apiserver_request_slo_duration_seconds_count{code=~"5.."}[5m])) / sum(rate(apiserver_request_slo_duration_seconds_count[5m]))` | Medium |
| `apiserver_request_timestamp_comparison_time_bucket` |Histogram of time spent by the Kubernetes API server comparing timestamps of incoming requests. Timestamp comparison is a step in request processing to ensure correct ordering and consistency, especially important for watch requests and status updates. High overhead in timestamp comparison can add latency to API server responses. | `histogram_quantile(0.95, sum(rate(apiserver_request_timestamp_comparison_time_bucket[5m])) by (le)) > 0.01` | Low |
| `apiserver_request_total` | Total count of API requests processed. High request volumes during VM operations (creation, migration, scaling) help identify capacity constraints and usage patterns. This is also useful to identify abnormal behaviour over time. | `rate(apiserver_request_total[5m])` | Medium |
| `apiserver_selfrequest_total` | Total count of API server requests made by the API server itself (internal self-requests), not by external clients. These are requests generated internally, for example, when the API server needs to fetch or update its own configuration, or when performing background synchronization or cleanup tasks. High rates of self-requests may indicate increased internal activity, which competes with user-initiated requests (like VM creation, migration, or status updates) for API server resources. While usually low, persistent high rates could signal internal issues or misconfiguration. | `rate(apiserver_selfrequest_total[5m]) > 10` | Low |
| `apiserver_storage_data_key_generation_failures_total` | Total number of failed data encryption key generation operations. Increasing values indicate issues with etcd encryption. If this increases, it could indicate problems with the encryption provider or configuration. | `increase(apiserver_storage_data_key_generation_failures_total[1h]) > 0` | Critical |
| `apiserver_storage_db_total_size_in_bytes` | Total size of the etcd database in bytes. As VM count grows, etcd size increases. Approaching the 8GB default limit will make the cluster read-only, preventing all VM operations (creation, migration, deletion). OpenShift > 4.16 has a default size of 8GB.| `apiserver_storage_db_total_size_in_bytes > 7.1e9` | Critical |
| `apiserver_storage_decode_errors_total` | Total count of storage decode errors when retrieving objects from etcd. Decode errors can prevent VM resource retrieval, causing VM status inconsistencies, failed operations, or inability to access VM configuration data. | `increase(apiserver_storage_decode_errors_total[15m]) > 0` | High |
| `apiserver_storage_events_received_total` | Total count of storage events received by the API server from the ETCD. This metric reflects the rate at which the API server is receiving state change notifications from the cluster’s data store. High event rates may indicate heavy cluster activity or frequent configuration updates, which can put pressure on the storage backend. Conversely, a sudden drop or sustained zero rate may signal connectivity problems, storage backend failures, or API server disconnection from the data store. | `rate(apiserver_storage_events_received_total[5m])` | Medium |
| `apiserver_storage_list_total` | Total count of list operations performed against storage. This may be a useful metric to use during intrusion detection. If an entity has gained access to the API, they may issue a lot of lists. | `rate(apiserver_storage_list_total[5m])` | Low |
| `apiserver_storage_size_bytes` | Size of the storage database file physically allocated in bytes. High values may indicate etcd database growth issues. If this approaches the etcd size limit (typically 2GB), it could lead to API server instability. | `apiserver_storage_size_bytes > 7*1024*1024*1024` | High |
| `apiserver_tls_handshake_errors_total` | Total count of TLS handshake errors for API server connections. TLS errors can prevent VM controllers, operators, and management tools from accessing the API, disrupting VM lifecycle operations and monitoring. | `increase(apiserver_tls_handshake_errors_total[15m]) > 0` | High |
| `apiserver_watch_cache_read_wait_seconds_bucket` | Histogram of time spent waiting to read from watch cache. This cache stores recent object states to serve watch requests efficiently. | `histogram_quantile(0.95, sum(rate(apiserver_watch_cache_read_wait_seconds_bucket[5m])) by (le)) > 0.1` | Medium |
| `apiserver_webhooks_x509_insecure_sha1_total` | Total count of times the API server detected insecure SHA1 certificates being used by admission webhooks. Impact: SHA1 certificates are considered cryptographically weak and are deprecated due to security vulnerabilities. When the API server encounters a webhook secured with such a certificate, it logs this event. This poses a security risk to the admission control process, which is critical for VM creation and updates. If a webhook certificate is insecure, it may fail validation, block VM lifecycle operations, or expose the cluster to man-in-the-middle attacks. | `increase(apiserver_webhooks_x509_insecure_sha1_total[15m]) > 0` | High |
| `argocd_app_reconcile_bucket` | Histogram of ArgoCD application reconciliation durations. High values indicate ArgoCD performance issues affecting automated deployment of kubernetes resources. | `histogram_quantile(0.95, sum(rate(argocd_app_reconcile_bucket[5m])) by (le)) > 30` | Medium |
| `argocd_git_request_duration_seconds_bucket` | Histogram of how long it takes ArgoCD to complete requests to Git repositories (in seconds). This metric tracks the time ArgoCD’s Repo Server spends fetching or interacting with Git repos to retrieve application manifests. If these Git operations are slow, it will delay the syncing and deployment of VM configurations or updates in a GitOps workflow. High values mean that ArgoCD is waiting on Git. | `histogram_quantile(0.95, sum(rate(argocd_git_request_duration_seconds_bucket[5m])) by (le)) > 5` | Medium |
| `argocd_git_request_duration_seconds_sum` | Total time spent on ArgoCD Git requests. Used with count to calculate average Git request latency. High values indicate Git performance issues affecting VM GitOps deployment speed. | `rate(argocd_git_request_duration_seconds_sum[5m]) / rate(argocd_git_request_duration_seconds_count[5m]) > 2` | Medium |
| `argocd_redis_request_duration_bucket` | Histogram of how long (in seconds) Redis requests take in ArgoCD. ArgoCD uses Redis as a cache and for internal state management (such as application status and cluster cache). If Redis requests are slow, it can delay application reconciliation, syncing, and status updates—including those for VM deployments managed through GitOps. | `histogram_quantile(0.95, sum(rate(argocd_redis_request_duration_bucket[5m])) by (le)) > 0.1` | Low |
| `authenticated_user_requests` | Count of authenticated requests. Unexpected changes may indicate authentication issues. A sudden drop could indicate authentication system problems. | `rate(authenticated_user_requests[5m])` | Medium |
| `authentication_attempts` | Count of authentication attempts. High failure rates indicate authentication configuration issues or potential security incidents. A high rate of failed attempts could indicate a brute force attack. | `sum(rate(authentication_attempts{result="error"}[5m])) / sum(rate(authentication_attempts[5m])) > 0.5` | High |
| `authentication_duration_seconds_bucket` | Histogram of time spent authenticating API requests (in seconds). All VM operations require authentication. Slow authentication delays VM creation, migration, deletion, and status updates, directly affecting user experience and automation workflows. | `histogram_quantile(0.95, sum(rate(authentication_duration_seconds_bucket[5m])) by (le)) > 1` | Medium |
| `authentication_duration_seconds_sum` | Total time spent on authentication across all requests. When combined with request count, shows average authentication latency. High values indicate authentication bottlenecks that slow down all VM management operations. | `rate(authentication_duration_seconds_sum[5m]) / rate(authentication_duration_seconds_count[5m]) > 0.5` | Medium |
| `cardinality_enforcement_unexpected_categorizations_total` | Cumulative count of metric series that could not be properly categorized for cardinality enforcement. High cardinality means a metric has too many unique label combinations (e.g., due to unique IDs, session tokens, or timestamps as labels). This can overwhelm monitoring systems, causing high memory usage, slow queries, or even data loss. When metrics cannot be categorized, it means the system cannot apply its cardinality controls, increasing the risk that runaway label combinations will degrade monitoring performance.  | `increase(cardinality_enforcement_unexpected_categorizations_total[15m]) > 0` | Low |
| `catalogsource_ready` | Indicates whether Operator Lifecycle Manager catalog sources are ready. Catalog sources provide operators and updates. If not ready, you cannot install, update, or manage OpenShift Virtualization operators, blocking platform maintenance and feature updates. | `catalogsource_ready{name="redhat-operators"} == 0` | Medium |
| `cco_controller_reconcile_seconds_bucket` | Histogram of Cloud Credential Operator reconciliation times (in seconds). CCO manages cloud credentials needed for storage and networking. Slow reconciliation can delay VM operations that require cloud resources like persistent volumes or load balancers. | `histogram_quantile(0.95, sum(rate(cco_controller_reconcile_seconds_bucket[5m])) by (le)) > 10` | Medium |
| `cco_credentials_requests_conditions` | Tracks the status conditions of Cloud Credential Operator credential requests. Indicates issues with cloud provider credentials that could affect cluster functionality. For example, if this metric shows credential requests in a degraded state, cloud resources may be inaccessible. | `cco_credentials_requests_conditions{condition="Degraded", status="True"} > 0` | High |
| `cco_credentials_requests` | Number of credential requests managed by Cloud Credential Operator. High numbers may indicate credential management issues. Failed credential requests can prevent VMs from accessing cloud storage or networking resources. | `increase(cco_credentials_requests[15m]) > 0` | High |
| `certwatcher_read_certificate_errors_total` | Total certificate reading errors from the certificate watcher. Certificate errors can break TLS communications between VM components, operators, and APIs, causing authentication failures and disrupting VM management operations. | `increase(certwatcher_read_certificate_errors_total[15m]) > 0` | High |
| `cluster:apiserver_current_inflight_requests:sum:max_over_time:2m` | Maximum number of concurrent requests to the API server over a 2-minute window. High values indicate API server congestion. For example, if this exceeds 100 for mutating requests, the API server may be overloaded, causing slow responses or failures. | `cluster:apiserver_current_inflight_requests:sum:max_over_time:2m{request_kind="mutating"} > 100` | High |
| `cluster:capacity_memory_bytes:sum` | Total **allocatable** memory across all nodes (not raw hardware capacity). This metric is critical for VM scheduling, as OpenShift Virtualization uses this value minus existing commitments to determine if new VMs can be scheduled. It does not account for memory overcommit configurations, so even if this metric shows 512GB but your VMs have 600GB in memory requests (with overcommit), new VMs may still schedule despite appearing to exceed "capacity." | `cluster:memory_usage_bytes:sum / cluster:capacity_memory_bytes:sum > 0.85` | Medium |
| `cluster:console_auth_login_failures_total:sum` | Total failed login attempts to the OpenShift web console. High failure rates may indicate security issues or user experience problems. Failed logins prevent users from accessing the VM management interface through the web console. | `increase(cluster:console_auth_login_failures_total:sum[15m]) > 10` | Medium |
| `cluster:console_auth_login_successes_total:sum` | Total successful login attempts to the OpenShift web console. Tracks user access to the platform. Sudden drops may indicate authentication system issues preventing users from managing VMs through the web interface. | `rate(cluster:console_auth_login_successes_total:sum[5m]) == 0` | Low |
| `cluster:container_cpu_usage:ratio` | Ratio of actual CPU usage to CPU requests across all containers in the cluster. This metric shows how much CPU your containers (including VM pods) are actually using compared to what they requested from Kubernetes. If the ratio is high (close to or above 1), it means containers are using as much or more CPU than they asked for. A consistently high ratio suggests you may be overcommitting CPU resources or need to adjust your requests and limits for better performance and reliability. | `cluster:container_cpu_usage:ratio > 0.8` | Medium |
| `cluster:control_plane:all_nodes_ready` | Indicates whether all control plane nodes are in a ready state (1 for yes, 0 for no). Non-ready control plane nodes affect cluster stability. For example, if this metric is 0, at least one control plane node is not ready, which could affect high availability. | `cluster:control_plane:all_nodes_ready == 0` | Critical |
| `cluster:cpu_usage_cores:sum` | Total CPU cores currently used cluster-wide. Shows how much CPU is being consumed by all workloads, including VMs. High usage can lead to VM throttling or inability to schedule new VMs. | `sum(cluster:cpu_usage_cores:sum) / sum(cluster:capacity_cpu_cores:sum) > 0.8` | High |
| `cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum` | Total storage requested by PVCs per storage provisioner. Ensures enough storage is available for VM disks. High values close to provisioner limits can block VM provisioning or expansion. | `cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum > $(provisioner_capacity)` | High |
| `cluster:master_nodes` | Count of master/control plane nodes in the cluster. Impact: Unexpected changes may indicate node failures. For example, if this drops below the expected number (typically 3), cluster control plane redundancy is compromised. | `cluster:master_nodes == 0` | Medium |
| `cluster_master_schedulable` | Indicates if master nodes are schedulable for workloads. Masters should not run VMs; if true, it risks control plane stability and resource contention. | `cluster_master_schedulable == 1` | Medium |
| `cluster:memory_usage_bytes:sum` | Total memory **usage** (working set) across all nodes. High values relative to *allocatable* memory indicate potential contention, but OpenShift Virtualization's memory overcommit allows scheduling beyond 100% of allocatable memory. This metric is critical for detecting swap/thrashing scenarios that degrade VM performance. For example, if this reaches 90% of allocatable memory and swap usage is increasing, VMs may experience severe latency due to host memory pressure, even if free memory exists in the overcommit pool.| `cluster:memory_usage_bytes:sum / cluster:capacity_memory_bytes:sum > 0.85` | Medium |
| `cluster:memory_usage:ratio` | Ratio of memory currently used to total physical memory available in the cluster. This metric shows how much of your cluster’s total RAM is actively in use by all workloads, including VMs. A high value (for example, >0.8 or 80%) means you are close to exhausting your cluster’s memory. | `cluster:memory_usage:ratio > 0.8` | High |
| `cluster_monitoring_operator_reconcile_attempts_total` | Number of times the cluster monitoring operator has attempted reconciliation. Frequent attempts may indicate monitoring instability, risking gaps in VM observability and alerting. | `rate(cluster_monitoring_operator_reconcile_attempts_total[5m]) > 5` | Medium |
| `cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits` | Total CPU limits set for pods in each namespace. Ensures VM pods (virt-launcher) have defined CPU limits, preventing noisy neighbor issues and resource contention. | `sum by (namespace)(cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits)` | Medium |
| `cluster:namespace:pod_memory:active:kube_pod_container_resource_limits` | Total memory limits set for pods in each namespace. Ensures VM pods have defined memory limits, reducing risk of OOM kills and ensuring fair resource allocation. | `sum by (namespace)(cluster:namespace:pod_memory:active:kube_pod_container_resource_limits)` | Medium |
| `cluster:network_attachment_definition_enabled_instance_up:max` | Maximum number of enabled network attachment definition instances that are up. Impact: Indicates health of multus CNI attachments. If this is 0 when using secondary networks, pods requiring additional networks may fail to start. | `cluster:network_attachment_definition_enabled_instance_up:max == 0` | High |
| `cluster:network_attachment_definition_enabled_instance_up:max` | Maximum number of enabled network attachment definition instances that are up. Indicates health of multus CNI attachments. For example, if this is 0 when using secondary networks, pods requiring additional networks may fail to start. | `cluster:network_attachment_definition_enabled_instance_up:max == 0` | High |
| `cluster:node_cpu:sum_rate5m` | Total rate of CPU usage (in cores) summed across all nodes in the cluster, averaged over 5 minutes.  This metric tells you how much CPU is actually being used by all workloads—including VMs, pods, and system processes—across your entire cluster. If this value, divided by your total available CPU cores, is high (for example, above 0.9 or 90%), it means your cluster is nearly fully loaded. | `sum(cluster:node_cpu:sum_rate5m) /sum(cluster:capacity_cpu_cores:sum) > 0.8` | High |
| `cluster_operator_conditions` | Represents the health conditions of core cluster operators. Degraded operators can affect overall cluster functionality. For example, if the OpenShift Virtualization operator shows a False Ready condition, virtualization features may be unavailable. | `cluster_operator_conditions{name="kubevirt-hyperconverged", condition="Ready", status="False"} == 1` | Critical |
| `cluster_operator_payload_errors` | Counts errors encountered during cluster operator payload processing. Increasing values indicate problems with operator updates. For example, if this increases during an upgrade, it may indicate problems applying new operator versions. | `sum(rate(cluster_operator_payload_errors[15m])) > 0` | High |
| `cluster_operator_up` | Indicates if cluster operators are running. Operators (e.g., KubeVirt, CNO) manage VM lifecycle, storage, and networking. If any are down, VM management may be disrupted. | `cluster_operator_up == 0` | Critical |
| `cluster:ovnkube_controller_admin_network_policies_db_objects:max` | Maximum number of admin network policy objects in OVN-Kubernetes. Large numbers may slow down network policy processing, affecting VM network security and connectivity. | `cluster:ovnkube_controller_admin_network_policies_db_objects:max > 10000` | Medium |
| `cluster:ovnkube_controller_admin_network_policies_rules:max` | Maximum number of admin network policy rules in OVN-Kubernetes. Too many rules can increase latency for VM network operations and policy enforcement. | `cluster:ovnkube_controller_admin_network_policies_rules:max > 1000` | Medium |
| `cluster:ovnkube_controller_egress_routing_via_host:max` | Maximum number of egress routes via host in OVN-Kubernetes. High values may indicate suboptimal VM network paths, increasing latency and reducing performance. | `cluster:ovnkube_controller_egress_routing_via_host:max > 100` | Low |
| `cluster_quantile:apiserver_request_duration_seconds:histogram_quantile` | Shows the API server request latency at a given percentile (e.g., 99th percentile). High values mean some API requests (including VM create, update, or delete) are slow, which can delay or disrupt VM operations. | `cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{quantile="0.99"}` | High |
| `cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile` | Shows how long the Kubernetes scheduler takes to make scheduling decisions at a given percentile. High values mean it takes longer to start or migrate VMs, which can slow down scaling or recovery from failures. | `cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile{quantile="0.95"} > 1` | Medium |
| `cluster:route_metrics_controller_routes_per_shard:max` | Maximum number of routes managed by any ingress shard. Too many routes on a single shard can cause networking bottlenecks, affecting how VMs are accessed externally. | `cluster:route_metrics_controller_routes_per_shard:max > 1000` | Low |
| `cluster:telemetry_selected_series:count` | Total number of active metrics time series being collected for monitoring. Too many series can overload monitoring systems, risking loss of VM and cluster observability. | `cluster:telemetry_selected_series:count > 1000000` | Low |
| `cluster:usage:ingress_frontend_bytes_in:rate5m:sum` | Total incoming network traffic (bytes) to all ingress frontends, per 5 minutes. High values may indicate heavy application or VM usage, or possible network saturation. | `cluster:usage:ingress_frontend_bytes_in:rate5m:sum > network_capacity` | Low |
| `cluster:usage:ingress_frontend_bytes_out:rate5m:sum` | Total outgoing network traffic (bytes) from all ingress frontends, per 5 minutes. High values could signal heavy VM or application traffic, which may lead to network congestion. | `cluster:usage:ingress_frontend_bytes_out:rate5m:sum > network_capacity` | Low |
| `cluster:usage:ingress_frontend_connections:sum` | Total number of active connections to all ingress frontends. High connection counts may indicate high demand or potential overload, affecting VM and application accessibility. | `cluster:usage:ingress_frontend_connections:sum > $(connection_limit)` | Low |
| `cluster:usage:openshift:ingress_request_error:fraction5m` | Fraction of ingress requests that resulted in errors over 5 minutes. High error rates mean users may not be able to access VMs or services reliably. | `cluster:usage:openshift:ingress_request_error:fraction5m > 0.05` | High |
| `cluster:usage:openshift:kube_running_pod_ready:avg` | Average fraction of running pods that are in the Ready state across the cluster.  This metric shows the overall health of your workloads, including VMs in OpenShift Virtualization. The value ranges from 0 (no pods are ready) to 1 (all running pods are ready). A value close to 1 means almost all pods are healthy and serving traffic. | `cluster:usage:openshift:kube_running_pod_ready:avg < 0.95` | High |
| `cluster:usage:resources:sum` | Aggregated resource usage across the cluster. Helps identify resource consumption trends. For example, if CPU usage is consistently high, it may indicate the need for cluster scaling. | `cluster:usage:resources:sum{resource="virtualmachineinstances.kubevirt.io"}` | Medium |
| `cluster_version_capability` | Shows which features are enabled (1) or disabled (2) in your OpenShift cluster. This tells you what capabilities your cluster supports, such as whether virtualization features are enabled. | `cluster_version_capability{capability="MachineAPI"}` | Low |
| `cluster_version_payload` | Shows the current version of OpenShift running on your cluster and tracks updates including the applied or pending versions in the cluster. Mostly used during or after an upgrade to ensure all components are at the same version. | `cluster_version_payload{version!="4.16.0"}` | Medium |
| `cluster:virt_platform_nodes:sum` | Total number of nodes that can run virtual machines in your cluster as well as make and model of the servers (if available). | `cluster:virt_platform_nodes:sum` | High |
| `cnv_abnormal` | The metric tracks two specific memory conditions:<ul><li>`memory_working_set_delta_from_request`: The difference between the working set memory and the requested memory</ul></li> <ul><li>`memory_rss_delta_from_request`: The difference between the resident set size (RSS) memory and the requested memory </ul></li>Impact: Abnormal conditions may affect VM functionality. Positive values indicate the component is using more memory than requested, while negative values indicate it's using less than requested.<br><br> Large positive values could indicate memory pressure in your virtualization components. Consistently high values might suggest you need to adjust the resource requests for your virtualization components| `sum by (container) (cnv_abnormal{reason="memory_working_set_delta_from_request"})` | High |
| `cnv:vmi_status_running:count` | Count of running Virtual Machine Instances. Unexpected changes may indicate VM issues. For example, if this drops suddenly, VMs may be failing or being terminated unexpectedly. | `sum by(node) (cnv:vmi_status_running:count)`<br><br>`sum by (guest_os_name)(cnv:vmi_status_running:count)` | Medium |
| `code_handler_job_namespace:lokistack_gateway_http_requests:irate1m` | Rate of HTTP requests to the logging system gateway per minute, by response code. This tracks access to your cluster's logging system. If VM logs aren't being collected properly due to gateway errors, you'll lose visibility into VM performance and troubleshooting information. | `sum(rate(code_handler_job_namespace:lokistack_gateway_http_requests:irate1m{code=~"5.."}[5m])) > 0` | Medium |
| `component_resource:apiserver_request_terminations_total:rate:1m` | Rate of API server request terminations over 1 minute. High values indicate API server overload. If this exceeds normal baseline, the API server is terminating requests due to overload. | `component_resource:apiserver_request_terminations_total:rate:1m > 10` | High |
| `component_resource:apiserver_request_terminations_total:rate:5m` | Rate of API server request terminations over 5 minutes. High values indicate sustained API server overload. If this exceeds normal baseline, the API server is consistently terminating requests. | `component_resource:apiserver_request_terminations_total:rate:5m > 5` | High |
| `console_auth_login_failures_total` | Total number of failed attempts to log into the OpenShift web console. High failure rates could indicate authentication problems that prevent users from accessing VM management tools, or potential security issues. | `increase(console_auth_login_failures_total[15m]) > 10` | Medium |
| `console_auth_login_requests_total` | Total number of login attempts to the OpenShift web console. Shows how much the web console (including VM management interfaces) is being used. Sudden drops might indicate the console is unavailable, preventing users from managing VMs through the web interface. | `rate(console_auth_login_requests_total[5m])` | Low |
| `console_auth_token_refresh_requests_total` | Total number of requests to refresh authentication tokens for the web console. Users need valid tokens to stay logged into the VM management interface. | `increase(console_auth_token_refresh_requests_total[15m]) > 10` | Low |
| `console_usage_total` | Total number of times the OpenShift web console has been used. This metric counts how often users access and interact with the OpenShift web console, including the VM management pages. | `console_usage_total` | Low |
| `container_blkio_device_usage_total` | Total disk I/O operations performed by containers (including VM containers). VMs on OpenShift run inside containers, and this shows how much disk activity they're generating. High values might indicate VMs doing heavy disk work, which could affect performance or indicate storage bottlenecks. To better understand device numbers see the [kernal documentation](https://www.kernel.org/doc/Documentation/admin-guide/devices.txt). | `rate(container_blkio_device_usage_total[5m]) > $(storage_iops_limit)` | Medium |
| `container_cpu_cfs_periods_total` | Total number of CPU scheduling periods that have elapsed for a container. This metric counts how many times the Linux CPU scheduler has checked whether a container (including VMs) can use CPU, based on its CPU limits. Each "period" is typically 100 milliseconds. By itself, this metric tells you how often the system has evaluated CPU usage for the container, but it is most useful when combined with throttling metrics to understand if your containers or VMs are being limited by CPU restrictions. | `rate(container_cpu_cfs_periods_total{container="virt-launcher"}[5m])` | Low |
| `container_cpu_cfs_throttled_periods_total` | Total number of periods that a container was throttled due to CPU limits. High values indicate CPU contention. For example, if a critical container shows high throttling, it may experience performance degradation. | `rate(container_cpu_cfs_throttled_periods_total{namespace="openshift-logging"}[5m]) / rate(container_cpu_cfs_periods_total{namespace="openshift-logging"}[5m]) > 0.25` | Medium |
| `container_cpu_cfs_throttled_seconds_total` | Total time (in seconds) that a container was throttled due to CPU limits. High values indicate CPU contention. For example, if collector pods show high throttling, logging may be affected. | `rate(container_cpu_cfs_throttled_seconds_total{namespace="openshift-logging"}[5m]) > 0.1` | Medium |
| `container_cpu_system_seconds_total` | Total CPU time spent by containers running system/kernel operations. Shows how much CPU your VMs are using for system-level tasks (like file I/O, network operations). High values might indicate VMs are doing intensive system operations that could impact performance. | `rate(container_cpu_system_seconds_total{pod=~"virt-launcher-.*"}[5m])` | Low |
| `containerd_cri_input_bytes_total` | Total bytes received by containerd CRI. Helps monitor container runtime network traffic. For example, sudden spikes may indicate unusual container activity or potential issues. | `rate(containerd_cri_input_bytes_total[5m]) > 1e6` | Low |
| `containerd_cri_output_bytes_total` | Total bytes of logs/output produced by containers managed by containerd. Large output can indicate chatty or misbehaving VMs/containers, and excessive logging may fill up disk space. | `rate(containerd_cri_output_bytes_total[5m]) > $(log_output_threshold)` | Low |
| `container_fs_reads_bytes_total` | Total bytes read from disk by a container. High values mean a VM or container is reading a lot from disk, which could indicate heavy database or application activity. Sudden spikes may signal backup jobs, indexing, or a VM under stress. | `rate(container_fs_reads_bytes_total[5m]) > $(read_bytes_threshold)` | Low |
| `container_fs_reads_total` | Total number of disk read operations performed by a container. Impact: High read counts may indicate disk-intensive workloads or that a VM is under heavy I/O load, which can slow down other VMs sharing the same storage. | `rate(container_fs_reads_total[5m]) > $(read_ops_threshold)` | Medium |
| `container_fs_usage_bytes` | Filesystem usage in bytes per container. High values may indicate disk space issues within containers. For example, if a container's filesystem usage approaches its limit, the container may experience write failures. | `container_fs_usage_bytes / container_fs_limit_bytes > 0.9` | Medium |
| `container_fs_write_seconds_total` | Total time (in seconds) spent writing to disk by a container. High values indicate containers (including VMs) are spending a lot of time on disk writes, which may signal slow disks or I/O bottlenecks affecting VM responsiveness. | `rate(container_fs_write_seconds_total[5m]) > $(write_time_threshold)` | Low |
| `container_fs_writes_total` | Total number of write operations performed by a container. High write counts can indicate heavy logging, database activity, or a misbehaving VM. Too many writes can wear out SSDs or fill up disk space.| `rate(container_fs_writes_total[5m]) > $(write_ops_threshold)` | Medium |
| `container_last_seen` | Timestamp of the last time a container was observed running. Helps detect if a VM or container has crashed or been deleted. If a VM’s container hasn’t been seen recently, it may have failed or been evicted. | `time() - container_last_seen > 300` | Medium |
| `container_memory_failcnt` | Number of memory allocation failures in a container. Indicates memory pressure. For example, if this is increasing, containers are hitting memory limits and failing to allocate memory. | `increase(container_memory_failcnt[5m]) > 0` | High |
| `container_memory_kernel_usage` | Amount of memory used by the kernel within the container. High kernel memory usage in a VM can indicate kernel leaks or abnormal behavior, which may lead to resource exhaustion or instability. | `container_memory_kernel_usage ` | Low |
| `container_memory_rss` | Resident Set Size: the amount of physical memory used by the container. Shows how much RAM a VM or container is actually using. High values can lead to memory pressure on the node, risking VM evictions or slowdowns. | `container_memory_rss` | Low |
| `container_memory_swap` | Amount of swap space used by a container. High values indicate memory pressure. For example, if containers are using swap, it can significantly degrade performance. | `container_memory_swap > 0` | Medium |
| `container_memory_usage_bytes` | Memory usage of a container in bytes. High values relative to limits may lead to OOM kills. For example, if a container's memory usage approaches its limit, it may be terminated by the OOM killer. | `container_memory_usage_bytes / container_memory_working_set_bytes > 0.9` | Medium |
| `container_memory_working_set_bytes` | Amount of memory actively used by the container (not easily reclaimed). This is the “real” memory footprint of a VM or container. High values indicate a VM is using a lot of memory, which can affect node stability and VM performance. | `container_memory_working_set_bytes{namespace="example-ns"}` | High |
| `container_network_receive_bytes_total` | Total bytes received over the network by a container. High network receive rates may indicate busy or heavily accessed VMs (e.g., web servers, databases). Sudden spikes could signal attacks or misconfigured workloads. | `rate(container_network_receive_bytes_total[5m]) > $(network_in_threshold)` | Medium |
| `container_network_receive_packets_total` | Total number of network packets received by a container. High packet counts can indicate network-intensive VMs or possible flooding/attack scenarios. | `rate(container_network_receive_packets_total[5m]) > $(packet_in_threshold)` | Medium |
| `container_network_transmit_bytes_total` | Total bytes sent over the network by a container. High transmit rates may indicate VMs exporting lots of data (e.g., file servers, backups). Sudden increases may signal data exfiltration or backup jobs.| `rate(container_network_transmit_bytes_total[5m]) > $(network_out_threshold)` | Medium |
| `container_oom_events_total` | Total number of OOM (Out of Memory) events for a container. Indicates memory-related container terminations. For example, if this increases, containers are being killed due to memory pressure. | `increase(container_oom_events_total[15m]) > 0` | High |
| `container_runtime_crio_containers_oom_count_total` | Total number of containers killed due to Out Of Memory (OOM) by CRI-O. If VMs are being OOM killed, they are running out of memory and crashing, leading to downtime and data loss. | `increase(container_runtime_crio_containers_oom_count_total[15m]) > 0` | High |
| `container_runtime_crio_containers_oom_total` | Total number of OOM events for containers managed by CRI-O. Indicates memory-related container terminations. For example, if this increases, CRI-O containers are being killed due to memory pressure. | `increase(container_runtime_crio_containers_oom_total[15m]) > 0` | High |
| `container_runtime_crio_image_pulls_failure_total` | Total number of failed image pulls by CRI-O. Failed image pulls can prevent VMs or workloads from starting, leading to service outages or failed VM launches. | `increase(container_runtime_crio_image_pulls_failure_total[15m]) > 0` | High |
| `container_runtime_crio_image_pulls_success_total` | Total number of successful image pulls by CRI-O. Shows that images are being pulled and workloads (including VMs) can start as expected. Sudden drops may indicate registry or network issues. | `increase(container_runtime_crio_image_pulls_success_total[15m])` | Low |
| `container_runtime_crio_operations_latency_seconds_total` | Precomputed quantiles (e.g., 0.5, 0.9, 0.99) of the time taken for CRI-O container runtime operations such as creating, attaching, or checking the status of containers. High values for operations like `CreateContainer` mean that starting VM pods is slow, which can delay VM launches, restarts, or migrations in OpenShift Virtualization. Use the quantile label to focus on the slowest operations (e.g., `quantile="0.99"` for the slowest 1% of requests). | `container_runtime_crio_operations_latency_seconds_total{operation="CreateContainer", quantile="0.99"} > $(crio_latency_threshold)` | High |
| `container_runtime_crio_operations_total` | Total number of operations performed by the CRI-O container runtime (e.g., starting, stopping, pulling images). High or spiking values may indicate many container (including VM pod) actions, which can signal cluster churn or issues with VM lifecycle operations. | `rate(container_runtime_crio_operations_total[5m]) > $(crio_ops_threshold)` | Low |
| `container_runtime_crio_processes_defunct` | Number of defunct (zombie) processes observed by CRI-O. Defunct processes can indicate problems with container or VM cleanup, potentially leading to resource leaks or node instability. | `container_runtime_crio_processes_defunct > $(defunct_proc_threshold)` | Medium |
| `container_spec_memory_limit_bytes` | The memory limit (in bytes) set for each container. Shows the configured memory cap for each VM or container. There are a lot of objects in OpenShift that have a 0 value. Too many will cause the web page to crash. You may wish to hide this by querying `container_spec_memory_limit_bytes >0`. If too low, VMs may be OOM killed; if too high, they may starve other workloads. | `container_spec_memory_limit_bytes > $(memory_limit_bytes)` | Low |
| `container_start_time_seconds` | Timestamp when each container started (in seconds since epoch). Useful for tracking VM/Pod uptime and troubleshooting restarts or unexpected terminations. The example query shows the days a container has been up. Remove the `/24` to show hours instead of days. | `(time() - container_start_time_seconds) /60 /60 /24` | Low |
| `container_threads` | Current number of threads used by each container. High thread counts in a VM or container can indicate heavy workload or runaway processes, possibly leading to resource exhaustion. The threshold will vary across environments based on numerous factors. | `container_threads > $(threads_threshold)` | Low |
| `container_threads_max` | Maximum number of threads allowed for each container. Shows the thread cap for containers. If usage approaches this value, VMs may be unable to spawn new threads, causing failures. | `container_threads / container_threads_max > 0.85` | Medium |
| `controller_runtime_active_workers` | Number of active worker threads in a controller High values may indicate controllers are busy reconciling many changes, which can be normal during scaling or upgrades but may also signal controller bottlenecks. | `controller_runtime_active_workers > $(active_workers_threshold)` | Low |
| `controller_runtime_reconcile_errors_total` | Total number of errors encountered during controller reconciliation loops. High error counts mean controllers (such as for VMs) are failing to process objects, which can cause VM creation, updates, or deletions to fail or stall. | `increase(controller_runtime_reconcile_errors_total[15m]) > $(reconcile_errors_threshold)` | High |
| `controller_runtime_reconcile_panics_total` | Total number of panics during controller reconciliation. Indicates serious issues with Kubernetes controllers. For example, any increase in this metric indicates controllers are experiencing critical failures. | `increase(controller_runtime_reconcile_panics_total[15m]) > 0` | Critical |
| `controller_runtime_reconcile_time_seconds_bucket` | Histogram of time spent reconciling resources in controllers (in seconds). High values mean controllers are taking longer to process VM or other resource changes, which can delay VM operations. | `histogram_quantile(0.95, sum(rate(controller_runtime_reconcile_time_seconds_bucket[5m])) by (le)) > $(reconcile_time_threshold)` | Medium |
| `controller_runtime_reconcile_time_seconds_per_instance_sum` | Total reconciliation time (in seconds) per controller instance. High values indicate some controllers are spending a lot of time processing changes, which may signal performance issues or heavy workloads. | `controller_runtime_reconcile_time_seconds_per_instance_sum > $(reconcile_time_sum_threshold)` | Medium |
| `controller_runtime_reconcile_time_seconds_sum` | Total reconciliation time (in seconds) across all controller instances. Shows the overall time controllers are spending on reconciliation, helping spot cluster-wide controller slowdowns. | `rate(controller_runtime_reconcile_time_seconds_sum[5m]) > $(reconcile_time_sum_rate_threshold)` | Medium |
| `controller_runtime_terminal_reconcile_errors_total` | Total number of terminal errors during controller reconciliation. Impact: Indicates controller failures. If this increases for a specific controller, resources managed by that controller may be in a bad state. | `increase(controller_runtime_terminal_reconcile_errors_total[15m]) > 0` | High |
| `coredns_cache_misses_total` | Total number of DNS queries not found in the CoreDNS cache (cache misses). High values mean DNS queries (including for VMs) are not being served from cache, possibly leading to slower DNS resolution and increased load on upstream servers. | `rate(coredns_cache_misses_total[5m]) > $(dns_miss_threshold)` | Low |
| `coredns_cache_requests_total` | Total number of DNS queries handled by the CoreDNS cache. Shows overall DNS cache usage. Comparing this with cache misses helps you understand DNS cache effectiveness for VM and cluster workloads.| `rate(coredns_cache_requests_total[5m]) > $(dns_cache_req_threshold)` | Low |
| `coredns_dns_request_duration_seconds_bucket` | Histogram showing how long it takes CoreDNS to answer DNS requests, grouped into time buckets. If DNS requests take too long, VMs and applications in your cluster will experience delays when trying to connect to services by name, which can slow down or disrupt VM operations and user experience. | `histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket[5m])) by (le)) > $(dns_latency_threshold)` | High |
| `coredns_dns_request_duration_seconds_count` | Total count of DNS requests processed. High counts may indicate heavy DNS traffic that could overwhelm CoreDNS, affecting VM connectivity. | `rate(coredns_dns_request_duration_seconds_count[5m]) > $(request_rate_threshold)` | Low |
| `coredns_dns_request_duration_seconds_sum` | Total time spent processing all DNS requests. Combined with count, shows average latency. High values mean overall slow DNS resolution for VMs. | `rate(coredns_dns_request_duration_seconds_sum[5m]) / rate(coredns_dns_request_duration_seconds_count[5m]) > $(avg_latency_threshold)` | Medium |
| `coredns_dns_request_size_bytes_count` | Counts the number of DNS requests received by CoreDNS, grouped by the size of each request (in bytes). Impact: This metric helps you understand the distribution and frequency of DNS request sizes in your cluster. Most DNS requests are small, but a sudden increase in large request sizes could indicate unusual or potentially malicious activity, misconfigured clients, or applications generating abnormally large queries. Large or unexpected DNS requests can put extra load on CoreDNS, potentially slowing down name resolution for VMs and applications, and may signal issues that could impact VM network performance or reliability | `rate(coredns_dns_request_size_bytes_count[5m]) > $(request_size_count_threshold)` | Low |
| `coredns_dns_response_size_bytes_count` | Number of DNS responses by size. Large responses might indicate misconfigured DNS records affecting VM services. | `rate(coredns_dns_response_size_bytes_count[5m]) > $(response_size_count_threshold)` | Low |
| `coredns_dns_responses_total` | Total DNS responses by response code. High error rates (e.g., SERVFAIL) can break VM-to-service communication. In CoreDNS there are often a high number of NXDOMAIN errors due to the way it is configured with `ndots` which cause a domain to be looked up with multiple permutations that ultimately do not exist. | `sum(rate(coredns_dns_responses_total{rcode!="NOERROR"}[5m])) by (rcode) > $(error_rate_threshold)` | Medium to High |
| `coredns_forward_healthcheck_broken_total` | Total number of failed CoreDNS forward health checks. Increasing values indicate DNS forwarding issues. For example, if this increases rapidly, DNS resolution to external domains may fail. | `rate(coredns_forward_healthcheck_broken_total[5m]) > 0` | High |
| `coredns_forward_max_concurrent_rejects_total` | Requests rejected due to upstream limits. May cause DNS failures for VMs if upstream servers are overloaded. | `increase(coredns_forward_max_concurrent_rejects_total[15m]) > 0` | Medium |
| `coredns_health_request_duration_seconds_bucket` | Histogram showing how long it takes CoreDNS to respond to health check requests, grouped into time buckets. This metric helps you see if CoreDNS is responding quickly to health checks. If health check responses become slow, Kubernetes may not detect a failing CoreDNS pod in time, which can delay repair or restart of DNS services. | `histogram_quantile(0.95, rate(coredns_health_request_duration_seconds_bucket[5m])) > $(health_check_threshold)` | Medium |
| `coredns_health_request_failures_total` | Total number of CoreDNS health check failures. Indicates DNS service health issues. For example, if this increases, the CoreDNS service may be experiencing problems affecting DNS resolution. | `rate(coredns_health_request_failures_total[5m]) > 0` | High |
| `coredns_kubernetes_dns_programming_duration_seconds_bucket` | Histogram showing how long it takes CoreDNS to update its DNS records after a change in Kubernetes (like when a new service or pod is created or deleted). | `histogram_quantile(0.95, rate(coredns_kubernetes_dns_programming_duration_seconds_bucket[5m])) > $(k8s_update_threshold)` | Medium |
| `coredns_kubernetes_rest_client_requests_total` | Total number of HTTP requests CoreDNS makes to the Kubernetes API server, labeled by HTTP method, response code, and API server host. CoreDNS relies on the Kubernetes API to stay up-to-date with changes to services, endpoints, and pods. If CoreDNS encounters a high rate of errors (especially 5xx status codes), it may not be able to update its DNS records correctly. | `rate(coredns_kubernetes_rest_client_requests_total{code=~"5.."}[5m]) > 0` | High |
| `coredns_panics_total` | Total number of panics in CoreDNS. Indicates serious issues with DNS service. For example, any increase in this metric indicates CoreDNS is experiencing critical failures. | `increase(coredns_panics_total[15m]) > 0` | Critical |
| `coredns_proxy_request_duration_seconds_count` | Total count of DNS proxy requests handled by CoreDNS. High counts may indicate heavy DNS proxy usage, which could affect DNS performance for VMs and cluster services. | `rate(coredns_proxy_request_duration_seconds_count[5m]) > $(proxy_req_threshold)` | Low |
| `coredns_reload_failed_total` | Total number of times CoreDNS failed to reload its configuration. Reload failures can leave CoreDNS running with outdated or incorrect DNS settings, causing DNS resolution issues for VMs and cluster workloads. | `increase(coredns_reload_failed_total[15m]) > 0` | High |
| `counter_memberlist_msg_dead` | Counts the number of "dead" messages detected by the cluster memberlist protocol, which is used for node discovery and cluster health. When a node in the cluster stops responding or is unreachable, other nodes will mark it as "dead" and send a "dead" message. A high or increasing count of dead messages may indicate node communication problems, network instability, or nodes crashing or being removed. | `increase(counter_memberlist_msg_dead[15m]) > $(dead_msg_threshold)` | Medium |
| `counter_memberlist_tcp_accept` | Total number of TCP connections accepted by the memberlist protocol. Tracks cluster node communication. Sudden drops or spikes may signal network issues affecting VM and pod placement. | `rate(counter_memberlist_tcp_accept[5m]) > $(tcp_accept_threshold)` | Low |
| `cronjob_controller_job_creation_skew_duration_seconds_bucket` | Histogram that measures the delay (in seconds) between when a CronJob is scheduled to run and when the job is actually created. If jobs (such as VM backups or scheduled maintenance) are created late, automation and SLAs can be disrupted. The histogram allows you to see not just the average delay, but also how bad the worst delays are (e.g., the slowest 5%). | `histogram_quantile(0.95, sum(rate(cronjob_controller_job_creation_skew_duration_seconds_bucket[5m])) by (le)) > $(job_skew_threshold)` | Medium |
| `cronjob_controller_job_creation_skew_duration_seconds_sum` | Total time (in seconds) of all job creation delays for cronjobs. High totals indicate persistent delays in job scheduling, which may impact regular VM or cluster maintenance tasks. | `rate(cronjob_controller_job_creation_skew_duration_seconds_sum[5m]) > $(job_skew_sum_threshold)` | Medium |
| `csi_operations_seconds_bucket` | Histogram of how long CSI (Container Storage Interface) storage operations take, grouped by duration buckets (seconds). This metric helps you understand if storage actions—like VM disk attach, detach, creation, or migration—are fast or slow. If storage operations are slow, VMs may be delayed or experience performance problems. | `histogram_quantile(0.95, sum(rate(csi_operations_seconds_bucket[5m])) by (le)) > $(csi_latency_threshold)` | High |
| `csi_operations_seconds_sum` | The total cumulative time (in seconds) spent on all CSI (Container Storage Interface) storage operations, summed across all operations and time. A high or rapidly increasing value means your storage subsystem is spending a lot of time processing requests, which can signal slow disk attach/detach, volume creation, or migration for VMs. | `rate(csi_operations_seconds_sum[5m]) > $(csi_op_sum_threshold)` | High |
| `csv_succeeded` | Indicates whether an Operator Lifecycle Manager (OLM) ClusterServiceVersion (CSV) has succeeded. A value of 1 means the operator is installed and running. | `csv_succeeded == 0` | High |
| `default_storage_class_count` | Number of default storage classes in the cluster. There should only be one default. More than one can cause unpredictable VM disk provisioning behavior. | `default_storage_class_count > 1` | Medium |
| `endpoint_slice_controller_changes` | Counts the number of changes (additions, removals, or updates) processed by the EndpointSlice controller in Kubernetes. Each time a Service’s set of endpoints (the Pods backing the Service) changes, the EndpointSlice controller must update the EndpointSlices. High rates of changes mean there are frequent updates to which Pods are available for Services—this can be due to rapid scaling, frequent pod restarts, or rolling updates. | `rate(endpoint_slice_controller_changes[5m]) > $(slice_change_threshold)` | Medium |
| `endpoint_slice_controller_endpoints_added_per_sync_count` | Counts how many endpoints (Pods or VMs) are added to EndpointSlices during each sync operation by the controller. Large numbers per sync mean many new Pods or VMs are being added at once—this can happen during rapid scaling, mass VM creation, or after a network disruption. | `rate(endpoint_slice_controller_endpoints_added_per_sync_count[5m]) > $(endpoints_added_threshold)` | Low |
| `endpoint_slice_controller_endpoints_desired` | Number of endpoints desired by the EndpointSlice controller. Indicates load on the EndpointSlice controller. For example, if this grows very large, it may indicate a large number of services or endpoints that could affect controller performance. | `endpoint_slice_controller_endpoints_desired > 10000` | Medium |
| `endpoint_slice_controller_endpointslices_changed_per_sync_count` | Counts how many EndpointSlices are changed (created, updated, or deleted) during each sync operation. High values mean the network topology is changing frequently, which can be caused by rapid scaling, rolling updates, or unstable workloads. Frequent changes can increase network traffic, slow down DNS and service discovery, and cause temporary connectivity issues for VMs and applications. | `rate(endpoint_slice_controller_endpointslices_changed_per_sync_count[5m]) > $(slices_changed_threshold)` | Medium |
| `endpoint_slice_controller_syncs` | Counts the total number of sync operations performed by the EndpointSlice controller over time. High sync frequency means the controller is working hard to keep up with changes in the cluster (such as Pods/VMs starting, stopping, or moving). If syncs are happening very frequently, it may indicate instability, a rapid deployment, or a misbehaving workload. | `rate(endpoint_slice_controller_syncs[5m]) > $(syncs_threshold)` | Low |
| `endpoint_slice_mirroring_controller_addresses_skipped_per_sync_sum` | Total number of endpoint addresses skipped during sync operations by the EndpointSlice mirroring controller. This controller maintains backward compatibility by creating legacy Endpoint objects from EndpointSlices. Skipped addresses may indicate configuration issues or resource limits that could affect VM service discovery for older applications that rely on legacy Endpoints. | `rate(endpoint_slice_mirroring_controller_addresses_skipped_per_sync_sum[5m]) > $(skipped_addresses_threshold)` | Low |
| `endpoint_slice_mirroring_controller_changes` | Total number of changes processed by the EndpointSlice mirroring controller.  High change rates may indicate frequent VM or service updates that stress the controller | `rate(endpoint_slice_mirroring_controller_changes[5m]) > $(mirroring_changes_threshold)` | Low |
| `endpoint_slice_mirroring_controller_endpoints_added_per_sync_sum` | Total number of endpoints added during all sync operations by the mirroring controller. Shows how many new VM or pod endpoints are being added to legacy Endpoint objects. High values may indicate rapid scaling or frequent VM creation that could stress the controller and delay service discovery for legacy applications. | `rate(endpoint_slice_mirroring_controller_endpoints_added_per_sync_sum[5m]) > $(endpoints_added_sum_threshold)` | Low |
| `endpoint_slice_mirroring_controller_endpoints_removed_per_sync_bucket` | Histogram of how many endpoints are removed per sync operation by the mirroring controller. Large numbers of removals per sync may indicate mass VM shutdowns, node failures, or scaling events. This can help identify when significant changes in VM availability are happening that might affect service discovery. | `histogram_quantile(0.95, sum(rate(endpoint_slice_mirroring_controller_endpoints_removed_per_sync_bucket[5m])) by (le)) > $(endpoints_removed_threshold)` | Low |
| `endpoint_slice_mirroring_controller_endpoints_sync_duration_bucket` | Histogram of how long it takes the mirroring controller to complete sync operations (in seconds). Slow sync operations can delay updates to legacy Endpoint objects, causing delays in service discovery for applications that haven't migrated to EndpointSlices. This can affect VM connectivity and application reliability. | `histogram_quantile(0.95, sum(rate(endpoint_slice_mirroring_controller_endpoints_sync_duration_bucket[5m])) by (le)) > $(sync_duration_threshold)` | Medium |
| `endpoint_slice_mirroring_controller_endpoints_updated_per_sync_bucket` | Histogram of how many endpoints are updated per sync operation by the mirroring controller. Frequent updates can indicate changing VM or pod states (like readiness changes). High update rates may stress the controller and delay service discovery updates for legacy applications. | `histogram_quantile(0.95, sum(rate(endpoint_slice_mirroring_controller_endpoints_updated_per_sync_bucket[5m])) by (le)) > $(endpoints_updated_threshold)` | Low |
| `ephemeral_volume_controller_create_failures_total` | Total number of failures when creating ephemeral volumes. Ephemeral volumes are used for temporary VM storage needs. Creation failures can prevent VMs from starting properly or cause them to fail during runtime, leading to service outages and data loss for applications using temporary storage. | `increase(ephemeral_volume_controller_create_failures_total[15m]) > 0` | Medium |
| `etcd_cluster_version` | Information about the etcd cluster version running in the cluster. etcd stores all Kubernetes state, including VM configurations. Version information helps with troubleshooting, ensuring compatibility, and planning upgrades. Mismatched versions can cause cluster instability affecting all VM operations. | `etcd_cluster_version` | Low |
| `etcd_debugging_disk_backend_commit_rebalance_duration_seconds_bucket` | Histogram of time spent rebalancing data during etcd disk commits (in seconds). Slow rebalance operations can indicate disk performance issues that may cause API server slowdowns, delayed VM operations, or cluster instability. | `histogram_quantile(0.95, sum(rate(etcd_debugging_disk_backend_commit_rebalance_duration_seconds_bucket[5m])) by (le)) > $(etcd_rebalance_threshold)` | High |
| `etcd_debugging_disk_backend_commit_rebalance_duration_seconds_sum` | Total time spent on etcd disk rebalance operations during commits. High values indicate etcd is spending significant time rebalancing data, which can slow down all cluster operations including VM creation, updates, and deletions. | `rate(etcd_debugging_disk_backend_commit_rebalance_duration_seconds_sum[5m]) > $(etcd_rebalance_sum_threshold)` | High |
| `etcd_debugging_disk_backend_commit_spill_duration_seconds_bucket` | Histogram of time spent on etcd disk spill operations during commits (in seconds). Spill operations occur when etcd's memory buffers are full and data must be written to disk. Slow spills can indicate storage bottlenecks that affect all VM and cluster operations. | `histogram_quantile(0.95, sum(rate(etcd_debugging_disk_backend_commit_spill_duration_seconds_bucket[5m])) by (le)) > $(etcd_spill_threshold)` | High |
| `etcd_debugging_disk_backend_commit_spill_duration_seconds_sum` | Total time spent on etcd disk spill operations during commits. High values indicate etcd is frequently spilling data to disk, suggesting memory pressure or slow storage that can degrade all cluster operations including VM management. | `rate(etcd_debugging_disk_backend_commit_spill_duration_seconds_sum[5m]) > $(etcd_spill_sum_threshold)` | High |
| `etcd_debugging_disk_backend_commit_write_duration_seconds_count` | Total number of disk write operations during etcd commits. High write counts may indicate frequent cluster state changes or storage inefficiency. Excessive writes can wear out SSDs and slow down VM operations that depend on etcd performance. | `rate(etcd_debugging_disk_backend_commit_write_duration_seconds_count[5m]) > $(etcd_write_count_threshold)` | Medium |
| `etcd_debugging_disk_backend_commit_write_duration_seconds_sum` | Total time spent writing to disk during etcd commits. High values indicate slow disk writes, which can cause API server timeouts and delays in all VM operations. Fast, reliable etcd writes are critical for responsive VM management. | `rate(etcd_debugging_disk_backend_commit_write_duration_seconds_sum[5m]) > $(etcd_write_sum_threshold)` | High |
| `etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds_sum` | Measures how long etcd pauses all operations during database compaction. In OpenShift Virtualization, long pauses can delay VM operations like starting, stopping, or migrating VMs because the cluster state becomes temporarily unavailable. If compaction takes 500ms, VM status updates are frozen during this time. | `rate(etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds_sum[5m]) / rate(etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds_count[5m])` | Medium |
| `etcd_debugging_mvcc_db_compaction_total_duration_milliseconds_sum` | Tracks the complete time etcd spends compacting its database to reclaim space from deleted keys. For virtualization workloads, frequent VM lifecycle changes create many deleted keys. Long compaction times indicate storage performance issues | `rate(etcd_debugging_mvcc_db_compaction_total_duration_milliseconds_sum[5m]) > $(total_compaction_threshold)` | Medium |
| `etcd_debugging_mvcc_events_total` | Counts all multi-version concurrency control events in etcd. Each VM state change (created, running, migrating, stopped) generates MVCC events. High event rates indicate heavy VM activity or potential issues with VM controllers continuously updating VM status. Example: 1000 events/minute might indicate VMs constantly restarting. | `rate(etcd_debugging_mvcc_events_total[5m]) > $(mvcc_events_threshold)` | Medium |
| `etcd_debugging_mvcc_index_compaction_pause_duration_milliseconds_bucket` | Provides detailed timing distribution of etcd index compaction pauses. The index helps etcd quickly find VM definitions and status. Long index compaction pauses can make VM queries slow, affecting the web console's VM list or CLI commands like `oc get vms`. Example: If 95th percentile exceeds 100ms, VM operations feel sluggish. | `histogram_quantile(0.95, etcd_debugging_mvcc_index_compaction_pause_duration_milliseconds_bucket) > $(index_pause_threshold)` | Medium |
| `etcd_debugging_mvcc_pending_events_total` | Shows events waiting to be processed by etcd's MVCC system. In virtualization environments, pending events can indicate that VM state changes are backing up, potentially causing delays in VM status updates, scheduling decisions, or live migration coordination. | `etcd_debugging_mvcc_pending_events_total > $(pending_events_threshold)` | Medium |
| `etcd_debugging_mvcc_slow_watcher_total` | Counts watchers that are slow to process etcd events. VM controllers and operators watch for VM state changes. Slow watchers can cause VM operations to appear delayed or inconsistent in the UI, and may indicate controller performance issues or network problems between cluster components. Example: Slow VM controller watchers delay VM startup notifications. | `etcd_debugging_mvcc_slow_watcher_total > $(slow_watchers_threshold)` | Medium |
| `etcd_debugging_mvcc_watcher_total` | Watchers are internal etcd constructs that monitor changes to keys or key ranges. In OpenShift Virtualization, components like virt-controller, virt-handler, and operators rely on watchers to receive real-time updates about VM objects and cluster state. A sudden increase may indicate resource leaks or runaway processes, potentially exhausting etcd resources and degrading cluster performance. A sudden drop may mean critical components are disconnected or failing. | `abs(increase(etcd_debugging_mvcc_watcher_total[5m]))` | Medium |
| `etcd_debugging_raft_terms_total` | Total number of Raft terms seen by this etcd member. Rapid increases indicate frequent leader elections and potential instability. If this increases rapidly over a short period, it indicates cluster instability with frequent leadership changes. | `delta(etcd_debugging_raft_terms_total[15m]) > 5` | High |
| `etcd_debugging_snap_save_total_duration_seconds_bucket` |  Histogram of complete snapshot save operations. Snapshots preserve the entire cluster state including all VM definitions, network configurations, and storage mappings. Slow snapshot saves can indicate storage issues and affect disaster recovery readiness. | `histogram_quantile(0.95, etcd_debugging_snap_save_total_duration_seconds_bucket) > $(snapshot_duration_threshold)` | Medium |
| `etcd_debugging_snap_save_total_duration_seconds_sum `|  Total time spent saving snapshots. This helps identify trends in snapshot performance over time. For virtualization workloads with frequent state changes, increasing snapshot times might indicate growing data size or degrading storage performance affecting backup reliability.| `rate(etcd_debugging_snap_save_total_duration_seconds_sum[1h]) > $(snapshot_rate_threshold)` | Medium |
|`etcd_debugging_store_writes_total`|This metric counts total write operations to the legacy etcd v2 store, which is not used in OpenShift 4.x clusters | `rate(etcd_debugging_store_writes_total[5m]) > $(store_writes_threshold)` | None |
| `etcd_disk_backend_commit_duration_seconds_bucket` | Histogram of latency distribution of commit operations called by the backend. High latencies indicate disk performance issues affecting etcd write operations. For example, if the 99th percentile exceeds 25ms, it may indicate disk performance issues affecting etcd stability. | `histogram_quantile(0.99,sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (le))` | High |
| `etcd_disk_backend_commit_duration_seconds_sum` | Cumulative time spent committing to disk. Increasing commit times indicate degrading storage performance that will directly impact VM responsiveness. This is critical for live migration success and VM startup times. | `rate(etcd_disk_backend_commit_duration_seconds_sum[5m]) > $(commit_sum_threshold)` | Critical |
| `etcd_disk_backend_defrag_duration_seconds_sum` | Time spent defragmenting etcd's disk storage. Defragmentation reclaims space from deleted VM records and configurations. Long defrag times might indicate that etcd's storage is heavily fragmented due to frequent VM lifecycle changes. | `increase(etcd_disk_backend_defrag_duration_seconds_count[5m]) > 0` | Medium |
| `etcd_disk_backend_snapshot_duration_seconds_bucket` | Histogram of time to write snapshots to disk. This directly affects backup creation and disaster recovery capabilities for your virtualization environment. Slow snapshot writes might indicate storage issues that could compromise your ability to recover VM configurations and state.  | `histogram_quantile(0.95, etcd_disk_backend_snapshot_duration_seconds_bucket)` | Medium |
| `etcd_disk_backend_snapshot_duration_seconds_sum` | Cumulative time writing snapshots to disk. Trends in this metric help identify degrading storage performance that affects backup reliability. For virtualization workloads, reliable snapshots are essential for maintaining VM state consistency and disaster recovery readiness. Example: Increasing snapshot times over days indicates storage degradation. | `rate(etcd_disk_backend_snapshot_duration_seconds_sum[1h])` | Medium |
| `etcd_disk_wal_fsync_duration_seconds_bucket` | Histogram of latency distribution of fsync operations to the WAL (Write-Ahead Log). High values indicate disk I/O issues affecting etcd performance. If the 99th percentile exceeds 10ms, it may indicate slow disk performance leading to potential leader election issues and slow writes. | `histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) by (le, instance))` | Critical |
| `etcd_disk_wal_fsync_duration_seconds_sum` | Sum of time spent on fsync operations to the WAL. Increasing values indicate cumulative disk I/O pressure. This metric is used with the count to calculate average fsync duration, which should be monitored for trends. | `rate(etcd_disk_wal_fsync_duration_seconds_sum[5m]) / rate(etcd_disk_wal_fsync_duration_seconds_count[5m])` | Medium |
| `etcd_disk_wal_write_bytes_total` | Total number of bytes written to the etcd Write-Ahead Log (WAL). WAL ensures all changes are safely written to disk before being committed, providing durability for VM and cluster state. High values indicate heavy write activity, which can be normal during frequent VM operations but may signal excessive churn if unexpectedly high. | `rate(etcd_disk_wal_write_bytes_total[5m])` | Medium |
| `etcd_disk_wal_write_duration_seconds_bucket` | Histogram of the duration (in seconds) for WAL write operations. Longer durations mean slower disk I/O, which can delay VM state persistence and cluster operations. If the 95th percentile write duration exceeds 0.1 seconds, etcd may be experiencing disk latency. | `histogram_quantile(0.95, rate(etcd_disk_wal_write_duration_seconds_bucket[5m])) > 0.1` | High |
| `etcd_disk_wal_write_duration_seconds_sum` | Cumulative sum of seconds spent writing to the WAL. Use in combination with `_count` to find the average write duration over a time window. If the average write time increases, it may indicate disk degradation. | `rate(etcd_disk_wal_write_duration_seconds_sum[5m]) / rate(etcd_disk_wal_write_duration_seconds_count[5m])` | High |
| `etcd_lease_object_counts_bucket` | Histogram of the number of objects attached to a single etcd lease. In etcd, a lease is a mechanism to associate a set of keys with a time-to-live (TTL). When the lease expires or is revoked, all attached keys are automatically deleted. This is used for ephemeral or short-lived resources, such as locks, leader elections, or temporary VM-related objects in OpenShift Virtualization. Large numbers of objects per lease may indicate resource leaks  or unusually high churn of ephemeral objects. | `histogram_quantile(0.95, sum(rate(etcd_lease_object_counts_bucket[5m])) by (le))` | Medium |
| `etcd_lease_object_counts_count` | Number of objects attached to leases. High values may indicate resource leaks. For example, if this grows continuously without bounds, it may indicate applications not properly releasing leases. | `etcd_lease_object_counts_count > 1000` | Medium |
| `etcd_lease_object_counts_sum` | Cumulative sum of all objects attached to leases. A growing sum may indicate accumulating temporary resources, which could exhaust etcd memory over time. Example: If this value grows steadily, investigate for resource leaks. | `increase(etcd_lease_object_counts_sum[1h])` | Medium |
| `etcd_mvcc_db_total_size_in_bytes` | Total size of the etcd database in bytes. As this approaches the quota, etcd may stop accepting writes. For example, if this exceeds 80% of the quota (typically 2-8GB), compaction and defragmentation may be needed to prevent etcd from becoming read-only. | `etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes > 0.8` | Critical |
| `etcd_mvcc_db_total_size_in_use_in_bytes` | Logical size (in bytes) of data actively used in the etcd database for a single member. This is the actual amount of data being used by etcd after compaction, not counting free space. If this is much less than the physical size, a lot of disk space is wasted due to fragmentation. When the in-use size is close to the physical size, the database is efficiently packed. This is just the "live" data, not the total file size. | `etcd_mvcc_db_total_size_in_use_in_bytes / etcd_mvcc_db_total_size_in_bytes` | Medium |
| `etcd_mvcc_hash_duration_seconds_bucket` | Histogram of durations for full etcd database hash computations. Hashing is used for consistency checks between etcd members (e.g., during leader changes or disaster recovery). Long hash durations can delay cluster operations and indicate performance issues, especially if the etcd database is large or the underlying disk is slow. This metric will return NaN if no hash operations have been performed since Prometheus started scraping metrics.| `histogram_quantile(0.95, rate(etcd_mvcc_hash_duration_seconds_bucket[5m])) ` | Low |
| `etcd_mvcc_hash_duration_seconds_sum` | 	Cumulative sum of seconds spent on full etcd database hash computations. This metric will return NaN if no hash operations have been performed since Prometheus started scraping metrics, which is normal for most OpenShift clusters. Hash operations are rare and typically only triggered during explicit consistency checks or troubleshooting. Cumulative sum of seconds spent hashing the database. Use with `_count` to calculate average hash duration. If average hash time increases, etcd consistency checks may be slowing down. | `rate(etcd_mvcc_hash_duration_seconds_sum[5m]) / rate(etcd_mvcc_hash_duration_seconds_count[5m]) ` | Low |
| `etcd_mvcc_hash_rev_duration_seconds_bucket` | Histogram of durations for revision-based hash computations. Used to verify consistency at specific revisions (e.g., after VM state changes). Long durations may indicate large or complex state changes.This metric will return NaN if no hash operations have been performed since Prometheus started scraping metrics | `histogram_quantile(0.95, rate(etcd_mvcc_hash_rev_duration_seconds_bucket[5m])) > $(hash_rev_duration_threshold)` | Low |
| `etcd_mvcc_hash_rev_duration_seconds_sum` | Cumulative sum of seconds spent on revision-based hash computations. Use with `_count` for average duration.  Spikes may indicate heavy cluster activity or performance issues. This metric will return NaN if no hash operations have been performed since Prometheus started scraping metrics| `rate(etcd_mvcc_hash_rev_duration_seconds_sum[5m]) / rate(etcd_mvcc_hash_rev_duration_seconds_count[5m])` | Low |
| `etcd_network_client_grpc_received_bytes_total` | Total bytes received by etcd from all clients over gRPC. High values indicate heavy client interaction, which is normal with many VMs or frequent control plane operations. Sudden spikes may signal a misbehaving controller or DDoS. | `rate(etcd_network_client_grpc_received_bytes_total[5m]) ` | Low |
| `etcd_network_client_grpc_sent_bytes_total` | Total bytes sent by etcd to all clients over gRPC. High values indicate heavy response traffic, which may be expected during VM migrations or mass updates. Sudden changes could indicate issues. | `rate(etcd_network_client_grpc_sent_bytes_total[5m])` | Medium |
| `etcd_network_disconnected_peers_total` | Total number of times etcd peers have disconnected. Peer disconnections can disrupt cluster consensus and impact VM state reliability. Frequent disconnects may indicate network or infrastructure issues. | `increase(etcd_network_disconnected_peers_total[1h])` | High |
| `etcd_network_peer_received_bytes_total` | Total bytes received by this etcd member from other peers. Used to monitor inter-node communication health. Drops may indicate network partitions, which can stall VM operations. | `rate(etcd_network_peer_received_bytes_total[5m])` | Medium |
| `etcd_network_peer_round_trip_time_seconds_count` | Total number of RTT samples collected. Useful for ensuring sample size in RTT analysis. Low counts may indicate connectivity issues or idle clusters. | `increase(etcd_network_peer_round_trip_time_seconds_count[1h])` | Medium |
| `etcd_network_peer_round_trip_time_seconds_sum` | Cumulative sum of RTT seconds. Use with `_count` for average peer RTT. Increasing averages may indicate network degradation. | `rate(etcd_network_peer_round_trip_time_seconds_sum[5m]) / rate(etcd_network_peer_round_trip_time_seconds_count[5m])` | High |
| `etcd_network_snapshot_receive_success` | Cumulative count of successful Raft snapshot receives by this etcd member from peers. In healthy clusters, this value is typically 1 (from initial bootstrap) and does not increase unless a peer falls far behind and needs to resync state via snapshot. A steady value of 1 is normal and expected. | `increase(etcd_network_snapshot_receive_success[1h])` | Low |
| `etcd_network_snapshot_receive_total_duration_seconds_bucket` | Histogram of durations (in seconds) for receiving Raft snapshots from peers.Raft snapshots are used when an etcd member falls far behind and must catch up by receiving a full state snapshot from another peer. In OpenShift Virtualization, this is rare and usually only happens after a network partition or severe lag. Long durations may indicate slow disk, network issues, or large cluster state, which can delay recovery and impact VM and cluster operations. If no snapshots are received, this metric may be absent or NaN, which is normal in healthy clusters. | `histogram_quantile(0.95, rate(etcd_network_snapshot_receive_total_duration_seconds_bucket[5m])) > $(snapshot_receive_duration_threshold)` | Medium |
| `etcd_network_snapshot_send_success` | Counter for the total number of successful Raft snapshot sends from this etcd member to peers. This number increases only when a peer falls so far behind that it must receive a full snapshot to catch up. In healthy OpenShift Virtualization clusters, this value is usually 0 or 1 and rarely increases. A sudden increase may indicate network instability or a peer consistently lagging.  If no snapshots have ever been sent, this metric may be absent or NaN, which is normal. | `increase(etcd_network_snapshot_send_success[1h]) > $(snapshot_send_threshold)` | Low |
| `etcd_request_duration_seconds_bucket` | Histogram of request durations (in seconds) for all etcd operations. This metric measures how long etcd takes to process client requests. Monitoring the 95th percentile helps detect performance degradation.  If no requests are processed in the window, result may be NaN. There are a large number of objects in this query, you may want to filter by namespace. | `histogram_quantile(0.95, rate(etcd_request_duration_seconds_bucket{namespace="default"}[5m])) > $(request_duration_threshold)` | Medium |
| `etcd_requests_total` | Total number of client requests received by etcd. This counter tracks all API requests to etcd. A sudden spike may indicate a misbehaving controller, DDoS, or runaway automation. In OpenShift Virtualization, sustained high rates may stress etcd and slow down VM operations. If no requests are received, metric may be NaN, but this is rare in active clusters. There are a large number of datapoints in this metric. You may want to filter by the namespace. | `rate(etcd_requests_total{namespace!="default"}[5m]) > $(requests_rate_threshold)` | Medium |
| `etcd_server_apply_duration_seconds_bucket` | Histogram of durations for applying committed proposals to the etcd state machine. After a change is committed in Raft, it must be applied to the database. High durations can signal slow disk, CPU bottlenecks, or complex transactions, impacting VM state changes and cluster responsiveness. If no proposals are applied, may be NaN. | `histogram_quantile(0.95, rate(etcd_server_apply_duration_seconds_bucket[5m])) > 0.5` | High |
| `etcd_server_apply_duration_seconds_sum` | Cumulative sum of seconds spent applying proposals. Use with the corresponding `_count` metric to get the average apply duration. Increasing averages may indicate etcd is struggling to keep up with workload, which can delay VM operations. | `rate(etcd_server_apply_duration_seconds_sum[5m]) / rate(etcd_server_apply_duration_seconds_count[5m]) > $(avg_apply_duration)` | High |
| `etcd_server_client_requests_total` | Total number of client requests received by type. Rate changes indicate client load patterns. For example, monitoring the rate of write requests can help identify periods of high load on the etcd cluster. | `sum(rate(etcd_server_client_requests_total{type="write"}[5m]))` | Medium |
| `etcd_server_has_leader` | Whether this etcd member has a leader (1) or not (0). Values of 0 indicate split-brain or isolation. If this is 0 for any etcd member, that member cannot process writes and may be network isolated. | `etcd_server_has_leader == 0` | Critical |
| `etcd_server_health_failures` | Total number of health check failures. Tracks how many times etcd failed a health check. Frequent failures can indicate instability, network issues, or resource exhaustion. In OpenShift Virtualization, this can lead to API unavailability and VM disruptions. | `increase(etcd_server_health_failures[1h]) > $(health_failure_threshold)` | Critical |
| `etcd_server_heartbeat_send_failures_total` | Total number of failed Raft leader heartbeats sent. Heartbeats are used to maintain cluster leadership and consensus. Failures can be caused by overload, slow disk, or network issues. Frequent failures can cause leader elections and cluster instability, impacting VM management. If no failures, metric may be zero or absent. | `increase(etcd_server_heartbeat_send_failures_total[1h]) > $(heartbeat_failure_threshold)` | Critical |
| `etcd_server_is_leader` | Indicates if this etcd member is the current leader (1 = yes, 0 = no). Leadership is required for serving write requests. In OpenShift Virtualization, if no leader exists, the cluster cannot process changes, impacting all VM and cluster operations. | `sum(etcd_server_is_leader{namespace="openshift-etcd"}) !=1` | Critical |
| `etcd_server_leader_changes_seen_total` | Total number of leader changes observed by this member. Leader changes are normal during upgrades or brief outages but frequent changes indicate instability, which can disrupt VM operations and cause API outages.| `sum(increase(etcd_server_leader_changes_seen_total[1h])) > 3` | High |
| `etcd_server_proposals_applied_total` | Total number of Raft proposals applied to the etcd state machine. In etcd, a proposal is a request to change the cluster state (such as creating, updating, or deleting a VM or resource). Once a proposal is agreed upon via the Raft consensus algorithm, it is "applied" to the etcd database. This metric counts how many such changes have been successfully applied. This metric should steadily increase as the cluster operates. If it stops increasing (i.e., the rate drops to zero), etcd may be unhealthy or stalled. If the cluster is idle or has just started, this metric may be zero or absent, resulting in a NaN for rate queries. | `rate(etcd_server_proposals_applied_total[5m]) < $(min_applied_rate)` | High |
| `etcd_server_proposals_failed_total` | Total number of failed proposals (consensus operations). Increasing values indicate consensus issues. If this increases, it indicates problems with the Raft consensus algorithm, potentially due to network issues or overload. | `increase(etcd_server_proposals_failed_total[15m]) > 0` | High |
| `etcd_server_quota_backend_bytes` | Maximum backend size in bytes etcd can use before triggering an alarm. Defines the limit for etcd database size. This is typically set to 2-8GB. If the database size approaches this value, maintenance is required. | `etcd_server_quota_backend_bytes` | Low |
| `etcd_server_slow_apply_total` | Total number of slow apply operations. Counts how often applying a proposal took longer than a threshold. Frequent slow applies can signal disk or CPU bottlenecks, impacting VM and cluster responsiveness. If no slow applies, may be zero or absent. | `increase(etcd_server_slow_apply_total[1h]) > $(slow_apply_threshold)` | Medium |
| `etcd_server_snapshot_apply_in_progress_total` | Number of snapshot apply operations currently in progress. Applying a snapshot is a blocking operation to restore state. Should be zero during normal operation. Nonzero values indicate recovery or major re-sync, which may temporarily impact cluster responsiveness. If no snapshot applies in progress, may be zero or absent. | `etcd_server_snapshot_apply_in_progress_total > 0` | Medium |
| `etcd_snap_db_save_total_duration_seconds_bucket` | Histogram of durations for saving the etcd database snapshot to disk. Snapshots are used for backup and disaster recovery. Long durations may indicate disk issues or large cluster state, impacting backup reliability. If no snapshots saved, may be absent or NaN. | `histogram_quantile(0.95, rate(etcd_snap_db_save_total_duration_seconds_bucket[5m])) > $(snapshot_save_duration_threshold)` | Medium |
| `etcd_snap_fsync_duration_seconds_bucket` | Histogram of durations for fsync (disk flush) operations during snapshot save. Fsync ensures data is safely written to disk. Long fsync durations may indicate disk latency, risking data durability and backup reliability. If no fsyncs, may be absent or NaN. | `histogram_quantile(0.95, rate(etcd_snap_fsync_duration_seconds_bucket[5m])) > $(fsync_duration_threshold)` | Medium |
| `etcd_snap_fsync_duration_seconds_sum` | Cumulative sum of seconds spent on fsync during snapshot saves. Use with corresponding `_count` for average fsync duration. Increasing averages may indicate deteriorating disk performance. If no fsyncs, may be absent or NaN. | `rate(etcd_snap_fsync_duration_seconds_sum[5m]) / rate(etcd_snap_fsync_duration_seconds_count[5m]) > $(avg_fsync_duration)` | Medium |
| `federate_requests_failed_total` | Total number of failed federation requests. Federation allows one Prometheus server to scrape metrics from another Prometheus server. This is commonly used in OpenShift to aggregate metrics from multiple clusters or to create hierarchical monitoring setups. Failed federation requests mean that monitoring data about your VMs and cluster health may not be properly aggregated. If no federation is configured or no requests have been made, this metric may be absent or NaN. | `increase(federate_requests_failed_total[1h]) > $(federation_failure_threshold)` | Medium |
| `federate_samples` | Total number of samples successfully federated. A sample is a single data point consisting of a metric name, labels, timestamp, and value. This metric indicates how much monitoring data is being successfully transferred between Prometheus instances. If federate_samples drops significantly, you may lose visibility into VM CPU, memory, and disk metrics in your centralized monitoring. If no federation is active, this metric may be absent or NaN. | `rate(federate_samples[5m]) < $(min_federation_samples_rate)` | Medium |
| `filter:apiserver_request_filter_duration_seconds_bucket:rate1m` | API server request filters are middleware components that process incoming requests before they reach the actual API handlers. They perform tasks like authentication, authorization, admission control, and request validation. Slow request filtering can delay VM operations like creation, deletion, live migration, and status updates. High filter durations indicate that the API server is struggling to process requests efficiently. If no API requests are processed in the 1-minute window, this may be NaN. | `histogram_quantile(0.95, filter:apiserver_request_filter_duration_seconds_bucket:rate1m)` | Medium |
| `filter:apiserver_request_filter_duration_seconds_bucket:rate5m` |  Same as the `rate1m` metric above but for 5 minute intervals. | `histogram_quantile(0.95, filter:apiserver_request_filter_duration_seconds_bucket:rate5m) > $(api_filter_duration_long_threshold)` | Medium |
| `flow_schema_priority_level:apiserver_flowcontrol_current_executing_requests:sum` | Current number of API requests being executed, grouped by flow schema and priority level. APF is a Kubernetes feature that manages API server request queuing and execution to prevent resource exhaustion and ensure fair access. Flow schemas classify requests, and priority levels determine their importance. If too many requests are executing simultaneously, new VM creation requests may be delayed or rejected. If no requests are currently executing, this may be zero or absent. | `flow_schema_priority_level:apiserver_flowcontrol_current_executing_requests:sum > $(max_executing_requests)` | Medium |
| `flow_schema_priority_level:apiserver_flowcontrol_current_inqueue_requests:sum` | Current number of API requests waiting in queues, grouped by flow schema and priority level. When the API server receives more requests than it can handle immediately, it places them in queues based on their priority and flow schema. Queued requests wait for their turn to be executed. High queue depths indicate API server backlog. If VM creation requests are queued for minutes, users will experience long delays when deploying new VMs. If no requests are queued, this may be zero or absent. | `flow_schema_priority_level:apiserver_flowcontrol_current_inqueue_requests:sum > $(max_queued_requests)` | Medium |
| `force_cleaned_failed_volume_operation_errors_total` | Total number of errors encountered during forced cleanup of failed volume operations. When volume operations (like mounting, unmounting, or detaching) fail and cannot be resolved normally, Kubernetes may attempt to force-clean these operations to prevent resource leaks and allow recovery. Errors during forced cleanup can leave VMs in inconsistent states, prevent VM deletion, or cause storage resource leaks. | `increase(force_cleaned_failed_volume_operation_errors_total[1h]) > $(volume_cleanup_error_threshold)` | High |
| `force_cleaned_failed_volume_operations_total` | Total number of failed volume operations that required forced cleanup.  High numbers indicate persistent storage issues that can affect VM reliability and data availability. If no volume operations have failed, this may be zero or absent. | `increase(force_cleaned_failed_volume_operations_total[1h]) > $(failed_volume_ops_threshold)` | High |
| `garbagecollector_controller_resources_sync_error_total` | Total number of errors encountered by the garbage collector controller during resource synchronization. This Kubernetes controller automatically deletes objects that are no longer needed.  Garbage collection errors can prevent proper cleanup of VM-related resources (pods, services, volumes), leading to resource leaks and potential cluster instability. Failed cleanup can also prevent VM deletion and cause storage waste.| `increase(garbagecollector_controller_resources_sync_error_total[1h]) > $(gc_sync_error_threshold)` | Low |
| `go_cpu_classes_gc_mark_dedicated_cpu_seconds_total` | Total CPU seconds spent in Go garbage collection mark phase using dedicated CPU resources. The mark phase identifies which objects are still in use. Dedicated CPU means CPU cores exclusively used for garbage collection. High garbage collection CPU usage indicates memory pressure in cluster components (API server, controllers, operators). If the virt-controller spends too much CPU on garbage collection, VM lifecycle operations may become slow. | `rate(go_cpu_classes_gc_mark_dedicated_cpu_seconds_total[5m]) > $(gc_mark_cpu_threshold)` | Low |
| `go_cpu_classes_gc_pause_cpu_seconds_total` | Total CPU seconds spent in Go garbage collection pause phases. During garbage collection, Go applications may pause execution to perform memory cleanup operations. These pauses can temporarily stop request processing. Frequent or long pauses indicate memory management issues. If no GC pauses have occurred, this may be zero or absent. | `rate(go_cpu_classes_gc_pause_cpu_seconds_total[5m]) > $(gc_pause_cpu_threshold)` | Low |
| `go_cpu_classes_gc_total_cpu_seconds_total` | Total CPU time spent on garbage collection. High values indicate excessive GC activity. For example, if this is increasing rapidly, it may indicate memory pressure in the Go runtime. | `rate(go_cpu_classes_gc_total_cpu_seconds_total[5m]) > 1` | Medium |
| `go_cpu_classes_scavenge_assist_cpu_seconds_total` | Total CPU seconds spent in Go garbage collection scavenge assist operations. This is a Go garbage collection mechanism where application goroutines help with memory cleanup when the garbage collector is overwhelmed. It indicates memory pressure. If no scavenge assist has been needed, this may be zero or absent. | `rate(go_cpu_classes_scavenge_assist_cpu_seconds_total[5m]) > $(scavenge_assist_threshold)` | Low |
| `go_cpu_classes_scavenge_background_cpu_seconds_total` |Total CPU seconds spent in Go garbage collection background scavenging. This is a low-priority garbage collection activity that runs in the background to return unused memory to the operating system without impacting application performance. If no background scavenging has occurred, this may be zero or absent. | `rate(go_cpu_classes_scavenge_background_cpu_seconds_total[5m]) > $(scavenge_background_threshold)` | Low |
| `go_cpu_classes_scavenge_total_cpu_seconds_total` | Total CPU seconds spent in all Go garbage collection scavenging activities. This combines all scavenging activities (assist, background, and other scavenging operations) into a single metric representing the total CPU overhead of memory management. This provides an overall view of garbage collection overhead in cluster components. High total scavenging indicates significant CPU resources are being diverted from VM operations to memory management, potentially impacting cluster performance. | `rate(go_cpu_classes_scavenge_total_cpu_seconds_total[5m]) > $(total_scavenge_threshold)` | Medium |
| `go_gc_cycles_total_gc_cycles_total` | Total number of completed Go garbage collection cycles. A garbage collection cycle is a complete pass through memory to identify and free unused objects. Each cycle represents one complete garbage collection operation. While some GC activity is normal, excessive cycles can indicate memory leaks or inefficient memory usage that could impact VM operation performance. | `rate(go_gc_cycles_total_gc_cycles_total[5m]) > $(gc_cycles_rate_threshold)` | Medium |
| `go_gc_duration_seconds` | Current or most recent Go garbage collection duration in seconds. This measures how long garbage collection operations take to complete. Longer durations indicate more work is needed to clean up memory or that the garbage collector is struggling with memory pressure. There are a lot of objects under this metric. You may want to filter by namespace | `go_gc_duration_seconds{namepsace="default"} > $(gc_duration_threshold)` | Medium |
| `go_gc_duration_seconds_sum` | Cumulative sum of all Go garbage collection durations in seconds. This is a counter that accumulates the total time spent in garbage collection across all cycles. Use with the corresponding count metric to calculate average GC duration over time. | `rate(go_gc_duration_seconds_sum[5m]) / rate(go_gc_duration_seconds_count[5m]) > $(avg_gc_duration_threshold)` | Medium |
| `go_gc_gomemlimit_bytes` | Memory limit for the Go runtime in bytes. Defines the ceiling for Go memory usage. This is a configuration setting that helps understand the memory constraints for the process. | `go_gc_gomemlimit_bytes` | Low |
| `go_gc_heap_allocs_bytes_total` | Total bytes allocated on the Go heap across all garbage collection cycles.  The heap is where Go programs allocate memory for objects. This metric tracks the cumulative amount of memory that has been allocated (not necessarily currently in use). High heap allocation rates indicate intensive memory usage in cluster components. While allocation itself isn't problematic, rapidly increasing allocation can indicate memory leaks or inefficient patterns that may eventually impact VM operations through resource exhaustion. | `rate(go_gc_heap_allocs_bytes_total[5m]) > $(heap_alloc_rate_threshold)` | Low |
| `go_gc_heap_goal_bytes` | Current Go garbage collection heap size goal in bytes. This is the target heap size that Go's garbage collector tries to maintain. The GC triggers more frequently as the heap approaches this goal to keep memory usage under control. If the heap goal for API server components grows significantly, it might indicate increased workload or memory leaks. Tis metric should always have a value in running Go applications, so NaN would indicate a problem with metric collection. | `go_gc_heap_goal_bytes > $(heap_goal_threshold)` | Low |
| `go_gc_heap_live_bytes` | Size of the live objects heap in bytes. High values relative to limits indicate memory pressure. For example, if this approaches the total heap size, it may indicate memory leaks or high memory pressure. | `go_gc_heap_live_bytes / go_memory_classes_total_bytes > 0.8` | Medium |
| `go_godebug_non_default_behavior_panicnil_events_total` | Total number of nil panic events. Any non-zero value indicates programming errors. For example, if this increases, it indicates nil pointer dereferences in the code, which should never happen in production. | `increase(go_godebug_non_default_behavior_panicnil_events_total[15m]) > 0` | Critical |
| `go_memory_classes_metadata_mspan_inuse_bytes` | Size of mspan structures currently in use in bytes. Indicates memory used for internal Go runtime structures. This is primarily useful for detailed Go runtime memory analysis. | `go_memory_classes_metadata_mspan_inuse_bytes` | Low |
| `go_memory_classes_total_bytes` | Total size of Go memory classes in bytes. Indicates overall memory usage by the Go runtime. This helps understand the total memory footprint of the process. | `go_memory_classes_total_bytes` | Medium |
| `group_sync_error` | Indicates errors during group synchronization. May affect cluster consistency in relation to permissions within the cluster. For example, if this increases, it could indicate issues with group membership updates. | `increase(group_sync_error[15m]) > 0` | Medium |
| `grpc_req_panics_recovered_total` | Total number of gRPC request panics recovered. Increasing values indicate gRPC service instability. For example, if this increases rapidly, it may indicate issues with gRPC request handling. | `rate(grpc_req_panics_recovered_total[5m]) > 0` | Medium |
| `haproxy_backend_connection_errors_total` | Total number of connection errors on HAProxy backends. Increasing values indicate issues with backend connectivity. For example, if this increases for a specific backend, it may indicate server or network issues. | `rate(haproxy_backend_connection_errors_total[5m]) > 0` | Medium |
| `haproxy_backend_http_average_response_latency_milliseconds` | Average response latency of HAProxy backend HTTP requests in milliseconds. High values indicate slow backend responses. For example, if this exceeds 500ms for a critical service, it may impact user experience. | `haproxy_backend_http_average_response_latency_milliseconds > 500` | Medium |
| `haproxy_backend_http_responses_total` | Total number of HTTP responses from HAProxy backends. Used to monitor backend request volume. For example, trend analysis is used to determine activity in the cluster. | `rate(haproxy_backend_http_responses_total[5m]) == 0` | Medium |
| `haproxy_backend_up` | Indicates whether HAProxy backends are up (1) or down (0). Values of 0 indicate backend availability issues. For example, if this is 0 for a critical backend, traffic may not be routed correctly. | `haproxy_backend_up == 0` | Critical |
| `haproxy_server_connections_total` | Total number of connections to HAProxy servers. Used to monitor server load. For example, if this increases rapidly, it may indicate a sudden spike in traffic. | `rate(haproxy_server_connections_total[5m]) > 100` | Medium |
| `haproxy_server_downtime_seconds_total` | Total downtime of HAProxy servers in seconds. Increasing values indicate server availability issues. For example, if this increases for a critical server, it may indicate recurring outages. | `increase(haproxy_server_downtime_seconds_total[15m]) > 0` | Medium |
| `haproxy_server_up` | Indicates whether HAProxy servers are up (1) or down (0). Values of 0 indicate server availability issues. For example, if this is 0 for a critical server, it may indicate a server failure. | `haproxy_server_up == 0` | Critical |
| `haproxy_up` | Indicates whether HAProxy is up (1) or down (0). Values of 0 indicate HAProxy service issues. For example, if this is 0, the HAProxy service is not running or not reachable. | `haproxy_up == 0` | Critical |
| `http_client_request_total` | Total number of HTTP client requests. Used to monitor client request volume. For example, if this drops suddenly, it may indicate client connectivity issues. | `rate(http_client_request_total[5m]) == 0` | Medium |
| `instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile` | Shows distribution of time etcd takes to commit transactions to disk. Fast commits are crucial for VM operations because every VM state change must be durably stored. Slow commits cause VM operations to feel sluggish and can lead to timeouts during VM lifecycle operations. Example: 95th percentile over 10ms indicates storage problems. | `histogram_quantile(0.95, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) > 10` | Critical |
| `instance:etcd_mvcc_db_total_size_in_bytes:sum` | This is the actual space allocated on disk for the etcd database, including both live data and unused/free space left after compaction. If this grows rapidly, it may indicate excessive churn (e.g., frequent VM creation/deletion) or that compaction/defragmentation is not keeping up. If it approaches the storage quota, etcd may reject writes, impacting VM and cluster operations.**Example:** If your etcd quota is 8GB and this metric is 7.5GB, you are at risk of cluster instability. | `sum(etcd_mvcc_db_total_size_in_bytes)` | High |
| `instance_request_kind:apiserver_current_inflight_requests:sum` | Sum of concurrent requests to the API server by request kind. High values indicate API server congestion. If this exceeds 100 for mutating requests, the API server may be overloaded. | `instance_request_kind:apiserver_current_inflight_requests:sum{request_kind="mutating"} > 100` | High |
| `kube_cronjob_spec_suspend` | Indicates whether a CronJob is suspended. Impact: Suspended CronJobs will not run scheduled tasks. If this is True for a critical CronJob, scheduled tasks may not execute. | `kube_cronjob_spec_suspend == "True"` | Medium |
| `kube_deployment_status_condition` | Status conditions of Deployments. Indicates health and readiness of Deployments. If a Deployment shows a False Ready condition, pods may not be available for traffic. | `kube_deployment_status_condition{condition="Available", status="false"} == 1` | Medium |
| `kube_deployment_status_replicas_available` | Number of available replicas in a Deployment. Used to monitor Deployment health. If this is less than the desired number, po{"ds may not be fully available. | `kube_deployment_status_replicas_available{deployment="apiserver"} <3` | Medium |
| `kube_job_status_failed` | Indicates failed Jobs. May indicate issues with Job execution or configuration. If this increases for a critical Job, it may indicate recurring failures. | `kube_job_status_failed == 1` | Medium |
| `kubelet_memory_manager_pinning_errors_total` | Total number of memory pinning errors by the Kubelet memory manager. Indicates issues with memory allocation. If memory pinning fails, workloads requiring guaranteed memory allocation may experience higher latency and decreased performance, especially in NUMA architectures. This metric should ideally always be 0 | `rate(kubelet_memory_manager_pinning_errors_total[5m]) > 0` | Medium |
| `kubelet_pleg_last_seen_seconds` | Time in seconds since the last PLEG (Pod Lifecycle Event Generator) update. High values indicate Kubelet issues. If this exceeds 60 seconds, it may indicate Kubelet is not updating pod status correctly. | `kubelet_pleg_last_seen_seconds > 60` | Medium |
| `kubelet_pod_start_sli_duration_seconds_bucket` | Histogram of pod start latency in seconds. High values indicate slow pod startup times. If the 95th percentile exceeds 30 seconds, pod startup may be slow, affecting service availability. | `histogram_quantile(0.95, sum(rate(kubelet_pod_start_sli_duration_seconds_bucket[5m])) by (le))` | Medium |
| `kubelet_volume_stats_available_bytes` | Available bytes in a volume. Used to monitor volume capacity and detect potential storage issues. If available bytes are low, it may indicate that the volume is running out of space. | `kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.2` | Medium |
| `kubelet_volume_stats_inodes_free` | Number of free inodes in a volume. Low values indicate potential inode exhaustion. If free inodes are less than 5% of total inodes, it may indicate inode exhaustion issues. | `kubelet_volume_stats_inodes_free / kubelet_volume_stats_inodes < 0.05` | Medium |
| `kube_pod_status_ready` | Indicates whether a pod is ready to serve requests. Non-ready pods may not receive traffic. If a critical pod is not ready, it may indicate issues with the pod or its containers. | `kube_pod_status_ready{namespace="default", pod="critical-pod"} == 0` | High |
| `kube_pod_status_unschedulable` | Indicates whether a pod is unschedulable. Unschedulable pods cannot be placed on nodes. If a pod is unschedulable due to resource constraints, it may indicate the need for node scaling. | `kube_pod_status_unschedulable{namespace="default", pod="critical-pod"} == 1` | Medium |
| `kube_running_pod_ready` | Number of running pods that are ready. Used to monitor pod health and availability. If this drops suddenly, it may indicate pod failures or readiness issues. | `kube_running_pod_ready{namespace="default"} < 10` | Medium |
| `kubevirt_hco_hyperconverged_cr_exists` | Indicates whether the Hyperconverged Custom Resource exists. Non-existent CRs may affect hyperconverged functionality. If this is False, it may indicate issues with hyperconverged deployment or configuration. | `kubevirt_hco_hyperconverged_cr_exists == 0` | Critical |
| `kubevirt_hco_system_health_status` | Numeric health status of the Hyperconverged Operator (1 = healthy, 0 = not healthy). A value of 0 indicates issues with hyperconverged components that may affect virtualization functionality. If this metric returns 0, the HCO is reporting a problem requiring investigation. The HyperConverged CR creates corresponding CRs for the operators of all other components related to Compute, storage, networking, templating and scaling within OpenShift Virtualization. | `kubevirt_hco_system_health_status == 0` | Critical |
| `kubevirt_hyperconverged_operator_health_status` | Health status of the Hyperconverged Operator. Non-healthy status may indicate issues with hyperconverged components. If this indicates a degraded status, it may affect hyperconverged functionality. | `kubevirt_hyperconverged_operator_health_status != 0` | High |
| `kubevirt_number_of_vms` | Number of Virtual Machines managed by KubeVirt. Used to monitor VM deployment and scaling. If this drops suddenly, it may indicate VM termination or deployment issues. | `kubevirt_number_of_vms{namespace="default"} < 10` | Medium |
| `kubevirt_ssp_operator_up` | Indicates whether the SSP (Specialized Service Provider) operator, a core component of OpenShift Virtualization, is up and running. If not running (`0`), key virtualization features such as VM templates, common templates, and data import will not function. | `kubevirt_ssp_operator_up == 0` | Critical |
| `kubevirt_virt_api_up` | The number of virt-api pods that are up (not a binary indicator). The virt-api service is the main API endpoint for KubeVirt operations, handling all VM lifecycle requests. Multiple pods provide redundancy and high availability. | `kubevirt_virt_api_up < 1` | Critical |
| `kubevirt_virt_controller_up` | Number of virt-controller pods currently running. The virt-controller manages VM lifecycle operations (creation, migration, deletion). Multiple pods provide redundancy. A value matching your configured replica count (typically 2) indicates healthy operations. If this drops to 0, all VM operations will fail. | `kubevirt_virt_controller_up < 1` | Critical |
| `kubevirt_virt_handler_up` | Number of virt-handler pods currently running. Impact: Each node requires one virt-handler pod to manage VM operations. A value matching node count indicates full coverage. If lower, affected nodes can't manage VMs. | `kubevirt_virt_handler_up < count(kube_node_role{role="worker"})` | Critical |
| `kubevirt_virt_operator_up` | The number of virt-operator pods that are up. If this drops below 1 the operator will cease to function and changes will not be reconciled. | `kubevirt_virt_operator_up < 1` | Critical |
| `kubevirt_vmi_cpu_usage_seconds_total` | Total CPU usage of Virtual Machine Instances (VMIs) in seconds. High values indicate CPU resource constraints. If CPU usage is consistently high, it may indicate the need for VM scaling or resource adjustments. | `rate(kubevirt_vmi_cpu_usage_seconds_total[5m]) > 100` | Medium |
| `kubevirt_vmi_memory_available_bytes` | Available memory for Virtual Machine Instances (VMIs) in bytes. Low values indicate memory constraints. If available memory is low, it may indicate that VMIs are experiencing memory pressure. | `kubevirt_vmi_memory_available_bytes{namespace="default"} < 1e9` | Medium |
| `kubevirt_vmi_memory_pgmajfault_total` | Total number of major page faults for Virtual Machine Instances (VMIs). High values indicate memory pressure. If major page faults are increasing, it may indicate that VMIs are experiencing memory constraints. | `rate(kubevirt_vmi_memory_pgmajfault_total[5m]) > 0` | Medium |
| `kubevirt_vmi_memory_swap_in_traffic_bytes` | Swap-in traffic for Virtual Machine Instances (VMIs) in bytes. High values indicate memory pressure. Swap-in refers to the process of moving data from swap space (usually disk storage) back into the VM's RAM | `rate(kubevirt_vmi_memory_swap_in_traffic_bytes[5m]) > 100` | Medium |
| `kubevirt_vmi_memory_swap_out_traffic_bytes` | Swap-out traffic for Virtual Machine Instances (VMIs) in bytes. High values indicate memory pressure. Sawp-out refers to the process of moving data from ram to swap space. If swap-out traffic is high, it may indicate that VMIs are experiencing memory constraints. | `rate(kubevirt_vmi_memory_swap_out_traffic_bytes[5m]) > 100` | Medium |
| `loki_query_frontend_shards_total` | Number of active query shards in the Loki query frontend. In OpenShift Virtualization, large log volumes from VMs, nodes, and hypervisors demand efficient log query parallelization. Too few shards can bottleneck query performance, making VM troubleshooting or compliance investigations slow. Too many shards can cause overhead and resource waste. Monitoring this helps ensure log query responsiveness for critical virtualization events. | `loki_query_frontend_shards_total` | Medium |
| `loki_runtime_config_last_reload_successful` | Indicates if the last Loki configuration reload was successful (1 for success, 0 for failure). Failed reloads may affect log collection patterns for VM logs, reducing visibility into virtualization problems. | `loki_runtime_config_last_reload_successful == 0` | Medium |
| `loki_store_series_total` | Total number of series in Loki's store. In virtualization environments, the number of log streams grows significantly with each VM, potentially causing high cardinality problems affecting query performance and storage. Monitoring this metric can help identify if VM logging is generating too many unique log streams, which could impact the performance of the entire logging stack. | `loki_store_series_total > 1000` | Medium |
| `loki_tsdb_build_index_last_successful_timestamp_seconds` | Timestamp of the last successful TSDB index build in seconds. Impact: Outdated indexes may affect ability to search VM logs, hampering troubleshooting of virtualization issues. If this timestamp is stale (more than an hour old), recent VM migration or error logs may not be searchable. | `time() - loki_tsdb_build_index_last_successful_timestamp_seconds > 3600` | Medium |
| `loki_write_failures_discarded_total` | Total count of log entries discarded due to write failures. Impact: May result in missing VM logs critical for troubleshooting virtualization issues and maintaining compliance. If increasing rapidly during VM migration activities, important logs about VM state transitions may be missing, making it difficult to diagnose failed migrations. | `increase(loki_write_failures_discarded_total[1h]) > 0` | High |
| `mapi_machinehealthcheck_short_circuit` | Indicates if machine health checks are short-circuited (disabled). When enabled (1), prevents automatic remediation of unhealthy nodes hosting VMs, potentially extending VM outages. During planned maintenance, this might be enabled, but if left enabled accidentally, it could prevent automatic recovery of nodes hosting critical VMs. | `mapi_machinehealthcheck_short_circuit > 0` | Medium |
| `mapi_mao_collector_up` | Indicates if the Machine API Operator collector is up (1) or down (0). When down, affects machine health monitoring for nodes hosting VMs, potentially leaving hardware issues undetected. If down, problems with worker nodes hosting VMs may go undetected. | `mapi_mao_collector_up == 0` | High |
| `mcd_reboots_failed_total` | Total count of failed node reboots during machine config updates. May leave nodes in inconsistent states with partial updates to virtualization components. If a node fails to reboot during an update, VMs scheduled on that node may experience stability issues or incompatibility with the rest of the cluster. | `increase(mcd_reboots_failed_total[1h]) > 0` | High |
| `mco_degraded_machine_count` | Count of machines in a degraded state according to the Machine Config Operator. Degraded machines may host VMs with reduced performance or with outdated virtualization components. If nodes hosting VMs are degraded, virtualization features like live migration may be compromised due to inconsistent configurations. | `mco_degraded_machine_count > 0` | High for Control Plane <br><br> Medium for Workers|
| `mco_unavailable_machine_count` | Count of unavailable machines according to the Machine Config Operator. Unavailable machines cannot host VMs, reducing total virtualization capacity. | `mco_unavailable_machine_count > 0` | High for Control Plane <br><br> Medium for Workers |
| `node_cpu_core_throttles_total` | Total number of CPU throttling events on a node. May indicate CPU contention or thermal issues affecting VM performance and stability. High throttling can cause VM workloads to experience unpredictable performance, especially for latency-sensitive applications, and may indicate that a node is oversubscribed or experiencing cooling problems. | `rate(node_cpu_core_throttles_total[5m]) > 10` | Medium |
| `node_edac_csrow_uncorrectable_errors_total` | Total uncorrectable memory errors at CSROW (Chip-Select Row) level. May indicate hardware memory issues that could lead to VM crashes or corruption. Increasing errors may precede host crashes affecting running VMs, or cause silent data corruption inside VM memory that could affect application integrity. | `increase(node_edac_csrow_uncorrectable_errors_total[1h]) > 0` | Critical |
| `node_edac_uncorrectable_errors_total` | Total uncorrectable memory errors across the system. May indicate hardware memory issues that could cause VM crashes, data corruption, or host failures. Even a single uncorrectable memory error could potentially crash VMs or cause data corruption within VM memory, making this a critical metric to monitor on virtualization hosts. | `increase(node_edac_uncorrectable_errors_total[1h]) > 0` | Critical |
| `node_fibrechannel_link_failure_total` | Total Fibre Channel link failures detected. May affect VM access to storage on SAN, causing I/O errors for VMs using FC-based persistent volumes. If increasing, VMs using Fibre Channel storage may experience I/O errors. | `increase(node_fibrechannel_link_failure_total[1h]) > 0` | High |
| `node_fibrechannel_loss_of_signal_total` | Total Fibre Channel signal loss events. May affect VM access to storage on SAN, causing I/O timeouts for VMs using FC storage. If increasing, VMs using Fibre Channel storage may experience I/O timeouts. | `increase(node_fibrechannel_loss_of_signal_total[1h]) > 0` | High |
| `node_fibrechannel_loss_of_sync_total` | Total Fibre Channel synchronization loss events. May affect VM access to storage on SAN, causing intermittent I/O issues. If increasing, VMs using Fibre Channel storage may experience intermittent I/O issues, leading to application performance degradation or increased latency. | `increase(node_fibrechannel_loss_of_sync_total[1h]) > 0` | High |
| `node_filesystem_avail_bytes` | Available bytes in the node's filesystem. Low values may affect local VM storage, container images, and ephemeral disks used by VMs. If a node runs out of filesystem space, VM creation may fail, ephemeral disks may become full causing VM application errors, and VM image pulling may fail. | `node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"} < 0.10` | High |
| `node_filesystem_free_bytes` | Free space in bytes in the node's filesystem. Critical for VM storage operations, as low free space can prevent VM creation, snapshot operations, and disk image transfers. VM disk I/O performance also degrades as filesystems approach capacity, affecting running workloads. | `node_filesystem_free_bytes{mountpoint="/var"} / node_filesystem_size_bytes{mountpoint="/var"} < 20` | Critical |
| `node_hwmon_temp_crit_celsius` | Critical temperature threshold for hardware components in Celsius. Helps determine thermal headroom before VM workloads are affected by throttling. When planning to deploy VMs with high CPU utilization patterns (ML workloads, database servers), compare current temperature against this critical threshold to ensure adequate thermal margin. | `(node_hwmon_temp_celsius / node_hwmon_temp_crit_celsius) > 0.8` | Medium |
| `node_infiniband_link_downed_total` | Total number of times an InfiniBand link went down. InfiniBand is often used for high-performance VM workloads requiring low-latency networking. Link failures directly impact VM network performance and can cause application errors within VMs. | `increase(node_infiniband_link_downed_total[1h]) > 0` | High |
| `node_infiniband_port_errors_received_total` | Total count of InfiniBand port errors received. Affects network reliability for VM workloads using InfiniBand for high-performance computing or storage access. Even small error rates can degrade performance of latency-sensitive VM applications. If this metric increases for nodes hosting database VMs using InfiniBand for storage access, those VMs may experience intermittent I/O errors leading to database corruption or performance issues. | `rate(node_infiniband_port_errors_received_total[5m]) > 0` | Medium |
| `node_memory_CommitLimit_bytes` | System-wide kernel configuration indicating the theoretical maximum memory allocation limit (physical RAM + swap × overcommit ratio). For OpenShift Virtualization, this metric has minimal impact on actual VM placement decisions, as scheduling is based on physical RAM and memory requests, not the commit limit. | `node_memory_CommitLimit_bytes / (1024*1024*1024)` | Low |
| `node_memory_MemFree_bytes` | Amount of physical RAM left unused by the system. Directly affects VM placement, migration capabilities, and performance under memory pressure. Low free memory can trigger swapping which severely impacts VM performance. Existing VMs may experience performance degradation as the hypervisor struggles to allocate memory pages. <br> <b>NOTE:</b> this does not take into account buffers, cache or overall available memory!| `node_memory_MemFree_bytes / node_memory_MemTotal_bytes` | Medium-Low |
| `node_netstat_Ip6_InOctets` | Total number of IPv6 octets (bytes) received. Measures IPv6 network throughput to nodes hosting VMs. When running dual-stack VMs, unexpected increases may indicate IPv6-specific traffic that could impact network performance. | `rate(node_netstat_Ip6_InOctets[5m])` | Low |
| `node_netstat_Ip6_OutOctets` | Total number of IPv6 octets (bytes) sent. Measures IPv6 network throughput from nodes hosting VMs. For VMs serving content over IPv6, this metric helps identify if network bandwidth is sufficient for the workload or if IPv6 traffic patterns differ from IPv4. | `rate(node_netstat_Ip6_OutOctets[5m])` | Low |
| `node_netstat_IpExt_InOctets` | Total number of IPv4 octets (bytes) received. Measures inbound network throughput to nodes hosting VMs. Critical for monitoring network-intensive VM workloads and identifying potential bandwidth constraints. If this metric approaches physical NIC capacity, VMs may experience network contention, packet drops, or increased latency, especially for VMs running streaming or real-time communication services. | `rate(node_netstat_IpExt_InOctets[5m]) / (1024 * 1024) > 100` | Medium |
| `node_netstat_IpExt_OutOctets` | Total number of IPv4 octets (bytes) sent. Measures outbound network throughput from nodes hosting VMs. Critical for VMs serving content or performing data transfers. | `rate(node_netstat_IpExt_OutOctets[5m]) / (1024 * 1024) > 1250` (for a 10g nic or 3150 for 25G nic)| Medium |
| `node_netstat_Tcp_ActiveOpens` | Total number of active TCP connection openings. Measures the rate of new outbound connections from VMs. Useful for identifying abnormal connection patterns or potential connection storms from VM workloads. A sudden spike in this metric may indicate a VM is performing aggressive outbound connection attempts, which could be a sign of compromised VM, misconfigured application, or DoS attack originating from within a VM. | `rate(node_netstat_Tcp_ActiveOpens[5m]) > 1000` | Medium |
| `node_network_receive_packets_total` | Total count of network packets received. Measures network load in packets rather than bytes. VMs running packet-processing workloads may hit packet processing limits before bandwidth limits. High packet rates can cause CPU saturation on the host, affecting all VMs. | `rate(node_network_receive_packets_total{device!="lo"}[5m]) > 100000` | Medium |
| `node_network_up` | Binary metric indicating if a network device is up (1) or down (0). Network interface failures directly impact VM connectivity. If this metric shows 0 for a network interface used by VM secondary NICs (like SR-IOV or bridge networks), those VMs will lose connectivity on that interface, potentially breaking application clustering or storage connectivity. | `node_network_up{device!="lo"} == 0` | Critical |
| `node_nfsd_connections_total` | Total count of NFS connections. Important if VMs are using NFS-based storage for their disks. NFS connection saturation can cause VM I/O operations to stall, leading to apparent VM freezes. If this metric approaches the maximum supported NFS connections on a node providing NFS storage for VM disks, VMs may experience slow I/O operations or apparent freezes as their disk operations queue. | `node_nfsd_connections_total` | Medium |
| `node_nfsd_rpc_errors_total` | Total count of NFS Remote Procedure Call (RPC) errors. Critical for VMs using NFS-backed storage. RPC errors can cause I/O errors within VMs, leading to filesystem corruption or application failures inside the VM. | `increase(node_nfsd_rpc_errors_total[15m]) > 0` | High |
| `node_role_os_version_machine:cpu_capacity_cores:sum` | Aggregated sum of CPU cores capacity across nodes, broken down by node role, OS version, and machine type. Critical for VM capacity planning and placement, as different classes of nodes may be designated for specific VM workloads. Insufficient CPU capacity directly impacts VM density and performance. | `node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io="worker"}` | High |
| `node_thermal_zone_temp` | Zone temperature in Celsius for node hardware components. If temperatures approach critical thresholds on compute nodes running VMs, performance degradation from thermal throttling may occur, causing inconsistent VM performance or unexpected VM migrations. | `max(node_thermal_zone_temp) by (instance) > 80` | High |
| `node_vmstat_oom_kill` | Count of Out-of-Memory kill events from `/proc/vmstat`. OOM events on nodes running OpenShift Virtualization are catastrophic as they can terminate the virt-launcher pods running VMs, causing immediate VM termination and potential data corruption.  | `increase(node_vmstat_oom_kill[15m]) > 0` | Critical |
| `node_watchdog_bootstatus` | Hardware watchdog boot status indicator. In virtualization environments, hardware watchdog failures may indicate underlying hardware issues on VM host nodes that could lead to unexpected reboots and VM outages. | `node_watchdog_bootstatus > 0` | Medium |
| `node_watchdog_timeleft_seconds` | Represents seconds remaining before the hardware watchdog timer would trigger a reboot if not reset. In properly functioning systems, this value is typically constant as the watchdog is continuously being reset. **This is NOT an indicator of node health problems when stable at a low value.** If this value remains stable across all nodes, this indicates normal operation where the watchdog is being reset regularly by the kernel. Only a decreasing value without reset would indicate a problem. | `rate(node_watchdog_timeleft_seconds[5m])` | Low |
| `openshift_auth_form_password_count_result` | Count of web console login attempts by result. Multiple failed logins from the same source may indicate attempts to access the virtualization console to manipulate VMs or their configurations. | `sum(rate(openshift_auth_form_password_count_result{result="error"}[15m])) > 5` | Medium |
| `openshift:cpu_usage_cores:sum` | Total CPU usage by OpenShift infrastructure components. During VM migration or creation operations, if this value spikes significantly, platform components may be consuming CPU resources needed for VM operations, causing delays or performance issues. | `openshift:cpu_usage_cores:sum / on() group_left() sum(cluster:capacity_cpu_cores:sum) > 0.3` | Medium |
| `openshift_etcd_operator_signer_expiration_days` | Days until the etcd operator signing certificates expire. Certificate expiration could disrupt etcd, which would affect all VM control operations and potentially make the virtualization platform unavailable. | `openshift_etcd_operator_signer_expiration_days < 14` | High |
| `openshift:memory_usage_bytes:sum` | Total memory usage by OpenShift infrastructure components. If memory usage by platform components is high on nodes intended for VM workloads, VM density will be reduced, and memory-intensive VMs may experience performance degradation or fail to be scheduled. | `openshift:memory_usage_bytes:sum / on() group_left() sum(cluster:capacity_memory_bytes:sum) > 0.3` | Medium |
| `operation:etcd_request_duration_seconds_bucket:rate1m` | Rate of request duration histogram buckets over 1 minute. This shows how frequently requests fall into each duration bucket, giving a short-term performance view. Useful for detecting sudden performance degradation in etcd that could impact VM operations.**NaN:** If no requests in the last minute, may be NaN. | `histogram_quantile(0.95, operation:etcd_request_duration_seconds_bucket:rate1m) > $(request_duration_short_window)` | Medium |
| `operation:etcd_request_duration_seconds_bucket:rate5m` | Rate of request duration histogram buckets over 5 minutes. Similar to the above but for a longer window, smoothing out short-term spikes and showing sustained performance trends. | `histogram_quantile(0.95, operation:etcd_request_duration_seconds_bucket:rate5m) > $(request_duration_long_window)` | Medium |
| `ovn_controller_bridge_mappings` | Status metric for OVN controller bridge mappings configuration. In OpenShift Virtualization, bridge mappings are critical for connecting VMs to physical networks, especially for VMs requiring direct layer 2 connectivity. If bridge mappings are misconfigured or unavailable, VMs with attachments to secondary networks or requiring physical network access may lose connectivity or fail to start. | `ovn_controller_bridge_mappings == 0` | High |
| `ovn_controller_ct_zone_commit_95th_percentile` | 95th percentile time taken to commit connection tracking zones in OVN. If this value is high, VMs with high network traffic may experience connection tracking issues, packet drops, or unexpected connection resets, especially during traffic spikes. | `ovn_controller_ct_zone_commit_95th_percentile > 100` | Medium |
| `ovn_controller_if_status_mgr_run_maximum` | Maximum execution time for the OVN interface status manager. If this value is high, detection and recovery of network interface failures for VMs may be delayed, extending network outages for virtualized workloads. | `ovn_controller_if_status_mgr_run_maximum > 200` | Medium |
| `ovn_controller_lflow_run` | Execution time metrics for logical flow processing in OVN. Logical flows control VM network traffic paths, and slow processing delays network policy and route updates. During network policy updates affecting VMs, slow logical flow processing could delay the application of security rules, temporarily leaving VMs with incorrect network access. | `increase(ovn_controller_lflow_run[5m]) > 1000` | Medium |
| `ovn_controller_monitor_all` | Status of OVN controller monitoring processes. If this metric indicates monitoring is failing, changes to VM network configuration may not be detected or applied, leading to inconsistent network behavior for VMs. | `ovn_controller_monitor_all == 0` | High |
| `ovn_controller_southbound_database_connected` | Binary status indicating if OVN controller is connected to the southbound database (1 = connected, 0 = disconnected). If this value is 0, the node's OVN controller cannot communicate with the central OVN database, preventing VM network configuration updates and potentially isolating VMs on that node. | `ovn_controller_southbound_database_connected == 0` | Critical |
| `ovn_controller_txn_error` | Count of OVN controller transaction errors. If transaction errors increase during VM creation or migration, the associated network setup may fail, leaving VMs with incorrect or missing network connectivity. | `increase(ovn_controller_txn_error[5m]) > 0` | High |
| `ovn_db_e2e_timestamp` | End-to-end timestamp information for OVN database operations. If the difference between current time and this timestamp increases, it indicates OVN database operation delays that could slow VM network provisioning or policy updates. | `time() - ovn_db_e2e_timestamp > 70`| Medium |
| `ovnkube_clustermanager_allocated_v4_host_subnets` | Number of IPv4 host subnets allocated in the OVN cluster. Determines availability of IPv4 subnets for VM placement. This should be compared against the expected number of subnets per cluster. | `ovnkube_clustermanager_allocated_v4_host_subnets` | Medium |
| `ovnkube_clustermanager_allocated_v6_host_subnets` |  Number of IPv4 host subnets allocated in the OVN cluster. Determines availability of IPv4 subnets for VM placement. This should be compared against the expected number of subnets per cluster. | `ovnkube_clustermanager_allocated_v6_host_subnets` | Medium |
| `ovnkube_controller_ipsec_enabled` | Binary indicator whether IPsec encryption is enabled for OVN network traffic (1=enabled, 0=disabled). Affects security of VM-to-VM traffic across nodes. Critical for multi-tenant virtualization where network traffic isolation is required. If this metric shows 0 when IPsec should be enabled, VM network traffic may be unencrypted, potentially exposing sensitive data in multi-tenant virtualization environments. | `ovnkube_controller_ipsec_enabled == 0` | High |
| `ovnkube_controller_pod_port_binding_chassis_port_binding_up_duration_seconds_bucket` | Histogram of time taken for OVN port bindings to become active. Directly affects how quickly VM network interfaces become available after VM creation or migration. If the 95th percentile exceeds 5 seconds, VM creation operations may take longer than expected, particularly affecting VM migration time and application availability during migrations. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_pod_port_binding_chassis_port_binding_up_duration_seconds_bucket[5m])) by (le)) > 5` | Medium |
| `ovnkube_controller_resource_add_latency_seconds_bucket` | Histogram of latency for adding network resources. Affects how quickly VM network resources (interfaces, IPs) are provisioned, directly impacting VM creation time. If the 95th percentile exceeds 2 seconds, VM network interfaces may be slow to initialize, affecting VM startup time and orchestration workflows. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_resource_add_latency_seconds_bucket[5m])) by (le, resource)) > 2` | Medium |
| `ovnkube_controller_resource_add_latency_seconds_count` | Count of network resource addition operations. High values indicate network resource churn which may affect OVN controller performance for all VM operations. During large-scale VM creation or migration operations, a high rate can indicate high controller load that might impact other network operations. | `rate(ovnkube_controller_resource_add_latency_seconds_count[5m]) > 10` | Low |
| `ovnkube_controller_resource_add_latency_seconds_sum` | Sum of latencies for adding network resources. Used with count to calculate average latency. High values indicate network control plane performance issues. When divided by the count, an increasing average latency for adding VM network resources may indicate control plane bottlenecks affecting VM deployment speed. | `rate(ovnkube_controller_resource_add_latency_seconds_sum[5m]) / rate(ovnkube_controller_resource_add_latency_seconds_count[5m]) > 1` | Medium |
| `ovnkube_controller_resource_delete_latency_seconds_bucket` | Histogram of latency for deleting network resources. Affects how quickly VM network resources are cleaned up after VM deletion or migration. If the 95th percentile exceeds 2 seconds, resources may not be cleaned up quickly, potentially affecting subsequent VM migrations to the same node or IP address reuse. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_resource_delete_latency_seconds_bucket[5m])) by (le, resource)) > 2` | Medium |
| `ovnkube_controller_resource_delete_latency_seconds_count` | Count of network resource deletion operations. High values indicate network resource cleanup activity which may affect controller performance. | `rate(ovnkube_controller_resource_delete_latency_seconds_count[5m]) > 10` | Low |
| `ovnkube_controller_resource_delete_latency_seconds_sum` | Sum of latencies for deleting network resources. Used with count to calculate average latency. High values indicate network control plane cleanup issues. When divided by the count, an increasing average latency for removing VM network resources may indicate control plane bottlenecks affecting resource cleanup. | `rate(ovnkube_controller_resource_delete_latency_seconds_sum[5m]) / rate(ovnkube_controller_resource_delete_latency_seconds_count[5m]) > 1` | Medium |
| `ovnkube_controller_resource_update_latency_seconds_bucket` | Histogram of latency for updating network resources. Affects how quickly VM network configuration changes take effect, including policy updates or interface reconfiguration. If the 95th percentile exceeds 2 seconds, network policy updates for VMs might be delayed, potentially leaving security gaps during policy transitions. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_resource_update_latency_seconds_bucket[5m])) by (le, resource)) > 2` | Medium |
| `ovnkube_master_libovsdb_disconnects_total` | Total count of disconnections from the OVS database. Database disconnections can disrupt VM networking and prevent network configuration changes from being applied. If this metric increases rapidly, the OVN control plane is experiencing database connectivity issues that may prevent VM network changes from being applied correctly. | `increase(ovnkube_master_libovsdb_disconnects_total[15m]) > 0` | High |
| `ovn_northd_nb_connection_status` | Status of the northbound database connection (1=connected, 0=disconnected). If this metric is 0, the OVN northd service cannot communicate with the configuration database, preventing all VM network changes from being processed. | `ovn_northd_nb_connection_status == 0` | Critical |
| `ovn_northd_ovnsb_db_run_maximum` | Maximum runtime for OVN Southbound database operations. If this value is high, VM network configuration changes may take longer to propagate, affecting the responsiveness of VM network management operations. | `ovn_northd_ovnsb_db_run_maximum > 1` | Medium |
| `ovn_northd_sb_connection_status` | Status of the southbound database connection (1=connected, 0=disconnected). If this metric is 0, the OVN northd service cannot communicate with the implementation database, preventing VM network configurations from reaching the hosts. | `ovn_northd_sb_connection_status == 0` | Critical |
| `ovn_northd_status` | Overall status of the OVN northd service (1=running, 0=not running). If this metric is 0, the OVN northd service is not running, which will prevent all VM network operations including creation, updates, and policy enforcement. | `ovn_northd_status == 0` | Critical |
| `ovs_vswitchd_bridge` | Metric related to OVS bridges in OpenShift, which are foundational for VM networking. In OpenShift, there are typically two critical bridges: `br-int` (handling internal pod/VM traffic) and `br-ex` (gateway for external traffic). These bridges are essential for VM network connectivity - `br-int` connects VM interfaces within the cluster, while `br-ex` enables external access. Bridge failures would isolate VMs from other workloads or external networks. In a healthy OpenShift Virtualization environment, this metric should show both `br-int` and `br-ex` bridges. If either bridge is missing or reports an abnormal state, VMs may have partial or complete network failure, especially affecting external connectivity and inter-node VM communication. | `ovs_vswitchd_bridge{name="br-int"} != 1 or ovs_vswitchd_bridge{name="br-ex"} != 1` | Critical |
| `ovs_vswitchd_interface_tx_dropped_total` | Total number of transmitted packets dropped by OVS interfaces. High drop rates would cause VM applications to experience slow or failed outbound connections, affecting service reliability, especially for VMs serving external clients. | `rate(ovs_vswitchd_interface_tx_dropped_total[5m]) > 100` | High |
| `ovs_vswitchd_ofproto_packet_out` | Count of OpenFlow packet-out operations in OVS. These operations manually inject packets into the switch and abnormal values may indicate OVS controller issues that could affect VM network policy enforcement.| `rate(ovs_vswitchd_ofproto_packet_out[5m]) > 10` | Medium |
| `ovs_vswitchd_packet_in` | Count of packet-in operations where the switch sends packets to the OpenFlow controller for handling. Spikes during VM creation or migration are normal, but sustained high values could indicate network control plane issues that consume resources needed by VMs. | `rate(ovs_vswitchd_packet_in[5m]) > 100` | Medium |
| `ovs_vswitchd_txn_error` | Count of transaction errors in OVS. During network policy updates for VMs, transaction errors could prevent security rules from being applied, leaving VMs with incorrect access controls or connectivity issues. | `increase(ovs_vswitchd_txn_error[15m]) > 0` | High |
| `ovs_vswitchd_upcall_flow_limit_kill` | Count of flows killed due to upcall limits in OVS. If this increases during high VM network activity, it may indicate that the flow table capacity is insufficient for the VM workload, causing connection failures or degraded network performance. | `increase(ovs_vswitchd_upcall_flow_limit_kill[5m]) > 0` | High |
| `prometheus_notifications_dropped_total` | Total count of alert notifications that Prometheus has failed to send to Alertmanager. In OpenShift Virtualization, dropped notifications represent alerts about VM and infrastructure issues that never reach notification systems. Common causes include network issues between Prometheus and Alertmanager, Alertmanager unavailability, or queue overflow due to alert storms.| `increase(prometheus_notifications_dropped_total[15m]) > 0` | High |
| `prometheus_ready` | Binary indicator of whether Prometheus is ready (1) or not (0). If this is 0, all VM monitoring would be unavailable, making it impossible to detect issues with VMs, hosts, or networking components, potentially leading to undetected outages. | `prometheus_ready == 0` | Critical |
| `prometheus_rule_evaluation_failures_total` | Count of Prometheus rule evaluation failures. If VM over-commitment rules fail to evaluate, operators may not receive alerts when hosts become overloaded, potentially causing VM performance degradation before manual detection. | `increase(prometheus_rule_evaluation_failures_total[15m]) > 0` | High |
| `prometheus_sd_dns_lookup_failures_total` | Number of DNS lookup failures during service discovery. DNS resolution failures can prevent discovery of virtualization components, causing gaps in monitoring data for VM workloads. | `increase(prometheus_sd_dns_lookup_failures_total[15m]) > 0` | Medium |
| `prometheus_target_scrape_pool_exceeded_target_limit_total` | Count of times when scrape targets exceeded the configured limit. In large clusters with many VMs, hitting target limits could cause monitoring gaps where some virtual machines have no metrics collected, creating blind spots in operations. | `increase(prometheus_target_scrape_pool_exceeded_target_limit_total[15m]) > 0` | Medium |
| `prometheus_target_scrape_pools_failed_total` | Count of scrape pool failures. Failed scrape pools can result in missing metrics from entire classes of virtualization components. | `increase(prometheus_target_scrape_pools_failed_total[15m]) > 0` | High |
| `prometheus_target_sync_failed_total` | Count of failures when syncing targets from discovery results. Sync failures may result in stale or missing monitoring targets for VM components. | `increase(prometheus_target_sync_failed_total[15m]) > 0` | Medium |
| `prometheus_tsdb_head_truncations_failed_total` | Count of failed TSDB head truncations in Prometheus. If TSDB truncations fail, Prometheus may run out of storage space and crash, causing complete loss of monitoring for VMs, preventing alerting for critical VM issues. | `increase(prometheus_tsdb_head_truncations_failed_total[5m]) > 0` | High |
| `prometheus_tsdb_wal_corruptions_total` | Count of WAL (Write-Ahead Log) corruptions. Corruptions can lead to data loss for recent VM metrics and potential monitoring system instability. | `increase(prometheus_tsdb_wal_corruptions_total[15m]) > 0` | High |
| `reconstruct_volume_operations_errors_total` | Count of errors during volume reconstruction operations. Critical for VM persistent storage reliability as failed operations may affect volume availability for VMs. | `increase(reconstruct_volume_operations_errors_total[15m]) > 0` | High |
| `rest_client_request_duration_seconds_bucket` | Histogram of REST client request durations. Slow API responses may delay critical VM operations like live migrations, potentially extending maintenance windows or causing application downtime when VMs need to be moved. | `histogram_quantile(0.95, sum(rate(rest_client_request_duration_seconds_bucket[5m])) by (le)) > 1` | Medium |
| `scheduler_pod_scheduling_sli_duration_seconds_bucket` | Histogram of scheduling latency for pods. If the 95th percentile exceeds 5 seconds, VM creation operations might take longer than expected, affecting user experience and migration performance. | `histogram_quantile(0.95, sum(rate(scheduler_pod_scheduling_sli_duration_seconds_bucket[5m])) by (le)) > 5` | Medium |
| `scheduler_unschedulable_pods` | Count of pods that can't be scheduled. Unschedulable VM pods prevent new VMs from starting or existing VMs from migrating, directly affecting virtualization platform capacity. | `scheduler_unschedulable_pods{namespace="openshift-virtualization"} > 0` | High |
| `service_ca_expiry_time_seconds` | The Unix timestamp (in seconds) when the OpenShift Service CA certificate will expire. If the Service CA certificate expires, internal TLS communication between OpenShift components-including those managing and exposing VM services-will fail. This can break service-to-service communication, API access, and disrupt VM management, migrations, and monitoring. | `service_ca_expiry_time_seconds - time() < 604800` (alerts if expiry is within 7 days) | Critical |
| `thanos_alert_sender_alerts_dropped_total` | Count of alerts dropped by Thanos alert sender.  Dropped alerts may mean missing critical notifications about VM health or performance issues. | `increase(thanos_alert_sender_alerts_dropped_total[15m]) > 0` | High |
| `thanos_rule_alertmanagers_dns_failures_total` | Count of DNS failures when connecting to Alertmanager. If DNS failures occur when trying to reach the Alertmanager, critical VM failure or performance alerts may not be delivered, leading to delayed incident response. | `increase(thanos_rule_alertmanagers_dns_failures_total[15m]) > 0` | High |
| `thanos_sidecar_prometheus_up` | Indicates if Prometheus is reachable via Thanos sidecar. Critical for long-term metric retention of VM performance data. If `0`, VM historical metrics become inaccessible. | `thanos_sidecar_prometheus_up == 0` | Critical |  
| `vector_checksum_errors_total` | Total number of checksum errors detected by Vector when processing log/event data. Indicates data integrity issues during log ingestion or forwarding. High or increasing values suggest that some log entries—potentially including those from VM, hypervisor, or cluster events—were corrupted or tampered with in transit. | `increase(vector_checksum_errors_total[15m]) > 0` | High |   
| `vector_http_server_handler_duration_seconds_bucket` | HTTP request latency for Vector. High latency delays VM log availability during incidents. | `histogram_quantile(0.95, sum(rate(vector_http_server_handler_duration_seconds_bucket[5m])) by (le)) > 2` | Medium |   
| `workload:cpu_usage_cores:sum` | Aggregated CPU usage across VMs. Sustained >85% usage risks VM throttling. | `sum(workload:cpu_usage_cores:sum) / sum(cluster:capacity_cpu_cores:sum) > 0.85` | Critical |  
| `workload:memory_usage_bytes:sum` | Total memory used by VMs. Combined with swap metrics, identifies memory pressure. | `sum(workload:memory_usage_bytes:sum) / sum(cluster:capacity_memory_bytes:sum) > 0.85` | High |  
| `write:apiserver_request_duration_seconds_bucket:rate1m` | API write latency. High values delay VM lifecycle operations. | `histogram_quantile(0.95, write:apiserver_request_duration_seconds_bucket:rate1m) > 1.5` | High | 

