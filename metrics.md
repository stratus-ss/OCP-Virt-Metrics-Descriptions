
| Name | Description (with Impact) | Practical Example | Example Query | Severity |
|------|---------------------------|-------------------|---------------|----------|
| `alertmanager_cluster_health_score` | Health score of the Alertmanager cluster. Lower values are better, and zero means "totally healthy". Impact: Higher values indicate degraded Alertmanager functionality, which could lead to missing or delayed alert notifications. | If the score rises above 1, it may indicate network issues between Alertmanager instances or other cluster health problems. | `alertmanager_cluster_health_score > 1` | High |
| `alertmanager_nflog_query_errors_total` | Counter for the number of notification log queries that failed. Impact: Increasing values indicate problems with the notification log system, potentially causing alerts to be lost or duplicated. | If this counter increases rapidly, it may indicate issues with the notification log storage or retrieval mechanism. | `rate(alertmanager_nflog_query_errors_total[5m]) > 0.1` | Medium |
| `alertmanager_marked_alerts` | Number of alerts marked by state (active, suppressed, unprocessed). Impact: Abnormal values may indicate alerts not being properly processed. | A high number of unprocessed alerts could indicate that Alertmanager is falling behind in processing alerts. | `sum(alertmanager_alerts{state="unprocessed"}) > 10` | Medium |
| `alertmanager_notification_requests_failed_total` | The total number of failed notification requests. Impact: Increasing values indicate that alert notifications are failing to be delivered to configured receivers. | If email notifications are failing, this metric would increase with the "integration=email" label. | `rate(alertmanager_notification_requests_failed_total{integration="email"}[5m]) > 0` | High |
| `alertmanager_notifications_failed_total` | Counter showing how many notifications have failed in total. Impact: Indicates problems with notification delivery to specific integrations. | If Slack notifications are failing, this would increase with the "integration=slack" label. | `increase(alertmanager_notifications_failed_total{integration="slack"}[1h]) > 0` | High |
| `alertmanager_silences_query_errors_total` | Counter for errors encountered when querying the silences database. Impact: Increasing values indicate problems with silence management, potentially causing alerts to fire when they should be silenced. | If this counter increases, it may indicate corruption in the silences database or other storage issues. | `increase(alertmanager_silences_query_errors_total[1h]) > 0` | Medium |
| `active_argocd_instances_total` | Shows the number of Argo CD instances currently managed across the cluster. Impact: Unexpected changes may indicate issues with Argo CD operator. | If this drops to 0 when you expect active instances, it indicates Argo CD instances are not being properly tracked. | `active_argocd_instances_total  0` | High |
| `apiserver_admission_controller_admission_duration_seconds_bucket` | Histogram of admission controller latencies. Impact: High latencies can slow down all API operations, affecting cluster performance. | If the 95th percentile latency exceeds 1 second, API operations will be noticeably slower. | `histogram_quantile(0.95, sum(rate(apiserver_admission_controller_admission_duration_seconds_bucket[5m])) by (le))` | Medium |
| `apiserver_audit_requests_rejected_total` | Counter of rejected audit requests. Impact: Increasing values indicate audit logging issues, potentially affecting compliance requirements. | If this counter increases rapidly, it could indicate audit backend problems or configuration issues. | `rate(apiserver_audit_requests_rejected_total[5m]) > 0` | Medium |
| `apiserver_current_inflight_requests` | Current number of requests being processed. Impact: High values indicate API server overload, potentially causing timeouts and degraded performance. | If this exceeds the configured max-in-flight limit, new requests will be rejected. | `sum(apiserver_current_inflight_requests) by (request_kind) > 100` | High |
| `apiserver_flowcontrol_current_executing_requests` | Number of requests currently being executed. Impact: High values may indicate API priority and fairness (APF) issues. | If a non-critical priority level is consuming too many resources, it could starve critical requests. | `sum(apiserver_flowcontrol_current_executing_requests) by (priorityLevel) > 50` | Medium |
| `apiserver_flowcontrol_current_executing_seats` | Number of seats currently occupied by executing requests. Impact: Relates to API server concurrency control. | If this approaches the upper limit, it indicates the API server is at maximum capacity. | `sum(apiserver_flowcontrol_current_executing_seats) / sum(apiserver_flowcontrol_upper_limit_seats) > 0.8` | Medium |
| `apiserver_flowcontrol_priority_level_request_utilization_count` | Indicates request concurrency utilization by priority level. Impact: High values may indicate certain priority levels are overloaded. | If the "system-node" priority level is highly utilized, node-related operations may be affected. | `apiserver_flowcontrol_priority_level_request_utilization_count{priorityLevel="system-node"} / apiserver_flowcontrol_upper_limit_seats{priorityLevel="system-node"} > 0.9` | Medium |
| `apiserver_flowcontrol_priority_level_seat_utilization_count` | Indicates seat concurrency utilization by priority level. Impact: High values may indicate certain priority levels are consuming too many resources. | If a workload priority level is using most available seats, it could impact other operations. | `apiserver_flowcontrol_priority_level_seat_utilization_count{priorityLevel="workload-high"} / apiserver_flowcontrol_upper_limit_seats{priorityLevel="workload-high"} > 0.9` | Medium |
| `apiserver_flowcontrol_request_dispatch_no_accommodation_total` | Total number of requests that could not be accommodated due to concurrency limits. Impact: Increasing values indicate requests being rejected due to overload. | If this increases for critical priority levels, important operations may be failing. | `increase(apiserver_flowcontrol_request_dispatch_no_accommodation_total{priorityLevel="system-cluster-critical"}[5m]) > 0` | High |
| `apiserver_flowcontrol_request_wait_duration_seconds_bucket` | Histogram of request waiting times. Impact: High wait times indicate API server congestion. | If the 95th percentile wait time exceeds 1 second, API operations will be noticeably delayed. | `histogram_quantile(0.95, sum(rate(apiserver_flowcontrol_request_wait_duration_seconds_bucket[5m])) by (le, priorityLevel))` | Medium |
| `apiserver_flowcontrol_upper_limit_seats` | Maximum number of seats available per priority level. Impact: This is a configuration limit that affects concurrency. | This is typically a static value but is important for calculating utilization percentages. | `apiserver_flowcontrol_upper_limit_seats` | Low |
| `apiserver_request:burnrate1d` | API request burn rate over 1 day. This is measured by taking the failed API requests divided by the total number of requests. This is a ratio for a given time period (in this case the ratio over 1 day) Impact: High values indicate sustained API server load. | If this exceeds SLO thresholds, it indicates the API server has been overloaded for an extended period. | `apiserver_request:burnrate1d > 2` | Medium |
| `apiserver_request:burnrate30m` | API request burn rate over 30 minutes. This is measured by taking the failed API requests divided by the total number of requests. This is a ratio for a given time period (in this case the ratio over 30 minutes) Impact: High values indicate medium-term API server load. | If this exceeds SLO thresholds, it indicates the API server has been overloaded recently. | `apiserver_request:burnrate30m > 5` | High |
| `apiserver_request:burnrate5m` | API request burn rate over 5 minutes. This is measured by taking the failed API requests divided by the total number of requests. This is a ratio for a given time period (in this case the ratio over 5 minutes). Impact: High values indicate short-term API server load spikes. | If this exceeds SLO thresholds, it indicates the API server is currently overloaded. | `apiserver_request:burnrate5m > 10` | Critical |
| `apiserver_request_post_timeout_total` | Total number of POST requests that have timed out. Impact: Increasing values indicate API server performance issues. | If this increases for create operations, new resources may fail to be created. | `increase(apiserver_request_post_timeout_total{verb="create"}[5m]) > 0` | High |
| `apiserver_request_sli_duration_seconds_count` | Count of requests used for SLI (Service Level Indicator) calculations. Impact: Used for SLO monitoring. | This is primarily used for calculating error rates and latencies against SLOs. | `sum(rate(apiserver_request_sli_duration_seconds_count{code=~"5.."}[5m])) / sum(rate(apiserver_request_sli_duration_seconds_count[5m]))` | Medium |
| `apiserver_request_slo_duration_seconds_count` | Count of requests used for SLO (Service Level Objective) calculations. Impact: Used for SLO monitoring. | Similar to SLI metric, used for calculating compliance with SLOs. | `sum(rate(apiserver_request_slo_duration_seconds_count{code=~"5.."}[5m])) / sum(rate(apiserver_request_slo_duration_seconds_count[5m]))` | Medium |
| `apiserver_storage_data_key_generation_failures_total` | Total number of failed data encryption key generation operations. Impact: Increasing values indicate issues with etcd encryption. | If this increases, it could indicate problems with the encryption provider or configuration. | `increase(apiserver_storage_data_key_generation_failures_total[1h]) > 0` | Critical |
| `apiserver_storage_size_bytes` | Size of the storage database file physically allocated in bytes. Impact: High values may indicate etcd database growth issues. | If this approaches the etcd size limit (typically 8GB), it could lead to API server instability. | `apiserver_storage_size_bytes > 7*1024*1024*1024` | High |
| `authenticated_user_requests` | Count of authenticated requests. Impact: Unexpected changes may indicate authentication issues. | A sudden drop could indicate authentication system problems. | `rate(authenticated_user_requests[5m])` | Medium |
| `authentication_attempts` | Count of authentication attempts. Impact: High failure rates indicate authentication configuration issues or potential security incidents. | A high rate of failed attempts could indicate a brute force attack. | `sum(rate(authentication_attempts{result="error"}[5m])) / sum(rate(authentication_attempts[5m])) > 0.5` | High |
| `apiserver_admission_webhook_rejection_count` | Alert for high rate of admission controller errors. Impact: Indicates problems with admission controllers, potentially preventing resource creation. | If ValidatingAdmissionWebhooks are failing, custom resources may not be created properly. | `sum(rate(apiserver_admission_webhook_rejection_count[5m])) / sum(rate(apiserver_admission_webhook_request_total[5m])) > 0.05` | High |
| `cco_credentials_requests_conditions` | Tracks the status conditions of Cloud Credential Operator credential requests. Impact: Indicates issues with cloud provider credentials that could affect cluster functionality. | If this metric shows credential requests in a degraded state, cloud resources may be inaccessible. | `cco_credentials_requests_conditions{condition="Degraded", status="True"} > 0` | High |
| `cluster_operator_conditions` | Represents the health conditions of core cluster operators. Impact: Degraded operators can affect overall cluster functionality. | If the OpenShift Virtualization operator shows a False Ready condition, virtualization features may be unavailable. | `cluster_operator_conditions{name="kubevirt-hyperconverged", condition="Ready", status="False"} == 1` | Critical |
| `cluster_operator_payload_errors` | Counts errors encountered during cluster operator payload processing. Impact: Increasing values indicate problems with operator updates. | If this increases during an upgrade, it may indicate problems applying new operator versions. | `sum(rate(cluster_operator_payload_errors[15m])) > 0` | High |
| `cluster:apiserver_current_inflight_requests:sum:max_over_time:2m` | Maximum number of concurrent requests to the API server over a 2-minute window. Impact: High values indicate API server congestion. | If this exceeds 100 for mutating requests, the API server may be overloaded, causing slow responses or failures. | `cluster:apiserver_current_inflight_requests:sum:max_over_time:2m{request_kind="mutating"} > 100` | High |
| `cluster:capacity_memory_bytes:sum` | Total memory capacity of the cluster in bytes. Impact: Used for capacity planning and resource monitoring. | If this value is close to the memory usage, the cluster may be running out of memory capacity. | `cluster:memory_usage_bytes:sum / cluster:capacity_memory_bytes:sum > 0.85` | Medium |
| `cluster:memory_usage_bytes:sum` | Total memory usage across the cluster in bytes. Impact: High values relative to capacity indicate potential resource constraints. | If memory usage exceeds 85% of capacity, workloads may experience memory pressure. | `cluster:memory_usage_bytes:sum / cluster:capacity_memory_bytes:sum > 0.85` | Medium |
| `cluster:usage:resources:sum` | Aggregated resource usage across the cluster. Impact: Helps identify resource consumption trends. | If CPU usage is consistently high, it may indicate the need for cluster scaling. | `cluster:usage:resources:sum{resource="cpu"} / cluster:capacity_cpu_cores:sum > 0.9` | Medium |
| `cluster:control_plane:all_nodes_ready` | Indicates whether all control plane nodes are in a ready state (1 for yes, 0 for no). Impact: Non-ready control plane nodes affect cluster stability. | If this metric is 0, at least one control plane node is not ready, which could affect high availability. | `cluster:control_plane:all_nodes_ready == 0` | Critical |
| `cluster:master_nodes` | Count of master/control plane nodes in the cluster. Impact: Unexpected changes may indicate node failures. | If this drops below the expected number (typically 3), cluster control plane redundancy is compromised. | `cluster:master_nodes  0.85` | Medium |
| `container_fs_usage_bytes` | Filesystem usage in bytes per container. Impact: High values may indicate disk space issues within containers. | If a container's filesystem usage approaches its limit, the container may experience write failures. | `container_fs_usage_bytes / container_fs_limit_bytes > 0.9` | Medium |
| `DiskSpaceLow` | Alert indicating low disk space on a node or volume. Impact: Low disk space can lead to service disruptions. | If this alert fires, a node or volume is running out of disk space and requires attention. | `DiskSpaceLow{severity="warning"}` | High |
| `Disk Space Limit Approaching` | Alert indicating disk space is approaching its limit. Impact: Potential future disk space issues. | If this alert fires, proactive action is needed to prevent disk space exhaustion. | `Disk Space Limit Approaching{severity="warning"}` | Medium |
| `cluster:network_attachment_definition_enabled_instance_up:max` | Maximum number of enabled network attachment definition instances that are up. Impact: Indicates health of multus CNI attachments. | If this is 0 when using secondary networks, pods requiring additional networks may fail to start. | `cluster:network_attachment_definition_enabled_instance_up:max == 0` | High |
| `coredns_forward_healthcheck_broken_total` | Total number of failed CoreDNS forward health checks. Impact: Increasing values indicate DNS forwarding issues. | If this increases rapidly, DNS resolution to external domains may fail. | `rate(coredns_forward_healthcheck_broken_total[5m]) > 0` | High |
| `coredns_health_request_failures_total` | Total number of CoreDNS health check failures. Impact: Indicates DNS service health issues. | If this increases, the CoreDNS service may be experiencing problems affecting DNS resolution. | `rate(coredns_health_request_failures_total[5m]) > 0` | High |
| `coredns_panics_total` | Total number of panics in CoreDNS. Impact: Indicates serious issues with DNS service. | Any increase in this metric indicates CoreDNS is experiencing critical failures. | `increase(coredns_panics_total[15m]) > 0` | Critical |
| `container_cpu_cfs_throttled_periods_total` | Total number of periods that a container was throttled due to CPU limits. Impact: High values indicate CPU contention. | If a critical container shows high throttling, it may experience performance degradation. | `rate(container_cpu_cfs_throttled_periods_total{namespace="openshift-virtualization"}[5m]) / rate(container_cpu_cfs_periods_total{namespace="openshift-virtualization"}[5m]) > 0.25` | Medium |
| `container_cpu_cfs_throttled_seconds_total` | Total time (in seconds) that a container was throttled due to CPU limits. Impact: High values indicate CPU contention. | If virt-launcher pods show high throttling, VM performance may be affected. | `rate(container_cpu_cfs_throttled_seconds_total{namespace="openshift-virtualization", pod=~"virt-launcher-.*"}[5m]) > 0.1` | Medium |
| `container_memory_failcnt` | Number of memory allocation failures in a container. Impact: Indicates memory pressure. | If this is increasing, containers are hitting memory limits and failing to allocate memory. | `increase(container_memory_failcnt[5m]) > 0` | High |
| `container_memory_swap` | Amount of swap space used by a container. Impact: High values indicate memory pressure. | If containers are using swap, it can significantly degrade performance. | `container_memory_swap > 0` | Medium |
| `container_memory_usage_bytes` | Memory usage of a container in bytes. Impact: High values relative to limits may lead to OOM kills. | If a container's memory usage approaches its limit, it may be terminated by the OOM killer. | `container_memory_usage_bytes / container_memory_working_set_bytes > 0.9` | Medium |
| `container_oom_events_total` | Total number of OOM (Out of Memory) events for a container. Impact: Indicates memory-related container terminations. | If this increases, containers are being killed due to memory pressure. | `increase(container_oom_events_total[15m]) > 0` | High |
| `container_runtime_crio_containers_oom_total` | Total number of OOM events for containers managed by CRI-O. Impact: Indicates memory-related container terminations. | If this increases, CRI-O containers are being killed due to memory pressure. | `increase(container_runtime_crio_containers_oom_total[15m]) > 0` | High |
| `containerd_cri_input_bytes_total` | Total bytes received by containerd CRI. Impact: Helps monitor container runtime network traffic. | Sudden spikes may indicate unusual container activity or potential issues. | `rate(containerd_cri_input_bytes_total[5m]) > 1e6` | Low |
| `cnv:vmi_status_running:count` | Count of running Virtual Machine Instances. Impact: Unexpected changes may indicate VM issues. | If this drops suddenly, VMs may be failing or being terminated unexpectedly. | `sum by(node) (cnv:vmi_status_running:count)` <br><br>`sum by (guest_os_name)(cnv:vmi_status_running:count)| Medium |
| `cnv_abnormal` | The metric tracks two specific memory conditions:<ul><li>`memory_working_set_delta_from_request`: The difference between the working set memory and the requested memory</ul></li> <ul><li>`memory_rss_delta_from_request`: The difference between the resident set size (RSS) memory and the requested memory </ul></li>Impact: Abnormal conditions may affect VM functionality. | Positive values indicate the component is using more memory than requested, while negative values indicate it's using less than requested.<br><br> Large positive values could indicate memory pressure in your virtualization components. Consistently high values might suggest you need to adjust the resource requests for your virtualization components| `sum by (container) (cnv_abnormal{reason="memory_working_set_delta_from_request"})` | High |
| `controller_runtime_reconcile_panics_total` | Total number of panics during controller reconciliation. Impact: Indicates serious issues with Kubernetes controllers. | Any increase in this metric indicates controllers are experiencing critical failures. | `increase(controller_runtime_reconcile_panics_total[15m]) > 0` | Critical |
| `controller_runtime_terminal_reconcile_errors_total` | Total number of terminal errors during controller reconciliation. Impact: Indicates controller failures. | If this increases for a specific controller, resources managed by that controller may be in a bad state. | `increase(controller_runtime_terminal_reconcile_errors_total[15m]) > 0` | High |
| `ErrorRateIncrease` | Alert for increased error rates in API requests. Impact: Indicates API server issues affecting operations. | If this alert fires, the API server is experiencing an elevated error rate, potentially affecting cluster operations. | `ErrorRateIncrease{severity="warning"}` | High |
| `component_resource:apiserver_request_terminations_total:rate:1m` | Rate of API server request terminations over 1 minute. Impact: High values indicate API server overload. | If this exceeds normal baseline, the API server is terminating requests due to overload. | `component_resource:apiserver_request_terminations_total:rate:1m > 10` | High |
| `component_resource:apiserver_request_terminations_total:rate:5m` | Rate of API server request terminations over 5 minutes. Impact: High values indicate sustained API server overload. | If this exceeds normal baseline, the API server is consistently terminating requests. | `component_resource:apiserver_request_terminations_total:rate:5m > 5` | High |
| `etcd_disk_backend_commit_duration_seconds_bucket` | Histogram of latency distribution of commit operations called by the backend. Impact: High latencies indicate disk performance issues affecting etcd write operations. | If the 99th percentile exceeds 25ms, it may indicate disk performance issues affecting etcd stability. | `histogram_quantile(0.99, sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (le))` | High |
| `etdc_disk_backend_defrag_duration_seconds_bucket` | Histogram of latency distribution of defragmentation operations. Impact: High values indicate defragmentation is taking too long, potentially affecting cluster performance. | If defragmentation operations consistently take more than 10 seconds, it may indicate underlying storage issues. | `histogram_quantile(0.95, sum(rate(etdc_disk_backend_defrag_duration_seconds_bucket[5m])) by (le))` | Medium |
| `etcd_disk_wal_fsync_duration_seconds_bucket` | Histogram of latency distribution of fsync operations to the WAL (Write-Ahead Log). Impact: High values indicate disk I/O issues affecting etcd performance. | If the 99th percentile exceeds 10ms, it may indicate slow disk performance leading to potential leader election issues and slow writes. | `histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) by (le, instance))` | Critical |
| `etcd_disk_wal_fsync_duration_seconds_sum` | Sum of time spent on fsync operations to the WAL. Impact: Increasing values indicate cumulative disk I/O pressure. | This metric is used with the count to calculate average fsync duration, which should be monitored for trends. | `rate(etcd_disk_wal_fsync_duration_seconds_sum[5m]) / rate(etcd_disk_wal_fsync_duration_seconds_count[5m])` | Medium |
| `etcd_mvcc_db_total_size_in_bytes` | Total size of the etcd database in bytes. Impact: As this approaches the quota, etcd may stop accepting writes. | If this exceeds 80% of the quota (typically 2-8GB), compaction and defragmentation may be needed to prevent etcd from becoming read-only. | `etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes > 0.8` | Critical |
| `etcd_server_quota_backend_bytes` | Maximum backend size in bytes etcd can use before triggering an alarm. Impact: Defines the limit for etcd database size. | This is typically set to 2-8GB. If the database size approaches this value, maintenance is required. | `etcd_server_quota_backend_bytes` | Low |
| `etcd_debugging_raft_terms_total` | Total number of Raft terms seen by this etcd member. Impact: Rapid increases indicate frequent leader elections and potential instability. | If this increases rapidly over a short period, it indicates cluster instability with frequent leadership changes. | `delta(etcd_debugging_raft_terms_total[15m]) > 5` | High |
| `etcd_network_active_peers` | Number of active peers in the etcd cluster. Impact: Values below expected indicate network connectivity issues. | In a 3-node etcd cluster, if this drops below 2 for any member, quorum may be at risk. | `etcd_network_active_peers  0` | High |
| `etcd_server_has_leader` | Whether this etcd member has a leader (1) or not (0). Impact: Values of 0 indicate split-brain or isolation. | If this is 0 for any etcd member, that member cannot process writes and may be network isolated. | `etcd_server_has_leader == 0` | Critical |
| `etcd_server_proposals_failed_total` | Total number of failed proposals (consensus operations). Impact: Increasing values indicate consensus issues. | If this increases, it indicates problems with the Raft consensus algorithm, potentially due to network issues or overload. | `increase(etcd_server_proposals_failed_total[15m]) > 0` | High |
| `etcd_lease_object_counts_count` | Number of objects attached to leases. Impact: High values may indicate resource leaks. | If this grows continuously without bounds, it may indicate applications not properly releasing leases. | `etcd_lease_object_counts_count > 1000` | Medium |
| `etcd_server_client_requests_total` | Total number of client requests received by type. Impact: Rate changes indicate client load patterns. | Monitoring the rate of write requests can help identify periods of high load on the etcd cluster. | `sum(rate(etcd_server_client_requests_total{type="write"}[5m]))` | Medium |
| `go_cpu_classes_gc_total_cpu_seconds_total` | Total CPU time spent on garbage collection. Impact: High values indicate excessive GC activity. | If this is increasing rapidly, it may indicate memory pressure in the Go runtime. | `rate(go_cpu_classes_gc_total_cpu_seconds_total[5m]) > 0.1` | Medium |
| `go_gc_gomemlimit_bytes` | Memory limit for the Go runtime in bytes. Impact: Defines the ceiling for Go memory usage. | This is a configuration setting that helps understand the memory constraints for the process. | `go_gc_gomemlimit_bytes` | Low |
| `go_gc_heap_live_bytes` | Size of the live objects heap in bytes. Impact: High values relative to limits indicate memory pressure. | If this approaches the total heap size, it may indicate memory leaks or high memory pressure. | `go_gc_heap_live_bytes / go_memory_classes_total_bytes > 0.8` | Medium |
| `go_godebug_non_default_behavior_panicnil_events_total` | Total number of nil panic events. Impact: Any non-zero value indicates programming errors. | If this increases, it indicates nil pointer dereferences in the code, which should never happen in production. | `increase(go_godebug_non_default_behavior_panicnil_events_total[15m]) > 0` | Critical |
| `go_memory_classes_metadata_mspan_inuse_bytes` | Size of mspan structures currently in use in bytes. Impact: Indicates memory used for internal Go runtime structures. | This is primarily useful for detailed Go runtime memory analysis. | `go_memory_classes_metadata_mspan_inuse_bytes` | Low |
| `go_memory_classes_total_bytes` | Total size of Go memory classes in bytes. Impact: Indicates overall memory usage by the Go runtime. | This helps understand the total memory footprint of the process. | `go_memory_classes_total_bytes` | Medium |
| `endpoint_slice_controller_endpoints_desired` | Number of endpoints desired by the EndpointSlice controller. Impact: Indicates load on the EndpointSlice controller. | If this grows very large, it may indicate a large number of services or endpoints that could affect controller performance. | `endpoint_slice_controller_endpoints_desired > 10000` | Medium |
| `group_sync_error` | Indicates errors during group synchronization. Impact: May affect cluster consistency in relation to permissions within the cluster. | If this increases, it could indicate issues with group membership updates. | `increase(group_sync_error[15m]) > 0` | Medium |
| `grpc_req_panics_recovered_total` | Total number of gRPC request panics recovered. Impact: Increasing values indicate gRPC service instability. | If this increases rapidly, it may indicate issues with gRPC request handling. | `rate(grpc_req_panics_recovered_total[5m]) > 0` | Medium |
| `haproxy_backend_connection_errors_total` | Total number of connection errors on HAProxy backends. Impact: Increasing values indicate issues with backend connectivity. | If this increases for a specific backend, it may indicate server or network issues. | `rate(haproxy_backend_connection_errors_total[5m]) > 0` | Medium |
| `haproxy_backend_http_average_response_latency_milliseconds` | Average response latency of HAProxy backend HTTP requests in milliseconds. Impact: High values indicate slow backend responses. | If this exceeds 500ms for a critical service, it may impact user experience. | `haproxy_backend_http_average_response_latency_milliseconds > 500` | Medium |
| `haproxy_backend_http_responses_total` | Total number of HTTP responses from HAProxy backends. Impact: Used to monitor backend request volume. | Trend analysis used to determine activity in the cluster. | `rate(haproxy_backend_http_responses_total[5m]) == 0` | Medium |
| `haproxy_backend_up` | Indicates whether HAProxy backends are up (1) or down (0). Impact: Values of 0 indicate backend availability issues. | If this is 0 for a critical backend, traffic may not be routed correctly. | `haproxy_backend_up == 0` | Critical |
| `haproxy_server_connections_total` | Total number of connections to HAProxy servers. Impact: Used to monitor server load. | If this increases rapidly, it may indicate a sudden spike in traffic. | `rate(haproxy_server_connections_total[5m]) > 100` | Medium |
| `haproxy_server_downtime_seconds_total` | Total downtime of HAProxy servers in seconds. Impact: Increasing values indicate server availability issues. | If this increases for a critical server, it may indicate recurring outages. | `increase(haproxy_server_downtime_seconds_total[15m]) > 0` | Medium |
| `haproxy_server_up` | Indicates whether HAProxy servers are up (1) or down (0). Impact: Values of 0 indicate server availability issues. | If this is 0 for a critical server, it may indicate a server failure. | `haproxy_server_up == 0` | Critical |
| `haproxy_up` | Indicates whether HAProxy is up (1) or down (0). Impact: Values of 0 indicate HAProxy service issues. | If this is 0, the HAProxy service is not running or not reachable. | `haproxy_up == 0` | Critical |
| `High Admission Errors Rate` | Alert for high rate of admission controller errors. Impact: Indicates problems with admission controllers, potentially preventing resource creation. | If ValidatingAdmissionWebhooks are failing, custom resources may not be created properly. | `sum(rate(apiserver_admission_webhook_rejection_count[5m])) / sum(rate(apiserver_admission_webhook_request_total[5m])) > 0.05` | High |
| `High Container Restart Rate` | Alert for high rate of container restarts. Impact: Indicates potential issues with container stability or resource constraints. | If containers are restarting frequently, it may indicate memory or CPU issues. | `sum(rate(kube_pod_container_status_restarts_total[5m])) > 10` | Medium |
| `High Error Rate` | Alert for high error rates in API requests or other operations. Impact: Indicates service instability or configuration issues. | If this alert fires, it may indicate problems with API server or backend services. | `sum(rate(apiserver_request_total{code=~"5.."}[5m])) / sum(rate(apiserver_request_total[5m])) > 0.1` | High |
| `HPA Status Condition Failure` | Indicates a failure in the Horizontal Pod Autoscaler (HPA) status condition. Impact: May prevent HPA from scaling workloads correctly. | If this condition is failing, HPA may not adjust pod counts based on resource utilization. | `kube_hpa_status_condition{condition="ScalingActive", status="False"} == 1` | Medium |
| `http_client_request_total` | Total number of HTTP client requests. Impact: Used to monitor client request volume. | If this drops suddenly, it may indicate client connectivity issues. | `rate(http_client_request_total[5m]) == 0` | Medium |
| `instance_request_kind:apiserver_current_inflight_requests:sum` | Sum of concurrent requests to the API server by request kind. Impact: High values indicate API server congestion. | If this exceeds 100 for mutating requests, the API server may be overloaded. | `instance_request_kind:apiserver_current_inflight_requests:sum{request_kind="mutating"} > 100` | High |
| `kube_cronjob_spec_suspend` | Indicates whether a CronJob is suspended. Impact: Suspended CronJobs will not run scheduled tasks. | If this is True for a critical CronJob, scheduled tasks may not execute. | `kube_cronjob_spec_suspend == "True"` | Medium |
| `kube_deployment_status_condition` | Status conditions of Deployments. Impact: Indicates health and readiness of Deployments. | If a Deployment shows a False Ready condition, pods may not be available for traffic. | `kube_deployment_status_condition{condition="Available", status="False"} == 1` | Medium |
| `kube_deployment_status_replicas_available` | Number of available replicas in a Deployment. Impact: Used to monitor Deployment health. | If this is less than the desired number, pods may not be fully available. | `kube_deployment_status_replicas_available  0` | Medium |
| `kube_job_status_failed` | Indicates failed Jobs. Impact: May indicate issues with Job execution or configuration. | If this increases for a critical Job, it may indicate recurring failures. | `kube_job_status_failed == 1` | Medium |
| `kubelet_memory_manager_pinning_errors_total` | Total number of memory pinning errors by the Kubelet memory manager. Impact: Indicates issues with memory allocation. | If memory pinning fails, workloads requiring guaranteed memory allocation may experience higher latency and decreased performance, especially in NUMA architectures. This metric should ideally always be 0 | `rate(kubelet_memory_manager_pinning_errors_total[5m]) > 0` | Medium |
| `kubelet_pleg_last_seen_seconds` | Time in seconds since the last PLEG (Pod Lifecycle Event Generator) update. Impact: High values indicate Kubelet issues. | If this exceeds 60 seconds, it may indicate Kubelet is not updating pod status correctly. | `kubelet_pleg_last_seen_seconds > 60` | Medium |
| `kubelet_pod_start_sli_duration_seconds_bucket` | Histogram of pod start latency in seconds. Impact: High values indicate slow pod startup times. | If the 95th percentile exceeds 30 seconds, pod startup may be slow, affecting service availability. | `histogram_quantile(0.95, sum(rate(kubelet_pod_start_sli_duration_seconds_bucket[5m])) by (le))` | Medium |
| `kubelet_volume_stats_available_bytes` | Available bytes in a volume. Impact: Used to monitor volume capacity and detect potential storage issues. | If available bytes are low, it may indicate that the volume is running out of space. | `kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.2` | Medium |
| `kubelet_volume_stats_inodes_free` | Number of free inodes in a volume. Impact: Low values indicate potential inode exhaustion. | If free inodes are less than 5% of total inodes, it may indicate inode exhaustion issues. | `kubelet_volume_stats_inodes_free / kubelet_volume_stats_inodes < 0.05` | Medium |
| `kube_pod_status_ready` | Indicates whether a pod is ready to serve requests. Impact: Non-ready pods may not receive traffic. | If a critical pod is not ready, it may indicate issues with the pod or its containers. | `kube_pod_status_ready{namespace="default", pod="critical-pod"} == 0` | High |
| `kube_pod_status_unschedulable` | Indicates whether a pod is unschedulable. Impact: Unschedulable pods cannot be placed on nodes. | If a pod is unschedulable due to resource constraints, it may indicate the need for node scaling. | `kube_pod_status_unschedulable{namespace="default", pod="critical-pod"} == 1` | Medium |
| `kube_running_pod_ready` | Number of running pods that are ready. Impact: Used to monitor pod health and availability. | If this drops suddenly, it may indicate pod failures or readiness issues. | `kube_running_pod_ready{namespace="default"} < 10` | Medium |
| `kubevirt_vmi_memory_swap_in_traffic_bytes` | Swap-in traffic for Virtual Machine Instances (VMIs) in bytes. Impact: High values indicate memory pressure. | Swap-in refers to the process of moving data from swap space (usually disk storage) back into the VM's RAM | `rate(kubevirt_vmi_memory_swap_in_traffic_bytes[5m]) > 100` | Medium |
| `kubevirt_vmi_memory_swap_out_traffic_bytes` | Swap-out traffic for Virtual Machine Instances (VMIs) in bytes. Impact: High values indicate memory pressure. | Sawp-out refers to the process of moving data from ram to swap space. If swap-out traffic is high, it may indicate that VMIs are experiencing memory constraints. | `rate(kubevirt_vmi_memory_swap_out_traffic_bytes[5m]) > 100` | Medium |
| `kubevirt_hco_hyperconverged_cr_exists` | Indicates whether the Hyperconverged Custom Resource exists. Impact: Non-existent CRs may affect hyperconverged functionality. | If this is False, it may indicate issues with hyperconverged deployment or configuration. | `kubevirt_hco_hyperconverged_cr_exists == 0` | Critical |
| `kubevirt_hco_system_health_status` | Numeric health status of the Hyperconverged Operator (1 = healthy, 0 = not healthy). Impact: A value of 0 indicates issues with hyperconverged components that may affect virtualization functionality. | If this metric returns 0, the HCO is reporting a problem requiring investigation. The HyperConverged CR creates corresponding CRs for the operators of all other components related to Compute, storage, networking, templating and scaling within OpenShift Virtualization. | `kubevirt_hco_system_health_status == 0` | Critical |
| `kubevirt_hyperconverged_operator_health_status` | Health status of the Hyperconverged Operator. Impact: Non-healthy status may indicate issues with hyperconverged components. | If this indicates a degraded status, it may affect hyperconverged functionality. | `kubevirt_hyperconverged_operator_health_status != 0` | High |
| `kubevirt_number_of_vms` | Number of Virtual Machines managed by KubeVirt. Impact: Used to monitor VM deployment and scaling. | If this drops suddenly, it may indicate VM termination or deployment issues. | `kubevirt_number_of_vms{namespace="default"} < 10` | Medium |
| `kubevirt_ssp_operator_up` | Indicates whether the SSP (Specialized Service Provider) operator, a core component of OpenShift Virtualization, is up and running. Impact: If not running (`0`), key virtualization features such as VM templates, common templates, and data import will not function, severely impacting OpenShift Virtualization operations. | If this metric is `0`, users will be unable to deploy new VMs from templates or use data import features until the operator is restored. | `kubevirt_ssp_operator_up == 0` | Critical |
| `kubevirt_virt_api_up` | Indicates whether the KubeVirt API is up and running. Impact: Non-running API may affect KubeVirt functionality. | If this is False, it may indicate issues with the API service or configuration. | `kubevirt_virt_api_up == 0` | Critical |
| `kubevirt_virt_controller_up` | Indicates whether the KubeVirt controller is up and running. **The KubeVirt controller is responsible for managing the lifecycle of all Virtual Machine Instances (VMIs) in the cluster, including creation, updates, and deletion.** Impact: If the controller is not running (`0`), no VMs can be scheduled, updated, or deleted, and the virtualization platform will be unable to react to changes in VM resources. This results in a complete loss of VM orchestration and automation. | If this metric returns `0`, VM lifecycle operations (such as creation, migration, or deletion) will not be processed, and VMs may become stuck in transitional states. | `kubevirt_virt_controller_up == 0` | Critical |
| `kubevirt_virt_handler_up` | Indicates whether the KubeVirt handler is up and running. **The KubeVirt handler runs as a DaemonSet on each node and is responsible for managing the actual execution of VMIs on the node, including monitoring, reporting status, and handling VM shutdown or migration.** Impact: If the handler is not running (`0`) on a node, VMIs on that node cannot be started, stopped, or migrated, and their status will not be reported accurately. This leads to loss of control and observability for VMs on affected nodes. | If this metric returns `0` for any node, VMs on that node may become unmanageable, unresponsive, or orphaned, severely impacting workload reliability. | `kubevirt_virt_handler_up == 0` | Critical |
| `kubevirt_virt_operator_up` | Indicates whether the KubeVirt operator is up and running. **The KubeVirt operator is responsible for deploying, updating, and maintaining all KubeVirt components (controllers, handlers, API, etc.) and ensuring their desired state.** Impact: If the operator is not running (`0`), the platform loses self-healing and automated management capabilities. This can prevent recovery from failures, upgrades, or configuration changes, and may eventually result in cluster drift or degraded service. | If this metric returns `0`, KubeVirt components will not be automatically reconciled, upgraded, or repaired, increasing risk of outages and operational drift. | `kubevirt_virt_operator_up == 0` | Critical |
| `kubevirt_vmi_cpu_usage_seconds_total` | Total CPU usage of Virtual Machine Instances (VMIs) in seconds. Impact: High values indicate CPU resource constraints. | If CPU usage is consistently high, it may indicate the need for VM scaling or resource adjustments. | `rate(kubevirt_vmi_cpu_usage_seconds_total[5m]) > 100` | Medium |
| `kubevirt_vmi_memory_available_bytes` | Available memory for Virtual Machine Instances (VMIs) in bytes. Impact: Low values indicate memory constraints. | If available memory is low, it may indicate that VMIs are experiencing memory pressure. | `kubevirt_vmi_memory_available_bytes{namespace="default"} < 1e9` | Medium |
| `kubevirt_vmi_memory_pgmajfault_total` | Total number of major page faults for Virtual Machine Instances (VMIs). Impact: High values indicate memory pressure. | If major page faults are increasing, it may indicate that VMIs are experiencing memory constraints. | `rate(kubevirt_vmi_memory_pgmajfault_total[5m]) > 0` | Medium |
| `loki_query_frontend_shards_total` | Number of active query shards in the Loki query frontend. Impact: In OpenShift Virtualization, large log volumes from VMs, nodes, and hypervisors demand efficient log query parallelization. Too few shards can bottleneck query performance, making VM troubleshooting or compliance investigations slow. Too many shards can cause overhead and resource waste. Monitoring this helps ensure log query responsiveness for critical virtualization events. | If a VM migration fails and you need to quickly search logs across many sources, a low shard count could make queries slow, delaying incident response. | `loki_query_frontend_shards_total` | Medium |
| `loki_runtime_config_last_reload_successful` | Indicates if the last Loki configuration reload was successful (1 for success, 0 for failure). Impact: Failed reloads may affect log collection patterns for VM logs, reducing visibility into virtualization problems. | After configuring custom log scraping rules for VM-specific error patterns, this metric helps verify the configuration is working correctly. | `loki_runtime_config_last_reload_successful == 0` | Medium |
| `loki_store_series_total` | Total number of series in Loki's store. Impact: In virtualization environments, the number of log streams grows significantly with each VM, potentially causing high cardinality problems affecting query performance and storage. | Monitoring this metric can help identify if VM logging is generating too many unique log streams, which could impact the performance of the entire logging stack. | `loki_store_series_total > 1000` | Medium |
| `loki_tsdb_build_index_last_successful_timestamp_seconds` | Timestamp of the last successful TSDB index build in seconds. Impact: Outdated indexes may affect ability to search VM logs, hampering troubleshooting of virtualization issues. | If this timestamp is stale (more than an hour old), recent VM migration or error logs may not be searchable, delaying incident response. | `time() - loki_tsdb_build_index_last_successful_timestamp_seconds > 3600` | Medium |
| `loki_write_failures_discarded_total` | Total count of log entries discarded due to write failures. Impact: May result in missing VM logs critical for troubleshooting virtualization issues and maintaining compliance. | If increasing rapidly during VM migration activities, important logs about VM state transitions may be missing, making it difficult to diagnose failed migrations. | `increase(loki_write_failures_discarded_total[1h]) > 0` | High |
| `mapi_machinehealthcheck_short_circuit` | Indicates if machine health checks are short-circuited (disabled). Impact: When enabled (1), prevents automatic remediation of unhealthy nodes hosting VMs, potentially extending VM outages. | During planned maintenance, this might be enabled, but if left enabled accidentally, it could prevent automatic recovery of nodes hosting critical VMs. | `mapi_machinehealthcheck_short_circuit > 0` | Medium |
| `mapi_mao_collector_up` | Indicates if the Machine API Operator collector is up (1) or down (0). Impact: When down, affects machine health monitoring for nodes hosting VMs, potentially leaving hardware issues undetected. | If down, problems with worker nodes hosting VMs may go undetected, such as cooling issues that could lead to thermal throttling affecting VM performance. | `mapi_mao_collector_up == 0` | High |
| `mcd_reboots_failed_total` | Total count of failed node reboots during machine config updates. Impact: May leave nodes in inconsistent states with partial updates to virtualization components. | If a node fails to reboot during a KVM or QEMU update, VMs scheduled on that node may experience stability issues or incompatibility with the rest of the cluster. | `increase(mcd_reboots_failed_total[1h]) > 0` | High |
| `mco_degraded_machine_count` | Count of machines in a degraded state according to the Machine Config Operator. Impact: Degraded machines may host VMs with reduced performance or with outdated virtualization components. | If nodes hosting VMs are degraded, virtualization features like live migration or device passthrough may be compromised due to inconsistent configurations. | `mco_degraded_machine_count > 0` | High for Control Plane <br><br> Medium for Workers|
| `mco_unavailable_machine_count` | Count of unavailable machines according to the Machine Config Operator. Impact: Unavailable machines cannot host VMs, reducing total virtualization capacity. | If this metric increases, the cluster's capacity for hosting VMs is reduced, potentially preventing new VM creation or causing resource contention. | `mco_unavailable_machine_count > 0` | High for Control Plane <br><br> Medium for Workers |
| `node_cpu_core_throttles_total` | Total number of CPU throttling events on a node. Impact: May indicate CPU contention or thermal issues affecting VM performance and stability. | High throttling can cause VM workloads to experience unpredictable performance, especially for latency-sensitive applications, and may indicate that a node is oversubscribed or experiencing cooling problems. | `rate(node_cpu_core_throttles_total[5m]) > 10` | Medium |
| `node_edac_csrow_uncorrectable_errors_total` | Total uncorrectable memory errors at CSROW (Chip-Select Row) level. Impact: May indicate hardware memory issues that could lead to VM crashes or corruption. | Increasing errors may precede host crashes affecting running VMs, or cause silent data corruption inside VM memory that could affect application integrity. | `increase(node_edac_csrow_uncorrectable_errors_total[1h]) > 0` | Critical |
| `node_edac_uncorrectable_errors_total` | Total uncorrectable memory errors across the system. Impact: May indicate hardware memory issues that could cause VM crashes, data corruption, or host failures. | Even a single uncorrectable memory error could potentially crash VMs or cause data corruption within VM memory, making this a critical metric to monitor on virtualization hosts. | `increase(node_edac_uncorrectable_errors_total[1h]) > 0` | Critical |
| `node_fibrechannel_link_failure_total` | Total Fibre Channel link failures detected. Impact: May affect VM access to storage on SAN, causing I/O errors for VMs using FC-based persistent volumes. | If increasing, VMs using Fibre Channel storage may experience I/O errors, leading to application failures or filesystem corruption within the VM. | `increase(node_fibrechannel_link_failure_total[1h]) > 0` | High |
| `node_fibrechannel_loss_of_signal_total` | Total Fibre Channel signal loss events. Impact: May affect VM access to storage on SAN, causing I/O timeouts for VMs using FC storage. | If increasing, VMs using Fibre Channel storage may experience I/O timeouts, leading to application hangs or perceived downtime for services running in VMs. | `increase(node_fibrechannel_loss_of_signal_total[1h]) > 0` | High |
| `node_fibrechannel_loss_of_sync_total` | Total Fibre Channel synchronization loss events. Impact: May affect VM access to storage on SAN, causing intermittent I/O issues. | If increasing, VMs using Fibre Channel storage may experience intermittent I/O issues, leading to application performance degradation or increased latency. | `increase(node_fibrechannel_loss_of_sync_total[1h]) > 0` | High |
| `node_filesystem_avail_bytes` | Available bytes in the node's filesystem. Impact: Low values may affect local VM storage, container images, and ephemeral disks used by VMs. | If a node runs out of filesystem space, VM creation may fail, ephemeral disks may become full causing VM application errors, and VM image pulling may fail. | `node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"} < 0.10` | High |
| `node_filesystem_free_bytes` | Free space in bytes in the node's filesystem. Impact: Critical for VM storage operations, as low free space can prevent VM creation, snapshot operations, and disk image transfers. VM disk I/O performance also degrades as filesystems approach capacity, affecting running workloads. | When creating new VMs or during VM migration operations, insufficient free space can cause failed migrations or prevent VM snapshots, leaving workloads vulnerable without backup options. | `node_filesystem_free_bytes{mountpoint="/var"} / node_filesystem_size_bytes{mountpoint="/var"}  0` | Critical |
| `node_hwmon_temp_crit_celsius` | Critical temperature threshold for hardware components in Celsius. Impact: Helps determine thermal headroom before VM workloads are affected by throttling. Important for capacity planning when placing VMs with intensive workloads. | When planning to deploy VMs with high CPU utilization patterns (ML workloads, database servers), compare current temperature against this critical threshold to ensure adequate thermal margin. | `(node_hwmon_temp_celsius / node_hwmon_temp_crit_celsius) > 0.8` | Medium |
| `node_infiniband_link_downed_total` | Total number of times an InfiniBand link went down. Impact: InfiniBand is often used for high-performance VM workloads requiring low-latency networking. Link failures directly impact VM network performance and can cause application errors within VMs. | VMs running clustered applications like databases or HPC workloads over InfiniBand will experience connection errors or timeouts when this value increases, potentially leading to application crashes. | `increase(node_infiniband_link_downed_total[1h]) > 0` | High |
| `node_infiniband_port_errors_received_total` | Total count of InfiniBand port errors received. Impact: Affects network reliability for VM workloads using InfiniBand for high-performance computing or storage access. Even small error rates can degrade performance of latency-sensitive VM applications. | If this metric increases for nodes hosting database VMs using InfiniBand for storage access, those VMs may experience intermittent I/O errors leading to database corruption or performance issues. | `rate(node_infiniband_port_errors_received_total[5m]) > 0` | Medium |
| `node_memory_CommitLimit_bytes` | System-wide kernel configuration indicating the theoretical maximum memory allocation limit (physical RAM + swap  overcommit ratio). Impact: For OpenShift Virtualization, this metric has minimal impact on actual VM placement decisions, as scheduling is based on physical RAM and memory requests, not the commit limit. | This static value is primarily useful for understanding the kernel's memory overcommit configuration rather than for operational decision-making about VM placement. Identical values across nodes indicate identical kernel memory configurations. | `node_memory_CommitLimit_bytes / (1024*1024*1024)` | Low |
| `node_memory_MemFree_bytes` | Amount of physical RAM left unused by the system. Impact: Directly affects VM placement, migration capabilities, and performance under memory pressure. Low free memory can trigger swapping which severely impacts VM performance. | When memory gets too low, memory-intensive VMs may be unable to start or migrate to a node. Existing VMs may experience performance degradation as the hypervisor struggles to allocate memory pages. <br> <b>NOTE:</b> this does not take into account buffers, cache or overall available memory!| `node_memory_MemFree_bytes / node_memory_MemTotal_bytes` | Medium-Low |
| `node_netstat_Ip6_InOctets` | Total number of IPv6 octets (bytes) received. Impact: Measures IPv6 network throughput to nodes hosting VMs. Important for dual-stack VM networking and monitoring IPv6-specific traffic patterns. | When running dual-stack VMs, unexpected increases may indicate IPv6-specific traffic that could impact network performance or security for those VMs. | `rate(node_netstat_Ip6_InOctets[5m])` | Low |
| `node_netstat_Ip6_OutOctets` | Total number of IPv6 octets (bytes) sent. Impact: Measures IPv6 network throughput from nodes hosting VMs. Important for dual-stack VM networking and monitoring IPv6-specific traffic patterns. | For VMs serving content over IPv6, this metric helps identify if network bandwidth is sufficient for the workload or if IPv6 traffic patterns differ from IPv4. | `rate(node_netstat_Ip6_OutOctets[5m])` | Low |
| `node_netstat_IpExt_InOctets` | Total number of IPv4 octets (bytes) received. Impact: Measures inbound network throughput to nodes hosting VMs. Critical for monitoring network-intensive VM workloads and identifying potential bandwidth constraints. | If this metric approaches physical NIC capacity, VMs may experience network contention, packet drops, or increased latency, especially for VMs running streaming or real-time communication services. | `rate(node_netstat_IpExt_InOctets[5m]) / (1024 * 1024) > 100` | Medium |
| `node_netstat_IpExt_OutOctets` | Total number of IPv4 octets (bytes) sent. Impact: Measures outbound network throughput from nodes hosting VMs. Critical for VMs serving content or performing data transfers. | VMs serving web content or performing large data exports may saturate outbound network capacity, causing slowdowns or timeouts for all VMs on the node when this metric approaches physical limits. | `rate(node_netstat_IpExt_OutOctets[5m]) / (1024 * 1024) > 1250` (for a 10g nic or 3150 for 25G nic)| Medium |
| `node_netstat_Tcp_ActiveOpens` | Total number of active TCP connection openings. Impact: Measures the rate of new outbound connections from VMs. Useful for identifying abnormal connection patterns or potential connection storms from VM workloads. | A sudden spike in this metric may indicate a VM is performing aggressive outbound connection attempts, which could be a sign of compromised VM, misconfigured application, or DoS attack originating from within a VM. | `rate(node_netstat_Tcp_ActiveOpens[5m]) > 1000` | Medium |
| `node_network_receive_packets_total` | Total count of network packets received. Impact: Measures network load in packets rather than bytes. Important for VMs with high packet rate workloads like DNS servers or network routing appliances. | VMs running packet-processing workloads may hit packet processing limits before bandwidth limits. High packet rates can cause CPU saturation on the host, affecting all VMs. | `rate(node_network_receive_packets_total{device!="lo"}[5m]) > 100000` | Medium |
| `node_network_up` | Binary metric indicating if a network device is up (1) or down (0). Impact: Network interface failures directly impact VM connectivity. Critical for multi-NIC VM configurations or VMs using specific network interfaces for specialized traffic. | If this metric shows 0 for a network interface used by VM secondary NICs (like SR-IOV or bridge networks), those VMs will lose connectivity on that interface, potentially breaking application clustering or storage connectivity. | `node_network_up{device!="lo"} == 0` | Critical |
| `node_nfsd_connections_total` | Total count of NFS connections. Impact: Important if VMs are using NFS-based storage for their disks. NFS connection saturation can cause VM I/O operations to stall, leading to apparent VM freezes. | If this metric approaches the maximum supported NFS connections on a node providing NFS storage for VM disks, VMs may experience slow I/O operations or apparent freezes as their disk operations queue. | `node_nfsd_connections_total > 100` | Medium |
| `node_nfsd_rpc_errors_total` | Total count of NFS Remote Procedure Call (RPC) errors. Impact: Critical for VMs using NFS-backed storage. RPC errors can cause I/O errors within VMs, leading to filesystem corruption or application failures inside the VM. | Even a small number of NFS RPC errors can cause VM disk I/O failures, potentially corrupting VM filesystems or causing applications inside VMs to crash due to I/O errors. | `increase(node_nfsd_rpc_errors_total[15m]) > 0` | High |
| `node_role_os_version_machine:cpu_capacity_cores:sum` | Aggregated sum of CPU cores capacity across nodes, broken down by node role, OS version, and machine type. Impact: Critical for VM capacity planning and placement, as different classes of nodes may be designated for specific VM workloads. Insufficient CPU capacity directly impacts VM density and performance. | When planning to deploy memory-intensive VMs, you'd use this metric to ensure targeted nodes (e.g., compute vs. infrastructure) have sufficient available CPU resources to handle the workload without overprovisioning. | `node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io="worker"}` | High |
| `node_thermal_zone_temp` | Zone temperature in Celsius for node hardware components. Impact: VM workloads can generate significant heat, and thermal events directly affect hypervisor performance through CPU throttling, which cascades to all VMs on that node. | If temperatures approach critical thresholds on compute nodes running VMs, performance degradation from thermal throttling may occur, causing inconsistent VM performance or unexpected VM migrations. | `max(node_thermal_zone_temp) by (instance) > 80` | High |
| `node_vmstat_oom_kill` | Count of Out-of-Memory kill events from /proc/vmstat. Impact: OOM events on nodes running OpenShift Virtualization are catastrophic as they can terminate the virt-launcher pods running VMs, causing immediate VM termination and potential data corruption. | If this metric increases on nodes running VMs, it indicates severe memory pressure that may have resulted in VM terminations, requiring immediate investigation of memory overcommit settings. | `increase(node_vmstat_oom_kill[15m]) > 0` | Critical |
| `node_watchdog_bootstatus` | Hardware watchdog boot status indicator. Impact: In virtualization environments, hardware watchdog failures may indicate underlying hardware issues on VM host nodes that could lead to unexpected reboots and VM outages. | If this metric shows non-zero values, it could indicate that a node hosting virtualization workloads experienced a non-clean boot or hardware instability that may affect VM reliability. | `node_watchdog_bootstatus > 0` | Medium |
| `node_watchdog_timeleft_seconds` | Represents seconds remaining before the hardware watchdog timer would trigger a reboot if not reset. Impact: In properly functioning systems, this value is typically constant as the watchdog is continuously being reset. **This is NOT an indicator of node health problems when stable at a low value.** | If this value remains stable across all nodes, this indicates normal operation where the watchdog is being reset regularly by the kernel. Only a decreasing value without reset would indicate a problem. | `rate(node_watchdog_timeleft_seconds[5m]) < 0` | Low |
| `openshift_auth_form_password_count_result` | Count of web console login attempts by result. Impact: Web console access is key for VM management in OpenShift Virtualization, and authentication issues could indicate attempts to compromise VM management. | Multiple failed logins from the same source may indicate attempts to access the virtualization console to manipulate VMs or their configurations. | `sum(rate(openshift_auth_form_password_count_result{result="error"}[15m])) > 5` | Medium |
| `openshift:cpu_usage_cores:sum` | Total CPU usage by OpenShift infrastructure components. Impact: High platform overhead reduces available CPU for VMs, affecting performance and density. | During VM migration or creation operations, if this value spikes significantly, platform components may be consuming CPU resources needed for VM operations, causing delays or performance issues. | `openshift:cpu_usage_cores:sum / on() group_left() sum(cluster:capacity_cpu_cores:sum) > 0.3` | Medium |
| `openshift:memory_usage_bytes:sum` | Total memory usage by OpenShift infrastructure components. Impact: High memory usage by platform components reduces available memory for VMs, affecting performance, density, and risking OOM events. | If memory usage by platform components is high on nodes intended for VM workloads, VM density will be reduced, and memory-intensive VMs may experience performance degradation or fail to be scheduled. | `openshift:memory_usage_bytes:sum / on() group_left() sum(cluster:capacity_memory_bytes:sum) > 0.3` | Medium |
| `openshift_etcd_operator_signer_expiration_days` | Days until the etcd operator signing certificates expire. Impact: Certificate expiration could disrupt etcd, which would affect all VM control operations and potentially make the virtualization platform unavailable. | If this value drops below 14 days, certificate renewal should be prioritized to prevent disruption to VM lifecycle operations like creation, migration, and snapshot management. | `openshift_etcd_operator_signer_expiration_days < 14` | High |
| `ovn_controller_bridge_mappings` | Status metric for OVN controller bridge mappings configuration. Impact: In OpenShift Virtualization, bridge mappings are critical for connecting VMs to physical networks, especially for VMs requiring direct layer 2 connectivity. | If bridge mappings are misconfigured or unavailable, VMs with attachments to secondary networks or requiring physical network access may lose connectivity or fail to start. | `ovn_controller_bridge_mappings == 0` | High |
| `ovn_controller_ct_zone_commit_95th_percentile` | 95th percentile time taken to commit connection tracking zones in OVN. Impact: Connection tracking affects stateful firewall rules for VM network traffic, and slow commits may cause packet drops or connection issues. | If this value is high, VMs with high network traffic may experience connection tracking issues, packet drops, or unexpected connection resets, especially during traffic spikes. | `ovn_controller_ct_zone_commit_95th_percentile > 100` | Medium |
| `ovn_controller_if_status_mgr_run_maximum` | Maximum execution time for the OVN interface status manager. Impact: Interface status affects VM network connectivity detection and recovery, and slow execution can delay network failure recovery. | If this value is high, detection and recovery of network interface failures for VMs may be delayed, extending network outages for virtualized workloads. | `ovn_controller_if_status_mgr_run_maximum > 200` | Medium |
| `ovn_controller_lflow_run` | Execution time metrics for logical flow processing in OVN. Impact: Logical flows control VM network traffic paths, and slow processing delays network policy and route updates. | During network policy updates affecting VMs, slow logical flow processing could delay the application of security rules, temporarily leaving VMs with incorrect network access. | `increase(ovn_controller_lflow_run[5m]) > 1000` | Medium |
| `ovn_controller_monitor_all` | Status of OVN controller monitoring processes. Impact: Monitoring impacts OVN's ability to detect and respond to network changes that affect VM connectivity. | If this metric indicates monitoring is failing, changes to VM network configuration may not be detected or applied, leading to inconsistent network behavior for VMs. | `ovn_controller_monitor_all == 0` | High |
| `ovn_controller_southbound_database_connected` | Binary status indicating if OVN controller is connected to the southbound database (1 = connected, 0 = disconnected). Impact: Database disconnection prevents VM network updates and confkubevirt_vmi_memory_pgmajfault_totaliguration changes from propagating. | If this value is 0, the node's OVN controller cannot communicate with the central OVN database, preventing VM network configuration updates and potentially isolating VMs on that node. | `ovn_controller_southbound_database_connected == 0` | Critical |
| `ovn_controller_txn_error` | Count of OVN controller transaction errors. Impact: Transaction errors may cause VM network configuration failures, leading to connectivity issues or security policy application failures. | If transaction errors increase during VM creation or migration, the associated network setup may fail, leaving VMs with incorrect or missing network connectivity. | `increase(ovn_controller_txn_error[5m]) > 0` | High |
| `ovn_db_e2e_timestamp` | End-to-end timestamp information for OVN database operations. Impact: Measures database operation latency which affects how quickly VM network changes are applied. | If the difference between current time and this timestamp increases, it indicates OVN database operation delays that could slow VM network provisioning or policy updates. | `time() - ovn_db_e2e_timestamp > 30` | Medium |

