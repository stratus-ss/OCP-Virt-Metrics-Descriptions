
| Name | Description (with Impact) | Practical Example | Example Query | Severity |
|------|---------------------------|-------------------|---------------|----------|
| `alertmanager_cluster_health_score` | Health score of the Alertmanager cluster. Lower values are better, and zero means "totally healthy". Impact: Higher values indicate degraded Alertmanager functionality, which could lead to missing or delayed alert notifications. | If the score rises above 1, it may indicate network issues between Alertmanager instances or other cluster health problems. | `alertmanager_cluster_health_score > 1` | High |
| `alertmanager_nflog_query_errors_total` | Counter for the number of notification log queries that failed. Impact: Increasing values indicate problems with the notification log system, potentially causing alerts to be lost or duplicated. | If this counter increases rapidly, it may indicate issues with the notification log storage or retrieval mechanism. | `rate(alertmanager_nflog_query_errors_total[5m]) > 0.1` | Medium |
| `alertmanager_marked_alerts` | Number of alerts marked by state (active, suppressed, unprocessed). Impact: Abnormal values may indicate alerts not being properly processed. | A high number of unprocessed alerts could indicate that Alertmanager is falling behind in processing alerts. | `sum(alertmanager_alerts{state="unprocessed"}) > 10` | Medium |
| `alertmanager_notification_requests_failed_total` | The total number of failed notification requests. Impact: Increasing values indicate that alert notifications are failing to be delivered to configured receivers. | If email notifications are failing, this metric would increase with the "integration=email" label. | `rate(alertmanager_notification_requests_failed_total{integration="email"}[5m]) > 0` | High |
| `alertmanager_notifications_failed_total` | Counter showing how many notifications have failed in total. Impact: Indicates problems with notification delivery to specific integrations. | If Slack notifications are failing, this would increase with the "integration=slack" label. | `increase(alertmanager_notifications_failed_total{integration="slack"}[1h]) > 0` | High |
| `alertmanager_silences_query_errors_total` | Counter for errors encountered when querying the silences database. Impact: Increasing values indicate problems with silence management, potentially causing alerts to fire when they should be silenced. | If this counter increases, it may indicate corruption in the silences database or other storage issues. | `increase(alertmanager_silences_query_errors_total[1h]) > 0` | Medium |
| `active_argocd_instances_total` | Shows the number of Argo CD instances currently managed across the cluster. Impact: Unexpected changes may indicate issues with Argo CD operator. | If this drops to 0 when you expect active instances, it indicates Argo CD instances are not being properly tracked. | `active_argocd_instances_total  0` | High |
| `apiserver_admission_controller_admission_duration_seconds_bucket` | Histogram of admission controller latencies. Impact: High latencies can slow down all API operations, affecting cluster performance. | If the 95th percentile latency exceeds 1 second, API operations will be noticeably slower. | `histogram_quantile(0.95, sum(rate(apiserver_admission_controller_admission_duration_seconds_bucket[5m])) by (le))` | Medium |
| `apiserver_audit_requests_rejected_total` | Counter of rejected audit requests. Impact: Increasing values indicate audit logging issues, potentially affecting compliance requirements. | If this counter increases rapidly, it could indicate audit backend problems or configuration issues. | `rate(apiserver_audit_requests_rejected_total[5m]) > 0` | Medium |
| `apiserver_current_inflight_requests` | Current number of requests being processed. Impact: High values indicate API server overload, potentially causing timeouts and degraded performance. | If this exceeds the configured max-in-flight limit, new requests will be rejected. | `sum(apiserver_current_inflight_requests) by (request_kind) > 100` | High |
| `apiserver_flowcontrol_current_executing_requests` | Number of requests currently being executed. Impact: High values may indicate API priority and fairness (APF) issues. | If a non-critical priority level is consuming too many resources, it could starve critical requests. | `sum(apiserver_flowcontrol_current_executing_requests) by (priorityLevel) > 50` | Medium |
| `apiserver_flowcontrol_current_executing_seats` | Number of seats currently occupied by executing requests. Impact: Relates to API server concurrency control. | If this approaches the upper limit, it indicates the API server is at maximum capacity. | `sum(apiserver_flowcontrol_current_executing_seats) / sum(apiserver_flowcontrol_upper_limit_seats) > 0.8` | Medium |
| `apiserver_flowcontrol_priority_level_request_utilization_count` | Indicates request concurrency utilization by priority level. Impact: High values may indicate certain priority levels are overloaded. | If the "system-node" priority level is highly utilized, node-related operations may be affected. | `apiserver_flowcontrol_priority_level_request_utilization_count{priorityLevel="system-node"} / apiserver_flowcontrol_upper_limit_seats{priorityLevel="system-node"} > 0.9` | Medium |
| `apiserver_flowcontrol_priority_level_seat_utilization_count` | Indicates seat concurrency utilization by priority level. Impact: High values may indicate certain priority levels are consuming too many resources. | If a workload priority level is using most available seats, it could impact other operations. | `apiserver_flowcontrol_priority_level_seat_utilization_count{priorityLevel="workload-high"} / apiserver_flowcontrol_upper_limit_seats{priorityLevel="workload-high"} > 0.9` | Medium |
| `apiserver_flowcontrol_request_dispatch_no_accommodation_total` | Total number of requests that could not be accommodated due to concurrency limits. Impact: Increasing values indicate requests being rejected due to overload. | If this increases for critical priority levels, important operations may be failing. | `increase(apiserver_flowcontrol_request_dispatch_no_accommodation_total{priorityLevel="system-cluster-critical"}[5m]) > 0` | High |
| `apiserver_flowcontrol_request_wait_duration_seconds_bucket` | Histogram of request waiting times. Impact: High wait times indicate API server congestion. | If the 95th percentile wait time exceeds 1 second, API operations will be noticeably delayed. | `histogram_quantile(0.95, sum(rate(apiserver_flowcontrol_request_wait_duration_seconds_bucket[5m])) by (le, priorityLevel))` | Medium |
| `apiserver_flowcontrol_upper_limit_seats` | Maximum number of seats available per priority level. Impact: This is a configuration limit that affects concurrency. | This is typically a static value but is important for calculating utilization percentages. | `apiserver_flowcontrol_upper_limit_seats` | Low |
| `apiserver_request:burnrate1d` | API request burn rate over 1 day. This is measured by taking the failed API requests divided by the total number of requests. This is a ratio for a given time period (in this case the ratio over 1 day) Impact: High values indicate sustained API server load. | If this exceeds SLO thresholds, it indicates the API server has been overloaded for an extended period. | `apiserver_request:burnrate1d > 2` | Medium |
| `apiserver_request:burnrate30m` | API request burn rate over 30 minutes. This is measured by taking the failed API requests divided by the total number of requests. This is a ratio for a given time period (in this case the ratio over 30 minutes) Impact: High values indicate medium-term API server load. | If this exceeds SLO thresholds, it indicates the API server has been overloaded recently. | `apiserver_request:burnrate30m > 5` | High |
| `apiserver_request:burnrate5m` | API request burn rate over 5 minutes. This is measured by taking the failed API requests divided by the total number of requests. This is a ratio for a given time period (in this case the ratio over 5 minutes). Impact: High values indicate short-term API server load spikes. | If this exceeds SLO thresholds, it indicates the API server is currently overloaded. | `apiserver_request:burnrate5m > 10` | Critical |
| `apiserver_request_post_timeout_total` | Total number of POST requests that have timed out. Impact: Increasing values indicate API server performance issues. | If this increases for create operations, new resources may fail to be created. | `increase(apiserver_request_post_timeout_total{verb="create"}[5m]) > 0` | High |
| `apiserver_request_sli_duration_seconds_count` | Count of requests used for SLI (Service Level Indicator) calculations. Impact: Used for SLO monitoring. | This is primarily used for calculating error rates and latencies against SLOs. | `sum(rate(apiserver_request_sli_duration_seconds_count{code=~"5.."}[5m])) / sum(rate(apiserver_request_sli_duration_seconds_count[5m]))` | Medium |
| `apiserver_request_slo_duration_seconds_count` | Count of requests used for SLO (Service Level Objective) calculations. Impact: Used for SLO monitoring. | Similar to SLI metric, used for calculating compliance with SLOs. | `sum(rate(apiserver_request_slo_duration_seconds_count{code=~"5.."}[5m])) / sum(rate(apiserver_request_slo_duration_seconds_count[5m]))` | Medium |
| `apiserver_storage_data_key_generation_failures_total` | Total number of failed data encryption key generation operations. Impact: Increasing values indicate issues with etcd encryption. | If this increases, it could indicate problems with the encryption provider or configuration. | `increase(apiserver_storage_data_key_generation_failures_total[1h]) > 0` | Critical |
| `apiserver_storage_size_bytes` | Size of the storage database file physically allocated in bytes. Impact: High values may indicate etcd database growth issues. | If this approaches the etcd size limit (typically 8GB), it could lead to API server instability. | `apiserver_storage_size_bytes > 7*1024*1024*1024` | High |
| `authenticated_user_requests` | Count of authenticated requests. Impact: Unexpected changes may indicate authentication issues. | A sudden drop could indicate authentication system problems. | `rate(authenticated_user_requests[5m])` | Medium |
| `authentication_attempts` | Count of authentication attempts. Impact: High failure rates indicate authentication configuration issues or potential security incidents. | A high rate of failed attempts could indicate a brute force attack. | `sum(rate(authentication_attempts{result="error"}[5m])) / sum(rate(authentication_attempts[5m])) > 0.5` | High |
| `apiserver_admission_webhook_rejection_count` | Alert for high rate of admission controller errors. Impact: Indicates problems with admission controllers, potentially preventing resource creation. | If ValidatingAdmissionWebhooks are failing, custom resources may not be created properly. | `sum(rate(apiserver_admission_webhook_rejection_count[5m])) / sum(rate(apiserver_admission_webhook_request_total[5m])) > 0.05` | High |
| `cco_credentials_requests_conditions` | Tracks the status conditions of Cloud Credential Operator credential requests. Impact: Indicates issues with cloud provider credentials that could affect cluster functionality. | If this metric shows credential requests in a degraded state, cloud resources may be inaccessible. | `cco_credentials_requests_conditions{condition="Degraded", status="True"} > 0` | High |
| `cluster_operator_conditions` | Represents the health conditions of core cluster operators. Impact: Degraded operators can affect overall cluster functionality. | If the OpenShift Virtualization operator shows a False Ready condition, virtualization features may be unavailable. | `cluster_operator_conditions{name="kubevirt-hyperconverged", condition="Ready", status="False"} == 1` | Critical |
| `cluster_operator_payload_errors` | Counts errors encountered during cluster operator payload processing. Impact: Increasing values indicate problems with operator updates. | If this increases during an upgrade, it may indicate problems applying new operator versions. | `sum(rate(cluster_operator_payload_errors[15m])) > 0` | High |
| `cluster:apiserver_current_inflight_requests:sum:max_over_time:2m` | Maximum number of concurrent requests to the API server over a 2-minute window. Impact: High values indicate API server congestion. | If this exceeds 100 for mutating requests, the API server may be overloaded, causing slow responses or failures. | `cluster:apiserver_current_inflight_requests:sum:max_over_time:2m{request_kind="mutating"} > 100` | High |
| `cluster:capacity_memory_bytes:sum` | Total memory capacity of the cluster in bytes. Impact: Used for capacity planning and resource monitoring. | If this value is close to the memory usage, the cluster may be running out of memory capacity. | `cluster:memory_usage_bytes:sum / cluster:capacity_memory_bytes:sum > 0.85` | Medium |
| `cluster:memory_usage_bytes:sum` | Total memory usage across the cluster in bytes. Impact: High values relative to capacity indicate potential resource constraints. | If memory usage exceeds 85% of capacity, workloads may experience memory pressure. | `cluster:memory_usage_bytes:sum / cluster:capacity_memory_bytes:sum > 0.85` | Medium |
| `cluster:usage:resources:sum` | Aggregated resource usage across the cluster. Impact: Helps identify resource consumption trends. | If CPU usage is consistently high, it may indicate the need for cluster scaling. | `cluster:usage:resources:sum{resource="cpu"} / cluster:capacity_cpu_cores:sum > 0.9` | Medium |
| `cluster:control_plane:all_nodes_ready` | Indicates whether all control plane nodes are in a ready state (1 for yes, 0 for no). Impact: Non-ready control plane nodes affect cluster stability. | If this metric is 0, at least one control plane node is not ready, which could affect high availability. | `cluster:control_plane:all_nodes_ready == 0` | Critical |
| `cluster:master_nodes` | Count of master/control plane nodes in the cluster. Impact: Unexpected changes may indicate node failures. | If this drops below the expected number (typically 3), cluster control plane redundancy is compromised. | `cluster:master_nodes  0.85` | Medium |
| `container_fs_usage_bytes` | Filesystem usage in bytes per container. Impact: High values may indicate disk space issues within containers. | If a container's filesystem usage approaches its limit, the container may experience write failures. | `container_fs_usage_bytes / container_fs_limit_bytes > 0.9` | Medium |
| `DiskSpaceLow` | Alert indicating low disk space on a node or volume. Impact: Low disk space can lead to service disruptions. | If this alert fires, a node or volume is running out of disk space and requires attention. | `DiskSpaceLow{severity="warning"}` | High |
| `Disk Space Limit Approaching` | Alert indicating disk space is approaching its limit. Impact: Potential future disk space issues. | If this alert fires, proactive action is needed to prevent disk space exhaustion. | `Disk Space Limit Approaching{severity="warning"}` | Medium |
| `cluster:network_attachment_definition_enabled_instance_up:max` | Maximum number of enabled network attachment definition instances that are up. Impact: Indicates health of multus CNI attachments. | If this is 0 when using secondary networks, pods requiring additional networks may fail to start. | `cluster:network_attachment_definition_enabled_instance_up:max == 0` | High |
| `coredns_forward_healthcheck_broken_total` | Total number of failed CoreDNS forward health checks. Impact: Increasing values indicate DNS forwarding issues. | If this increases rapidly, DNS resolution to external domains may fail. | `rate(coredns_forward_healthcheck_broken_total[5m]) > 0` | High |
| `coredns_health_request_failures_total` | Total number of CoreDNS health check failures. Impact: Indicates DNS service health issues. | If this increases, the CoreDNS service may be experiencing problems affecting DNS resolution. | `rate(coredns_health_request_failures_total[5m]) > 0` | High |
| `coredns_panics_total` | Total number of panics in CoreDNS. Impact: Indicates serious issues with DNS service. | Any increase in this metric indicates CoreDNS is experiencing critical failures. | `increase(coredns_panics_total[15m]) > 0` | Critical |
| `container_cpu_cfs_throttled_periods_total` | Total number of periods that a container was throttled due to CPU limits. Impact: High values indicate CPU contention. | If a critical container shows high throttling, it may experience performance degradation. | `rate(container_cpu_cfs_throttled_periods_total{namespace="openshift-virtualization"}[5m]) / rate(container_cpu_cfs_periods_total{namespace="openshift-virtualization"}[5m]) > 0.25` | Medium |
| `container_cpu_cfs_throttled_seconds_total` | Total time (in seconds) that a container was throttled due to CPU limits. Impact: High values indicate CPU contention. | If virt-launcher pods show high throttling, VM performance may be affected. | `rate(container_cpu_cfs_throttled_seconds_total{namespace="openshift-virtualization", pod=~"virt-launcher-.*"}[5m]) > 0.1` | Medium |
| `container_memory_failcnt` | Number of memory allocation failures in a container. Impact: Indicates memory pressure. | If this is increasing, containers are hitting memory limits and failing to allocate memory. | `increase(container_memory_failcnt[5m]) > 0` | High |
| `container_memory_swap` | Amount of swap space used by a container. Impact: High values indicate memory pressure. | If containers are using swap, it can significantly degrade performance. | `container_memory_swap > 0` | Medium |
| `container_memory_usage_bytes` | Memory usage of a container in bytes. Impact: High values relative to limits may lead to OOM kills. | If a container's memory usage approaches its limit, it may be terminated by the OOM killer. | `container_memory_usage_bytes / container_memory_working_set_bytes > 0.9` | Medium |
| `container_oom_events_total` | Total number of OOM (Out of Memory) events for a container. Impact: Indicates memory-related container terminations. | If this increases, containers are being killed due to memory pressure. | `increase(container_oom_events_total[15m]) > 0` | High |
| `container_runtime_crio_containers_oom_total` | Total number of OOM events for containers managed by CRI-O. Impact: Indicates memory-related container terminations. | If this increases, CRI-O containers are being killed due to memory pressure. | `increase(container_runtime_crio_containers_oom_total[15m]) > 0` | High |
| `containerd_cri_input_bytes_total` | Total bytes received by containerd CRI. Impact: Helps monitor container runtime network traffic. | Sudden spikes may indicate unusual container activity or potential issues. | `rate(containerd_cri_input_bytes_total[5m]) > 1e6` | Low |
| `cnv:vmi_status_running:count` | Count of running Virtual Machine Instances. Impact: Unexpected changes may indicate VM issues. | If this drops suddenly, VMs may be failing or being terminated unexpectedly. | `sum by(node) (cnv:vmi_status_running:count)` <br><br>`sum by (guest_os_name)(cnv:vmi_status_running:count)| Medium |
| `cnv_abnormal` | The metric tracks two specific memory conditions:<ul><li>`memory_working_set_delta_from_request`: The difference between the working set memory and the requested memory</ul></li> <ul><li>`memory_rss_delta_from_request`: The difference between the resident set size (RSS) memory and the requested memory </ul></li>Impact: Abnormal conditions may affect VM functionality. | Positive values indicate the component is using more memory than requested, while negative values indicate it's using less than requested.<br><br> Large positive values could indicate memory pressure in your virtualization components. Consistently high values might suggest you need to adjust the resource requests for your virtualization components| `sum by (container) (cnv_abnormal{reason="memory_working_set_delta_from_request"})` | High |
| `controller_runtime_reconcile_panics_total` | Total number of panics during controller reconciliation. Impact: Indicates serious issues with Kubernetes controllers. | Any increase in this metric indicates controllers are experiencing critical failures. | `increase(controller_runtime_reconcile_panics_total[15m]) > 0` | Critical |
| `controller_runtime_terminal_reconcile_errors_total` | Total number of terminal errors during controller reconciliation. Impact: Indicates controller failures. | If this increases for a specific controller, resources managed by that controller may be in a bad state. | `increase(controller_runtime_terminal_reconcile_errors_total[15m]) > 0` | High |
| `ErrorRateIncrease` | Alert for increased error rates in API requests. Impact: Indicates API server issues affecting operations. | If this alert fires, the API server is experiencing an elevated error rate, potentially affecting cluster operations. | `ErrorRateIncrease{severity="warning"}` | High |
| `component_resource:apiserver_request_terminations_total:rate:1m` | Rate of API server request terminations over 1 minute. Impact: High values indicate API server overload. | If this exceeds normal baseline, the API server is terminating requests due to overload. | `component_resource:apiserver_request_terminations_total:rate:1m > 10` | High |
| `component_resource:apiserver_request_terminations_total:rate:5m` | Rate of API server request terminations over 5 minutes. Impact: High values indicate sustained API server overload. | If this exceeds normal baseline, the API server is consistently terminating requests. | `component_resource:apiserver_request_terminations_total:rate:5m > 5` | High |
| `etcd_disk_backend_commit_duration_seconds_bucket` | Histogram of latency distribution of commit operations called by the backend. Impact: High latencies indicate disk performance issues affecting etcd write operations. | If the 99th percentile exceeds 25ms, it may indicate disk performance issues affecting etcd stability. | `histogram_quantile(0.99, sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (le))` | High |
| `etdc_disk_backend_defrag_duration_seconds_bucket` | Histogram of latency distribution of defragmentation operations. Impact: High values indicate defragmentation is taking too long, potentially affecting cluster performance. | If defragmentation operations consistently take more than 10 seconds, it may indicate underlying storage issues. | `histogram_quantile(0.95, sum(rate(etdc_disk_backend_defrag_duration_seconds_bucket[5m])) by (le))` | Medium |
| `etcd_disk_wal_fsync_duration_seconds_bucket` | Histogram of latency distribution of fsync operations to the WAL (Write-Ahead Log). Impact: High values indicate disk I/O issues affecting etcd performance. | If the 99th percentile exceeds 10ms, it may indicate slow disk performance leading to potential leader election issues and slow writes. | `histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) by (le, instance))` | Critical |
| `etcd_disk_wal_fsync_duration_seconds_sum` | Sum of time spent on fsync operations to the WAL. Impact: Increasing values indicate cumulative disk I/O pressure. | This metric is used with the count to calculate average fsync duration, which should be monitored for trends. | `rate(etcd_disk_wal_fsync_duration_seconds_sum[5m]) / rate(etcd_disk_wal_fsync_duration_seconds_count[5m])` | Medium |
| `etcd_mvcc_db_total_size_in_bytes` | Total size of the etcd database in bytes. Impact: As this approaches the quota, etcd may stop accepting writes. | If this exceeds 80% of the quota (typically 2-8GB), compaction and defragmentation may be needed to prevent etcd from becoming read-only. | `etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes > 0.8` | Critical |
| `etcd_server_quota_backend_bytes` | Maximum backend size in bytes etcd can use before triggering an alarm. Impact: Defines the limit for etcd database size. | This is typically set to 2-8GB. If the database size approaches this value, maintenance is required. | `etcd_server_quota_backend_bytes` | Low |
| `etcd_debugging_raft_terms_total` | Total number of Raft terms seen by this etcd member. Impact: Rapid increases indicate frequent leader elections and potential instability. | If this increases rapidly over a short period, it indicates cluster instability with frequent leadership changes. | `delta(etcd_debugging_raft_terms_total[15m]) > 5` | High |
| `etcd_network_active_peers` | Number of active peers in the etcd cluster. Impact: Values below expected indicate network connectivity issues. | In a 3-node etcd cluster, if this drops below 2 for any member, quorum may be at risk. | `etcd_network_active_peers  0` | High |
| `etcd_server_has_leader` | Whether this etcd member has a leader (1) or not (0). Impact: Values of 0 indicate split-brain or isolation. | If this is 0 for any etcd member, that member cannot process writes and may be network isolated. | `etcd_server_has_leader == 0` | Critical |
| `etcd_server_proposals_failed_total` | Total number of failed proposals (consensus operations). Impact: Increasing values indicate consensus issues. | If this increases, it indicates problems with the Raft consensus algorithm, potentially due to network issues or overload. | `increase(etcd_server_proposals_failed_total[15m]) > 0` | High |
| `etcd_lease_object_counts_count` | Number of objects attached to leases. Impact: High values may indicate resource leaks. | If this grows continuously without bounds, it may indicate applications not properly releasing leases. | `etcd_lease_object_counts_count > 1000` | Medium |
| `etcd_server_client_requests_total` | Total number of client requests received by type. Impact: Rate changes indicate client load patterns. | Monitoring the rate of write requests can help identify periods of high load on the etcd cluster. | `sum(rate(etcd_server_client_requests_total{type="write"}[5m]))` | Medium |
| `go_cpu_classes_gc_total_cpu_seconds_total` | Total CPU time spent on garbage collection. Impact: High values indicate excessive GC activity. | If this is increasing rapidly, it may indicate memory pressure in the Go runtime. | `rate(go_cpu_classes_gc_total_cpu_seconds_total[5m]) > 0.1` | Medium |
| `go_gc_gomemlimit_bytes` | Memory limit for the Go runtime in bytes. Impact: Defines the ceiling for Go memory usage. | This is a configuration setting that helps understand the memory constraints for the process. | `go_gc_gomemlimit_bytes` | Low |
| `go_gc_heap_live_bytes` | Size of the live objects heap in bytes. Impact: High values relative to limits indicate memory pressure. | If this approaches the total heap size, it may indicate memory leaks or high memory pressure. | `go_gc_heap_live_bytes / go_memory_classes_total_bytes > 0.8` | Medium |
| `go_godebug_non_default_behavior_panicnil_events_total` | Total number of nil panic events. Impact: Any non-zero value indicates programming errors. | If this increases, it indicates nil pointer dereferences in the code, which should never happen in production. | `increase(go_godebug_non_default_behavior_panicnil_events_total[15m]) > 0` | Critical |
| `go_memory_classes_metadata_mspan_inuse_bytes` | Size of mspan structures currently in use in bytes. Impact: Indicates memory used for internal Go runtime structures. | This is primarily useful for detailed Go runtime memory analysis. | `go_memory_classes_metadata_mspan_inuse_bytes` | Low |
| `go_memory_classes_total_bytes` | Total size of Go memory classes in bytes. Impact: Indicates overall memory usage by the Go runtime. | This helps understand the total memory footprint of the process. | `go_memory_classes_total_bytes` | Medium |
| `endpoint_slice_controller_endpoints_desired` | Number of endpoints desired by the EndpointSlice controller. Impact: Indicates load on the EndpointSlice controller. | If this grows very large, it may indicate a large number of services or endpoints that could affect controller performance. | `endpoint_slice_controller_endpoints_desired > 10000` | Medium |
| `group_sync_error` | Indicates errors during group synchronization. Impact: May affect cluster consistency in relation to permissions within the cluster. | If this increases, it could indicate issues with group membership updates. | `increase(group_sync_error[15m]) > 0` | Medium |
| `grpc_req_panics_recovered_total` | Total number of gRPC request panics recovered. Impact: Increasing values indicate gRPC service instability. | If this increases rapidly, it may indicate issues with gRPC request handling. | `rate(grpc_req_panics_recovered_total[5m]) > 0` | Medium |
| `haproxy_backend_connection_errors_total` | Total number of connection errors on HAProxy backends. Impact: Increasing values indicate issues with backend connectivity. | If this increases for a specific backend, it may indicate server or network issues. | `rate(haproxy_backend_connection_errors_total[5m]) > 0` | Medium |
| `haproxy_backend_http_average_response_latency_milliseconds` | Average response latency of HAProxy backend HTTP requests in milliseconds. Impact: High values indicate slow backend responses. | If this exceeds 500ms for a critical service, it may impact user experience. | `haproxy_backend_http_average_response_latency_milliseconds > 500` | Medium |
| `haproxy_backend_http_responses_total` | Total number of HTTP responses from HAProxy backends. Impact: Used to monitor backend request volume. | Trend analysis used to determine activity in the cluster. | `rate(haproxy_backend_http_responses_total[5m]) == 0` | Medium |
| `haproxy_backend_up` | Indicates whether HAProxy backends are up (1) or down (0). Impact: Values of 0 indicate backend availability issues. | If this is 0 for a critical backend, traffic may not be routed correctly. | `haproxy_backend_up == 0` | Critical |
| `haproxy_server_connections_total` | Total number of connections to HAProxy servers. Impact: Used to monitor server load. | If this increases rapidly, it may indicate a sudden spike in traffic. | `rate(haproxy_server_connections_total[5m]) > 100` | Medium |
| `haproxy_server_downtime_seconds_total` | Total downtime of HAProxy servers in seconds. Impact: Increasing values indicate server availability issues. | If this increases for a critical server, it may indicate recurring outages. | `increase(haproxy_server_downtime_seconds_total[15m]) > 0` | Medium |
| `haproxy_server_up` | Indicates whether HAProxy servers are up (1) or down (0). Impact: Values of 0 indicate server availability issues. | If this is 0 for a critical server, it may indicate a server failure. | `haproxy_server_up == 0` | Critical |
| `haproxy_up` | Indicates whether HAProxy is up (1) or down (0). Impact: Values of 0 indicate HAProxy service issues. | If this is 0, the HAProxy service is not running or not reachable. | `haproxy_up == 0` | Critical |
| `High Admission Errors Rate` | Alert for high rate of admission controller errors. Impact: Indicates problems with admission controllers, potentially preventing resource creation. | If ValidatingAdmissionWebhooks are failing, custom resources may not be created properly. | `sum(rate(apiserver_admission_webhook_rejection_count[5m])) / sum(rate(apiserver_admission_webhook_request_total[5m])) > 0.05` | High |
| `High Container Restart Rate` | Alert for high rate of container restarts. Impact: Indicates potential issues with container stability or resource constraints. | If containers are restarting frequently, it may indicate memory or CPU issues. | `sum(rate(kube_pod_container_status_restarts_total[5m])) > 10` | Medium |
| `High Error Rate` | Alert for high error rates in API requests or other operations. Impact: Indicates service instability or configuration issues. | If this alert fires, it may indicate problems with API server or backend services. | `sum(rate(apiserver_request_total{code=~"5.."}[5m])) / sum(rate(apiserver_request_total[5m])) > 0.1` | High |
| `HPA Status Condition Failure` | Indicates a failure in the Horizontal Pod Autoscaler (HPA) status condition. Impact: May prevent HPA from scaling workloads correctly. | If this condition is failing, HPA may not adjust pod counts based on resource utilization. | `kube_hpa_status_condition{condition="ScalingActive", status="False"} == 1` | Medium |
| `http_client_request_total` | Total number of HTTP client requests. Impact: Used to monitor client request volume. | If this drops suddenly, it may indicate client connectivity issues. | `rate(http_client_request_total[5m]) == 0` | Medium |
| `instance_request_kind:apiserver_current_inflight_requests:sum` | Sum of concurrent requests to the API server by request kind. Impact: High values indicate API server congestion. | If this exceeds 100 for mutating requests, the API server may be overloaded. | `instance_request_kind:apiserver_current_inflight_requests:sum{request_kind="mutating"} > 100` | High |
| `kube_cronjob_spec_suspend` | Indicates whether a CronJob is suspended. Impact: Suspended CronJobs will not run scheduled tasks. | If this is True for a critical CronJob, scheduled tasks may not execute. | `kube_cronjob_spec_suspend == "True"` | Medium |
| `kube_deployment_status_condition` | Status conditions of Deployments. Impact: Indicates health and readiness of Deployments. | If a Deployment shows a False Ready condition, pods may not be available for traffic. | `kube_deployment_status_condition{condition="Available", status="False"} == 1` | Medium |
| `kube_deployment_status_replicas_available` | Number of available replicas in a Deployment. Impact: Used to monitor Deployment health. | If this is less than the desired number, pods may not be fully available. | `kube_deployment_status_replicas_available  0` | Medium |
| `kube_job_status_failed` | Indicates failed Jobs. Impact: May indicate issues with Job execution or configuration. | If this increases for a critical Job, it may indicate recurring failures. | `kube_job_status_failed == 1` | Medium |
| `kubelet_memory_manager_pinning_errors_total` | Total number of memory pinning errors by the Kubelet memory manager. Impact: Indicates issues with memory allocation. | If memory pinning fails, workloads requiring guaranteed memory allocation may experience higher latency and decreased performance, especially in NUMA architectures. This metric should ideally always be 0 | `rate(kubelet_memory_manager_pinning_errors_total[5m]) > 0` | Medium |
| `kubelet_pleg_last_seen_seconds` | Time in seconds since the last PLEG (Pod Lifecycle Event Generator) update. Impact: High values indicate Kubelet issues. | If this exceeds 60 seconds, it may indicate Kubelet is not updating pod status correctly. | `kubelet_pleg_last_seen_seconds > 60` | Medium |
| `kubelet_pod_start_sli_duration_seconds_bucket` | Histogram of pod start latency in seconds. Impact: High values indicate slow pod startup times. | If the 95th percentile exceeds 30 seconds, pod startup may be slow, affecting service availability. | `histogram_quantile(0.95, sum(rate(kubelet_pod_start_sli_duration_seconds_bucket[5m])) by (le))` | Medium |
| `kubelet_volume_stats_available_bytes` | Available bytes in a volume. Impact: Used to monitor volume capacity and detect potential storage issues. | If available bytes are low, it may indicate that the volume is running out of space. | `kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.2` | Medium |
| `kubelet_volume_stats_inodes_free` | Number of free inodes in a volume. Impact: Low values indicate potential inode exhaustion. | If free inodes are less than 5% of total inodes, it may indicate inode exhaustion issues. | `kubelet_volume_stats_inodes_free / kubelet_volume_stats_inodes < 0.05` | Medium |
| `kube_pod_status_ready` | Indicates whether a pod is ready to serve requests. Impact: Non-ready pods may not receive traffic. | If a critical pod is not ready, it may indicate issues with the pod or its containers. | `kube_pod_status_ready{namespace="default", pod="critical-pod"} == 0` | High |
| `kube_pod_status_unschedulable` | Indicates whether a pod is unschedulable. Impact: Unschedulable pods cannot be placed on nodes. | If a pod is unschedulable due to resource constraints, it may indicate the need for node scaling. | `kube_pod_status_unschedulable{namespace="default", pod="critical-pod"} == 1` | Medium |
| `kube_running_pod_ready` | Number of running pods that are ready. Impact: Used to monitor pod health and availability. | If this drops suddenly, it may indicate pod failures or readiness issues. | `kube_running_pod_ready{namespace="default"} < 10` | Medium |
| `kubevirt_vmi_memory_swap_in_traffic_bytes` | Swap-in traffic for Virtual Machine Instances (VMIs) in bytes. Impact: High values indicate memory pressure. | Swap-in refers to the process of moving data from swap space (usually disk storage) back into the VM's RAM | `rate(kubevirt_vmi_memory_swap_in_traffic_bytes[5m]) > 100` | Medium |
| `kubevirt_vmi_memory_swap_out_traffic_bytes` | Swap-out traffic for Virtual Machine Instances (VMIs) in bytes. Impact: High values indicate memory pressure. | Sawp-out refers to the process of moving data from ram to swap space. If swap-out traffic is high, it may indicate that VMIs are experiencing memory constraints. | `rate(kubevirt_vmi_memory_swap_out_traffic_bytes[5m]) > 100` | Medium |
| `kubevirt_hco_hyperconverged_cr_exists` | Indicates whether the Hyperconverged Custom Resource exists. Impact: Non-existent CRs may affect hyperconverged functionality. | If this is False, it may indicate issues with hyperconverged deployment or configuration. | `kubevirt_hco_hyperconverged_cr_exists == 0` | Critical |
| `kubevirt_hco_system_health_status` | Numeric health status of the Hyperconverged Operator (1 = healthy, 0 = not healthy). Impact: A value of 0 indicates issues with hyperconverged components that may affect virtualization functionality. | If this metric returns 0, the HCO is reporting a problem requiring investigation. The HyperConverged CR creates corresponding CRs for the operators of all other components related to Compute, storage, networking, templating and scaling within OpenShift Virtualization. | `kubevirt_hco_system_health_status == 0` | Critical |
| `kubevirt_hyperconverged_operator_health_status` | Health status of the Hyperconverged Operator. Impact: Non-healthy status may indicate issues with hyperconverged components. | If this indicates a degraded status, it may affect hyperconverged functionality. | `kubevirt_hyperconverged_operator_health_status != 0` | High |
| `kubevirt_number_of_vms` | Number of Virtual Machines managed by KubeVirt. Impact: Used to monitor VM deployment and scaling. | If this drops suddenly, it may indicate VM termination or deployment issues. | `kubevirt_number_of_vms{namespace="default"} < 10` | Medium |
| `kubevirt_ssp_operator_up` | Indicates whether the SSP (Specialized Service Provider) operator, a core component of OpenShift Virtualization, is up and running. Impact: If not running (`0`), key virtualization features such as VM templates, common templates, and data import will not function, severely impacting OpenShift Virtualization operations. | If this metric is `0`, users will be unable to deploy new VMs from templates or use data import features until the operator is restored. | `kubevirt_ssp_operator_up == 0` | Critical |
| `kubevirt_virt_api_up` | Indicates whether the KubeVirt API is up and running. Impact: Non-running API may affect KubeVirt functionality. | If this is False, it may indicate issues with the API service or configuration. | `kubevirt_virt_api_up == 0` | Critical |
| `kubevirt_virt_controller_up` | Indicates whether the KubeVirt controller is up and running. **The KubeVirt controller is responsible for managing the lifecycle of all Virtual Machine Instances (VMIs) in the cluster, including creation, updates, and deletion.** Impact: If the controller is not running (`0`), no VMs can be scheduled, updated, or deleted, and the virtualization platform will be unable to react to changes in VM resources. This results in a complete loss of VM orchestration and automation. | If this metric returns `0`, VM lifecycle operations (such as creation, migration, or deletion) will not be processed, and VMs may become stuck in transitional states. | `kubevirt_virt_controller_up == 0` | Critical |
| `kubevirt_virt_handler_up` | Indicates whether the KubeVirt handler is up and running. **The KubeVirt handler runs as a DaemonSet on each node and is responsible for managing the actual execution of VMIs on the node, including monitoring, reporting status, and handling VM shutdown or migration.** Impact: If the handler is not running (`0`) on a node, VMIs on that node cannot be started, stopped, or migrated, and their status will not be reported accurately. This leads to loss of control and observability for VMs on affected nodes. | If this metric returns `0` for any node, VMs on that node may become unmanageable, unresponsive, or orphaned, severely impacting workload reliability. | `kubevirt_virt_handler_up == 0` | Critical |
| `kubevirt_virt_operator_up` | Indicates whether the KubeVirt operator is up and running. **The KubeVirt operator is responsible for deploying, updating, and maintaining all KubeVirt components (controllers, handlers, API, etc.) and ensuring their desired state.** Impact: If the operator is not running (`0`), the platform loses self-healing and automated management capabilities. This can prevent recovery from failures, upgrades, or configuration changes, and may eventually result in cluster drift or degraded service. | If this metric returns `0`, KubeVirt components will not be automatically reconciled, upgraded, or repaired, increasing risk of outages and operational drift. | `kubevirt_virt_operator_up == 0` | Critical |
| `kubevirt_vmi_cpu_usage_seconds_total` | Total CPU usage of Virtual Machine Instances (VMIs) in seconds. Impact: High values indicate CPU resource constraints. | If CPU usage is consistently high, it may indicate the need for VM scaling or resource adjustments. | `rate(kubevirt_vmi_cpu_usage_seconds_total[5m]) > 100` | Medium |
| `kubevirt_vmi_memory_available_bytes` | Available memory for Virtual Machine Instances (VMIs) in bytes. Impact: Low values indicate memory constraints. | If available memory is low, it may indicate that VMIs are experiencing memory pressure. | `kubevirt_vmi_memory_available_bytes{namespace="default"} < 1e9` | Medium |
| `kubevirt_vmi_memory_pgmajfault_total` | Total number of major page faults for Virtual Machine Instances (VMIs). Impact: High values indicate memory pressure. | If major page faults are increasing, it may indicate that VMIs are experiencing memory constraints. | `rate(kubevirt_vmi_memory_pgmajfault_total[5m]) > 0` | Medium |