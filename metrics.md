| Name | Description & Example | Example Query | Severity |
|------|----------------------|--------------|----------|
| `active_argocd_instances_total` | Shows the number of Argo CD instances currently managed across the cluster. Unexpected changes may indicate issues with Argo CD operator. For example, if this drops to 0 when you expect active instances, it indicates Argo CD instances are not being properly tracked. | `active_argocd_instances_total == 0` | High |
| `aggregator_unavailable_apiservice` | Number of APIService endpoints that are unavailable. Unavailable APIs can block VM management operations or integration with external platforms. | `aggregator_unavailable_apiservice > 0` | High |
| `alertmanager_cluster_health_score` | Health score of the Alertmanager cluster. Lower values are better, and zero means "totally healthy". Higher values indicate degraded Alertmanager functionality, which could lead to missing or delayed alert notifications. If the score rises above 1, it may indicate network issues between Alertmanager instances or other cluster health problems. | `alertmanager_cluster_health_score > 1` | High |
| `alertmanager_cluster_members` | Number of Alertmanager members in the cluster. Should match expected size. In OpenShift a size of 2 is expected. Lower values indicate degraded alerting HA, risking loss of VM/infrastructure alert delivery. | `alertmanager_cluster_members < 2` | Medium |
| `alertmanager_cluster_refresh_join_failed_total` | Total number of failed attempts to join the Alertmanager cluster. High values indicate cluster instability or misconfiguration, risking alerting HA. | `increase(alertmanager_cluster_refresh_join_failed_total[15m]) > 0` | High |
| `alertmanager_dispatcher_alert_processing_duration_seconds_count` | Number of alert events processed by the dispatcher per second. Used to monitor alert processing throughput; a sudden increase indicates an increasing number of problems observed in the platform | `rate(alertmanager_dispatcher_alert_processing_duration_seconds_count[5m]) == 0` | High |
| `alertmanager_http_concurrency_limit_exceeded_total` | Total HTTP requests rejected due to concurrency limits. Indicates Alertmanager is overloaded, risking dropped or delayed alerts for VM failures. | `increase(alertmanager_http_concurrency_limit_exceeded_total[15m]) > 0` | High |
| `alertmanager_http_request_duration_seconds_count` | 	Cumulative count of HTTP requests handled by Alertmanager, as tracked by the request duration histogram. Impact: This metric is a counter that increases with every HTTP request processed by Alertmanager. A sudden drop to zero in the rate of increase indicates that Alertmanager is not receiving or processing HTTP requests, which could mean the service is down, unreachable, or stalled. | `rate(alertmanager_http_request_duration_seconds_count[5m]) == 0` | Medium |
| `alertmanager_http_requests_in_flight` | Current number of concurrent HTTP requests being processed by Alertmanager. In most clusters, this value is typically very low, reflecting the fact that Alertmanager's API is lightly used except during bursts of alert delivery or configuration reloads. | `alertmanager_http_requests_in_flight > 20` | Medium |
| `alertmanager_marked_alerts` | Number of alerts marked by state (active, suppressed, unprocessed). Abnormal values may indicate alerts not being properly processed. A high number of unprocessed alerts could indicate that Alertmanager is falling behind in processing alerts. | `sum(alertmanager_alerts{state="unprocessed"}) > 10` | Medium |
| `alertmanager_nflog_gossip_messages_propagated_total` | Cumulative count of notification log ("nflog") gossip messages propagated by this Alertmanager instance. This metric tracks the number of notification log updates (such as sent notifications and silences) that have been gossiped to peers in an Alertmanager HA cluster. | `rate(alertmanager_nflog_gossip_messages_propagated_total[5m]) > 0` | High |
| `alertmanager_nflog_query_errors_total` | Counter for the number of notification log queries that failed. Increasing values indicate problems with the notification log system, potentially causing alerts to be lost or duplicated. If this counter increases rapidly, it may indicate issues with the notification log storage or retrieval mechanism. | `rate(alertmanager_nflog_query_errors_total[5m]) > 0.1` | Medium |
| `alertmanager_nflog_query_errors_total` | Total number of notification log query errors. High values indicate issues querying alert state, risking incorrect alert deduplication or suppression. | `increase(alertmanager_nflog_query_errors_total[15m]) > 0` | High |
| `alertmanager_nflog_snapshot_duration_seconds_count` | Cumulative count of notification log (nflog) snapshot operations in Alertmanager. This metric tracks how many times Alertmanager has taken a snapshot of its notification log, which is used for persisting alert state and supporting crash recovery and HA. | `rate(alertmanager_nflog_snapshot_duration_seconds_count[5m]) == 0` | Medium |
| `alertmanager_notification_latency_seconds_bucket` | Histogram of notification delivery latency. High values indicate delays in sending alerts for VM or infra failures, risking slow incident response. | `histogram_quantile(0.95, sum(rate(alertmanager_notification_latency_seconds_bucket[5m])) by (le)) > 5` | High |
| `alertmanager_notification_requests_failed_total` | The total number of failed notification requests. Increasing values indicate that alert notifications are failing to be delivered to configured receivers. If email notifications are failing, this metric would increase with the "integration=email" label. | `rate(alertmanager_notification_requests_failed_total{integration="email"}[5m]) > 0` | High |
| `alertmanager_notifications_failed_total` | Counter showing how many notifications have failed in total. Indicates problems with notification delivery to specific integrations. For example, if Slack notifications are failing, this would increase with the "integration=slack" label. | `increase(alertmanager_notifications_failed_total{integration="slack"}[1h]) > 0` | High |
| `alertmanager_silences_query_errors_total` | Counter for errors encountered when querying the silences database. Increasing values indicate problems with silence management, potentially causing alerts to fire when they should be silenced. If this counter increases, it may indicate corruption in the silences database or other storage issues. | `increase(alertmanager_silences_query_errors_total[1h]) > 0` | Medium |
| `apiextensions_apiserver_validation_ratcheting_seconds_sum` | Cumulative sum of seconds spent by the API server on ratcheting validation for CustomResourceDefinitions (CRDs). Ratcheting validation refers to the process where the API server enforces stricter schema validation rules as CRDs evolve between versions (e.g., from v1beta1 to v1). Excessive time spent here may indicate slow schema upgrades or problematic CRD changes, which can delay or disrupt VM CRD rollouts and upgrades in OpenShift Virtualization. | `increase(apiextensions_apiserver_validation_ratcheting_seconds_sum[15m]) > 5` | Low |
| `apiserver_admission_controller_admission_duration_seconds_bucket` | Histogram of admission controller latencies. High latencies can slow down all API operations, affecting cluster performance. If the 95th percentile latency exceeds 1 second, API operations will be noticeably slower. | `histogram_quantile(0.95, sum(rate(apiserver_admission_controller_admission_duration_seconds_bucket[5m])) by (le))` | Medium |
| `apiserver_admission_webhook_rejection_count` | Alert for high rate of admission controller errors. Indicates problems with admission controllers, potentially preventing resource creation. For example, if ValidatingAdmissionWebhooks are failing, custom resources may not be created properly. | `sum(rate(apiserver_admission_webhook_rejection_count[5m])) / sum(rate(apiserver_admission_webhook_request_total[5m])) > 0.05` | High |
| `apiserver_admission_webhook_rejection_count` | Alert for high rate of admission controller errors. Indicates problems with admission controllers, potentially preventing resource creation. If ValidatingAdmissionWebhooks are failing, custom resources may not be created properly. | `sum(rate(apiserver_admission_webhook_rejection_count[5m])) / sum(rate(apiserver_admission_webhook_request_total[5m])) > 0.05` | High |
| `apiserver_admission_webhook_rejection_count` | Count of admission webhook rejections, labeled by webhook, operation, and error type.  Frequent rejections can block VM creation, migration, or updates. | `increase(apiserver_admission_webhook_rejection_count[15m]) > 0` | Medium |
| `apiserver_audit_requests_rejected_total` | Counter of rejected audit requests. Increasing values indicate audit logging issues, potentially affecting compliance requirements. If this counter increases rapidly, it could indicate audit backend problems or configuration issues. | `rate(apiserver_audit_requests_rejected_total[5m]) > 0` | Medium |
| `apiserver_authorization_decisions_total` | Total number of authorization decisions (allow/forbid). Spikes in denied requests may indicate RBAC misconfiguration, blocking VM operations. | `sum(rate(apiserver_authorization_decisions_total{decision="forbid"}[5m])) > 0` | Medium |
| `apiserver_cache_list_total` | Total cache list operations. High values may indicate heavy API usage, possibly from VM controllers or monitoring. | `rate(apiserver_cache_list_total[5m]) > 100` | Low |
| `apiserver_client_certificate_expiration_seconds_bucket` | Histogram of client certificate expiration times. Imminent certificate expiry can break API access for VM controllers/operators. If you have less than 14 days until expiry and you manage the certs yourself, you will need to take action. | `histogram_quantile(0.01, sum(rate(apiserver_client_certificate_expiration_seconds_bucket[5m])) by (le)) < 1209600` | High |
| `apiserver_crd_conversion_webhook_duration_seconds_bucket` | Histogram of response times (in seconds) for CRD conversion webhooks. High latency in conversion webhooks can delay or block CRD object operations, including those for VM custom resources in OpenShift Virtualization. This can slow down VM creation, updates, or migrations if the conversion webhook is slow or unresponsive. | `histogram_quantile(0.95, sum(rate(apiserver_crd_conversion_webhook_duration_seconds_bucket[5m])) by (le)) > 1` | Medium |
| `apiserver_current_inflight_requests` | Current number of requests being processed. High values indicate API server overload, potentially causing timeouts and degraded performance. If this exceeds the configured max-in-flight limit, new requests will be rejected. | `sum(apiserver_current_inflight_requests) by (request_kind) > 100` | High |
| `apiserver_current_inqueue_requests` | Number of API requests currently queued. High values indicate API server overload, risking delays in VM operations. | `apiserver_current_inqueue_requests > 10` | High |
| `apiserver_delegated_authn_request_duration_seconds_bucket` | Histogram of durations (in seconds) for delegated authentication requests made by the Kubernetes API server. High latency in delegated authentication can delay all API calls that require authenticationâ€”including VM lifecycle operations in OpenShift Virtualization.| `histogram_quantile(0.95, sum(rate(apiserver_delegated_authn_request_duration_seconds_bucket[5m])) by (le)) > 1` | Medium |
| `apiserver_delegated_authz_request_duration_seconds_count` | Cumulative count of delegated authorization requests handled by the Kubernetes API server. This metric is most useful when combined with its companion sum metric to calculate average authorization latency. Spikes may signal RBAC or webhook bottlenecks that could delay or block VM operations in OpenShift Virtualization. | `rate(apiserver_delegated_authz_request_duration_seconds_count[5m]) > 100` | Low |
| `apiserver_delegated_authz_request_duration_seconds_sum` | Total time spent on delegated authorization. Used to compute average latency for authorization. | `rate(apiserver_delegated_authz_request_duration_seconds_sum[5m]) / rate(apiserver_delegated_authz_request_duration_seconds_count[5m]) > 1` | Medium |
| `apiserver_flowcontrol_current_executing_requests` | Number of requests currently being executed. High values may indicate API priority and fairness (APF) issues. If a non-critical priority level is consuming too many resources, it could starve critical requests. | `sum(apiserver_flowcontrol_current_executing_requests) by (priorityLevel) > 50` | Medium |
| `apiserver_flowcontrol_current_executing_seats` | Number of seats currently occupied by executing requests. Relates to API server concurrency control. If this approaches the upper limit, it indicates the API server is at maximum capacity. | `sum(apiserver_flowcontrol_current_executing_seats) / sum(apiserver_flowcontrol_upper_limit_seats) > 0.8` | Medium |
| `apiserver_flowcontrol_current_inqueue_requests` | **Current number of API requests waiting in the APF queue (not yet executing), labeled by `flow_schema` and `priority_level`.** High values indicate API server congestion, which can delay VM operations (creation, migration, deletion) as requests wait for available concurrency slots. | `apiserver_flowcontrol_current_inqueue_requests > 10` | High |
| `apiserver_flowcontrol_current_inqueue_seats` | Number of seats occupied by requests currently waiting in queues. Represents the resource consumption of queued requests. High values indicate that queued requests are consuming significant "seat" resources, which can lead to request throttling and delays in VM operations. Each request consumes one or more seats based on its resource requirements. | `apiserver_flowcontrol_current_inqueue_seats > 10` | Medium |
| `apiserver_flowcontrol_demand_seats_average` | Time-weighted average seat demand during the last concurrency borrowing adjustment period. Part of APF's dynamic concurrency borrowing algorithm. Values represent average demand pressure on the API server. | `apiserver_flowcontrol_demand_seats_average > 5` | Medium |
| `apiserver_flowcontrol_demand_seats_high_watermark` | High watermark of seat demand. Indicates peak API pressure, useful for capacity planning. | `apiserver_flowcontrol_demand_seats_high_watermark` | Medium |
| `apiserver_flowcontrol_demand_seats_smoothed` | Smoothed value of seat demand over time. **Impact**: Tracks ongoing API pressure affecting VM operations. | `apiserver_flowcontrol_demand_seats_smoothed > 2` | Medium |
| `apiserver_flowcontrol_demand_seats_sum` | Sum of seat demand across requests. **Impact**: High values indicate API server is under heavy load, risking delays for VM lifecycle events. | `apiserver_flowcontrol_demand_seats_sum > 10` | Medium |
| `apiserver_flowcontrol_priority_level_request_utilization_count` | Indicates request concurrency utilization by priority level. High values may indicate certain priority levels are overloaded. If the "system-node" priority level is highly utilized, node-related operations may be affected. | `apiserver_flowcontrol_priority_level_request_utilization_count{priorityLevel="system-node"} / apiserver_flowcontrol_upper_limit_seats{priorityLevel="system-node"} > 0.9` | Medium |
| `apiserver_flowcontrol_priority_level_seat_utilization_count` | Indicates seat concurrency utilization by priority level. High values may indicate certain priority levels are consuming too many resources. If a workload priority level is using most available seats, it could impact other operations. | `apiserver_flowcontrol_priority_level_seat_utilization_count{priorityLevel="workload-high"} / apiserver_flowcontrol_upper_limit_seats{priorityLevel="workload-high"} > 0.9` | Medium |
| `apiserver_flowcontrol_request_dispatch_no_accommodation_total` | Total number of requests that could not be accommodated due to concurrency limits. Increasing values indicate requests being rejected due to overload. If this increases for critical priority levels, important operations may be failing. | `increase(apiserver_flowcontrol_request_dispatch_no_accommodation_total{priorityLevel="system-cluster-critical"}[5m]) > 0` | High |
| `apiserver_flowcontrol_request_wait_duration_seconds_bucket` | Histogram of request waiting times. High wait times indicate API server congestion. If the 95th percentile wait time exceeds 1 second, API operations will be noticeably delayed. | `histogram_quantile(0.95, sum(rate(apiserver_flowcontrol_request_wait_duration_seconds_bucket[5m])) by (le, priorityLevel))` | Medium |
| `apiserver_flowcontrol_upper_limit_seats` | Maximum number of seats available per priority level. This is a configuration limit that affects concurrency. This is typically a static value but is important for calculating utilization percentages. | `apiserver_flowcontrol_upper_limit_seats` | Low |
| `apiserver_request:burnrate1d` | API request burn rate over 1 day, measured by taking the failed API requests divided by the total number of requests. High values indicate sustained API server load. If this exceeds SLO thresholds, it indicates the API server has been overloaded for an extended period. | `apiserver_request:burnrate1d > 2` | Medium |
| `apiserver_request:burnrate30m` | API request burn rate over 30 minutes, measured by taking the failed API requests divided by the total number of requests. High values indicate medium-term API server load. If this exceeds SLO thresholds, it indicates the API server has been overloaded recently. | `apiserver_request:burnrate30m > 5` | High |
| `apiserver_request:burnrate5m` | API request burn rate over 5 minutes, measured by taking the failed API requests divided by the total number of requests. High values indicate short-term API server load spikes. If this exceeds SLO thresholds, it indicates the API server is currently overloaded. | `apiserver_request:burnrate5m > 10` | Critical |
| `apiserver_request_post_timeout_total` | Total number of POST requests that have timed out. Increasing values indicate API server performance issues. If this increases for create operations, new resources may fail to be created. | `increase(apiserver_request_post_timeout_total{verb="create"}[5m]) > 0` | High |
| `apiserver_request_sli_duration_seconds_count` | Count of requests used for SLI (Service Level Indicator) calculations. Used for SLO monitoring. This is primarily used for calculating error rates and latencies against SLOs. | `sum(rate(apiserver_request_sli_duration_seconds_count{code=~"5.."}[5m])) / sum(rate(apiserver_request_sli_duration_seconds_count[5m]))` | Medium |
| `apiserver_request_slo_duration_seconds_count` | Count of requests used for SLO (Service Level Objective) calculations. Used for SLO monitoring. Similar to SLI metric, used for calculating compliance with SLOs. | `sum(rate(apiserver_request_slo_duration_seconds_count{code=~"5.."}[5m])) / sum(rate(apiserver_request_slo_duration_seconds_count[5m]))` | Medium |
| `apiserver_storage_data_key_generation_failures_total` | Total number of failed data encryption key generation operations. Increasing values indicate issues with etcd encryption. If this increases, it could indicate problems with the encryption provider or configuration. | `increase(apiserver_storage_data_key_generation_failures_total[1h]) > 0` | Critical |
| `apiserver_storage_size_bytes` | Size of the storage database file physically allocated in bytes. High values may indicate etcd database growth issues. If this approaches the etcd size limit (typically 2GB), it could lead to API server instability. | `apiserver_storage_size_bytes > 7*1024*1024*1024` | High |
| `authenticated_user_requests` | Count of authenticated requests. Unexpected changes may indicate authentication issues. A sudden drop could indicate authentication system problems. | `rate(authenticated_user_requests[5m])` | Medium |
| `authentication_attempts` | Count of authentication attempts. High failure rates indicate authentication configuration issues or potential security incidents. A high rate of failed attempts could indicate a brute force attack. | `sum(rate(authentication_attempts{result="error"}[5m])) / sum(rate(authentication_attempts[5m])) > 0.5` | High |
| `cco_credentials_requests_conditions` | Tracks the status conditions of Cloud Credential Operator credential requests. Indicates issues with cloud provider credentials that could affect cluster functionality. For example, if this metric shows credential requests in a degraded state, cloud resources may be inaccessible. | `cco_credentials_requests_conditions{condition="Degraded", status="True"} > 0` | High |
| `cluster:apiserver_current_inflight_requests:sum:max_over_time:2m` | Maximum number of concurrent requests to the API server over a 2-minute window. High values indicate API server congestion. For example, if this exceeds 100 for mutating requests, the API server may be overloaded, causing slow responses or failures. | `cluster:apiserver_current_inflight_requests:sum:max_over_time:2m{request_kind="mutating"} > 100` | High |
| `cluster:capacity_memory_bytes:sum` | Total **allocatable** memory across all nodes (not raw hardware capacity). This metric is critical for VM scheduling, as OpenShift Virtualization uses this value minus existing commitments to determine if new VMs can be scheduled. It does not account for memory overcommit configurations, so even if this metric shows 512GB but your VMs have 600GB in memory requests (with overcommit), new VMs may still schedule despite appearing to exceed "capacity." | `cluster:memory_usage_bytes:sum / cluster:capacity_memory_bytes:sum > 0.85` | Medium |
| `cluster:control_plane:all_nodes_ready` | Indicates whether all control plane nodes are in a ready state (1 for yes, 0 for no). Non-ready control plane nodes affect cluster stability. For example, if this metric is 0, at least one control plane node is not ready, which could affect high availability. | `cluster:control_plane:all_nodes_ready == 0` | Critical |
| `cluster:master_nodes` | Count of master/control plane nodes in the cluster. Impact: Unexpected changes may indicate node failures. For example, if this drops below the expected number (typically 3), cluster control plane redundancy is compromised. | `cluster:master_nodes == 0` | Medium |
| `cluster:memory_usage_bytes:sum` | Total memory **usage** (working set) across all nodes. High values relative to *allocatable* memory indicate potential contention, but OpenShift Virtualization's memory overcommit allows scheduling beyond 100% of allocatable memory. This metric is critical for detecting swap/thrashing scenarios that degrade VM performance. For example, if this reaches 90% of allocatable memory and swap usage is increasing, VMs may experience severe latency due to host memory pressure, even if free memory exists in the overcommit pool.| `cluster:memory_usage_bytes:sum / cluster:capacity_memory_bytes:sum > 0.85` | Medium |
| `cluster:network_attachment_definition_enabled_instance_up:max` | Maximum number of enabled network attachment definition instances that are up. Impact: Indicates health of multus CNI attachments. If this is 0 when using secondary networks, pods requiring additional networks may fail to start. | `cluster:network_attachment_definition_enabled_instance_up:max == 0` | High |
| `cluster:network_attachment_definition_enabled_instance_up:max` | Maximum number of enabled network attachment definition instances that are up. Indicates health of multus CNI attachments. For example, if this is 0 when using secondary networks, pods requiring additional networks may fail to start. | `cluster:network_attachment_definition_enabled_instance_up:max == 0` | High |
| `cluster_operator_conditions` | Represents the health conditions of core cluster operators. Degraded operators can affect overall cluster functionality. For example, if the OpenShift Virtualization operator shows a False Ready condition, virtualization features may be unavailable. | `cluster_operator_conditions{name="kubevirt-hyperconverged", condition="Ready", status="False"} == 1` | Critical |
| `cluster_operator_payload_errors` | Counts errors encountered during cluster operator payload processing. Increasing values indicate problems with operator updates. For example, if this increases during an upgrade, it may indicate problems applying new operator versions. | `sum(rate(cluster_operator_payload_errors[15m])) > 0` | High |
| `cluster:usage:resources:sum` | Aggregated resource usage across the cluster. Helps identify resource consumption trends. For example, if CPU usage is consistently high, it may indicate the need for cluster scaling. | `cluster:usage:resources:sum{resource="virtualmachineinstances.kubevirt.io"}` | Medium |
| `cnv_abnormal` | The metric tracks two specific memory conditions:<ul><li>`memory_working_set_delta_from_request`: The difference between the working set memory and the requested memory</ul></li> <ul><li>`memory_rss_delta_from_request`: The difference between the resident set size (RSS) memory and the requested memory </ul></li>Impact: Abnormal conditions may affect VM functionality. Positive values indicate the component is using more memory than requested, while negative values indicate it's using less than requested.<br><br> Large positive values could indicate memory pressure in your virtualization components. Consistently high values might suggest you need to adjust the resource requests for your virtualization components| `sum by (container) (cnv_abnormal{reason="memory_working_set_delta_from_request"})` | High |
| `cnv:vmi_status_running:count` | Count of running Virtual Machine Instances. Unexpected changes may indicate VM issues. For example, if this drops suddenly, VMs may be failing or being terminated unexpectedly. | `sum by(node) (cnv:vmi_status_running:count)`<br><br>`sum by (guest_os_name)(cnv:vmi_status_running:count)` | Medium |
| `component_resource:apiserver_request_terminations_total:rate:1m` | Rate of API server request terminations over 1 minute. High values indicate API server overload. If this exceeds normal baseline, the API server is terminating requests due to overload. | `component_resource:apiserver_request_terminations_total:rate:1m > 10` | High |
| `component_resource:apiserver_request_terminations_total:rate:5m` | Rate of API server request terminations over 5 minutes. High values indicate sustained API server overload. If this exceeds normal baseline, the API server is consistently terminating requests. | `component_resource:apiserver_request_terminations_total:rate:5m > 5` | High |
| `container_cpu_cfs_throttled_periods_total` | Total number of periods that a container was throttled due to CPU limits. High values indicate CPU contention. For example, if a critical container shows high throttling, it may experience performance degradation. | `rate(container_cpu_cfs_throttled_periods_total{namespace="openshift-logging"}[5m]) / rate(container_cpu_cfs_periods_total{namespace="openshift-logging"}[5m]) > 0.25` | Medium |
| `container_cpu_cfs_throttled_seconds_total` | Total time (in seconds) that a container was throttled due to CPU limits. High values indicate CPU contention. For example, if collector pods show high throttling, logging may be affected. | `rate(container_cpu_cfs_throttled_seconds_total{namespace="openshift-logging"}[5m]) > 0.1` | Medium |
| `containerd_cri_input_bytes_total` | Total bytes received by containerd CRI. Helps monitor container runtime network traffic. For example, sudden spikes may indicate unusual container activity or potential issues. | `rate(containerd_cri_input_bytes_total[5m]) > 1e6` | Low |
| `container_fs_usage_bytes` | Filesystem usage in bytes per container. High values may indicate disk space issues within containers. For example, if a container's filesystem usage approaches its limit, the container may experience write failures. | `container_fs_usage_bytes / container_fs_limit_bytes > 0.9` | Medium |
| `container_memory_failcnt` | Number of memory allocation failures in a container. Indicates memory pressure. For example, if this is increasing, containers are hitting memory limits and failing to allocate memory. | `increase(container_memory_failcnt[5m]) > 0` | High |
| `container_memory_swap` | Amount of swap space used by a container. High values indicate memory pressure. For example, if containers are using swap, it can significantly degrade performance. | `container_memory_swap > 0` | Medium |
| `container_memory_usage_bytes` | Memory usage of a container in bytes. High values relative to limits may lead to OOM kills. For example, if a container's memory usage approaches its limit, it may be terminated by the OOM killer. | `container_memory_usage_bytes / container_memory_working_set_bytes > 0.9` | Medium |
| `container_oom_events_total` | Total number of OOM (Out of Memory) events for a container. Indicates memory-related container terminations. For example, if this increases, containers are being killed due to memory pressure. | `increase(container_oom_events_total[15m]) > 0` | High |
| `container_runtime_crio_containers_oom_total` | Total number of OOM events for containers managed by CRI-O. Indicates memory-related container terminations. For example, if this increases, CRI-O containers are being killed due to memory pressure. | `increase(container_runtime_crio_containers_oom_total[15m]) > 0` | High |
| `controller_runtime_reconcile_panics_total` | Total number of panics during controller reconciliation. Indicates serious issues with Kubernetes controllers. For example, any increase in this metric indicates controllers are experiencing critical failures. | `increase(controller_runtime_reconcile_panics_total[15m]) > 0` | Critical |
| `controller_runtime_terminal_reconcile_errors_total` | Total number of terminal errors during controller reconciliation. Impact: Indicates controller failures. If this increases for a specific controller, resources managed by that controller may be in a bad state. | `increase(controller_runtime_terminal_reconcile_errors_total[15m]) > 0` | High |
| `coredns_forward_healthcheck_broken_total` | Total number of failed CoreDNS forward health checks. Increasing values indicate DNS forwarding issues. For example, if this increases rapidly, DNS resolution to external domains may fail. | `rate(coredns_forward_healthcheck_broken_total[5m]) > 0` | High |
| `coredns_health_request_failures_total` | Total number of CoreDNS health check failures. Indicates DNS service health issues. For example, if this increases, the CoreDNS service may be experiencing problems affecting DNS resolution. | `rate(coredns_health_request_failures_total[5m]) > 0` | High |
| `coredns_panics_total` | Total number of panics in CoreDNS. Indicates serious issues with DNS service. For example, any increase in this metric indicates CoreDNS is experiencing critical failures. | `increase(coredns_panics_total[15m]) > 0` | Critical |
| `endpoint_slice_controller_endpoints_desired` | Number of endpoints desired by the EndpointSlice controller. Indicates load on the EndpointSlice controller. For example, if this grows very large, it may indicate a large number of services or endpoints that could affect controller performance. | `endpoint_slice_controller_endpoints_desired > 10000` | Medium |
| `ErrorRateIncrease` | Alert for increased error rates in API requests. Indicates API server issues affecting operations. If this alert fires, the API server is experiencing an elevated error rate, potentially affecting cluster operations. | `ErrorRateIncrease{severity="warning"}` | High |
| `etcd_debugging_raft_terms_total` | Total number of Raft terms seen by this etcd member. Rapid increases indicate frequent leader elections and potential instability. If this increases rapidly over a short period, it indicates cluster instability with frequent leadership changes. | `delta(etcd_debugging_raft_terms_total[15m]) > 5` | High |
| `etcd_disk_backend_commit_duration_seconds_bucket` | Histogram of latency distribution of commit operations called by the backend. High latencies indicate disk performance issues affecting etcd write operations. For example, if the 99th percentile exceeds 25ms, it may indicate disk performance issues affecting etcd stability. | `histogram_quantile(0.99,sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (le))` | High |
| `etcd_disk_wal_fsync_duration_seconds_bucket` | Histogram of latency distribution of fsync operations to the WAL (Write-Ahead Log). High values indicate disk I/O issues affecting etcd performance. If the 99th percentile exceeds 10ms, it may indicate slow disk performance leading to potential leader election issues and slow writes. | `histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) by (le, instance))` | Critical |
| `etcd_disk_wal_fsync_duration_seconds_sum` | Sum of time spent on fsync operations to the WAL. Increasing values indicate cumulative disk I/O pressure. This metric is used with the count to calculate average fsync duration, which should be monitored for trends. | `rate(etcd_disk_wal_fsync_duration_seconds_sum[5m]) / rate(etcd_disk_wal_fsync_duration_seconds_count[5m])` | Medium |
| `etcd_lease_object_counts_count` | Number of objects attached to leases. High values may indicate resource leaks. For example, if this grows continuously without bounds, it may indicate applications not properly releasing leases. | `etcd_lease_object_counts_count > 1000` | Medium |
| `etcd_mvcc_db_total_size_in_bytes` | Total size of the etcd database in bytes. As this approaches the quota, etcd may stop accepting writes. For example, if this exceeds 80% of the quota (typically 2-8GB), compaction and defragmentation may be needed to prevent etcd from becoming read-only. | `etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes > 0.8` | Critical |
| `etcd_server_client_requests_total` | Total number of client requests received by type. Rate changes indicate client load patterns. For example, monitoring the rate of write requests can help identify periods of high load on the etcd cluster. | `sum(rate(etcd_server_client_requests_total{type="write"}[5m]))` | Medium |
| `etcd_server_has_leader` | Whether this etcd member has a leader (1) or not (0). Values of 0 indicate split-brain or isolation. If this is 0 for any etcd member, that member cannot process writes and may be network isolated. | `etcd_server_has_leader == 0` | Critical |
| `etcd_server_proposals_failed_total` | Total number of failed proposals (consensus operations). Increasing values indicate consensus issues. If this increases, it indicates problems with the Raft consensus algorithm, potentially due to network issues or overload. | `increase(etcd_server_proposals_failed_total[15m]) > 0` | High |
| `etcd_server_quota_backend_bytes` | Maximum backend size in bytes etcd can use before triggering an alarm. Defines the limit for etcd database size. This is typically set to 2-8GB. If the database size approaches this value, maintenance is required. | `etcd_server_quota_backend_bytes` | Low |
| `go_cpu_classes_gc_total_cpu_seconds_total` | Total CPU time spent on garbage collection. High values indicate excessive GC activity. For example, if this is increasing rapidly, it may indicate memory pressure in the Go runtime. | `rate(go_cpu_classes_gc_total_cpu_seconds_total[5m]) > 1` | Medium |
| `go_gc_gomemlimit_bytes` | Memory limit for the Go runtime in bytes. Defines the ceiling for Go memory usage. This is a configuration setting that helps understand the memory constraints for the process. | `go_gc_gomemlimit_bytes` | Low |
| `go_gc_heap_live_bytes` | Size of the live objects heap in bytes. High values relative to limits indicate memory pressure. For example, if this approaches the total heap size, it may indicate memory leaks or high memory pressure. | `go_gc_heap_live_bytes / go_memory_classes_total_bytes > 0.8` | Medium |
| `go_godebug_non_default_behavior_panicnil_events_total` | Total number of nil panic events. Any non-zero value indicates programming errors. For example, if this increases, it indicates nil pointer dereferences in the code, which should never happen in production. | `increase(go_godebug_non_default_behavior_panicnil_events_total[15m]) > 0` | Critical |
| `go_memory_classes_metadata_mspan_inuse_bytes` | Size of mspan structures currently in use in bytes. Indicates memory used for internal Go runtime structures. This is primarily useful for detailed Go runtime memory analysis. | `go_memory_classes_metadata_mspan_inuse_bytes` | Low |
| `go_memory_classes_total_bytes` | Total size of Go memory classes in bytes. Indicates overall memory usage by the Go runtime. This helps understand the total memory footprint of the process. | `go_memory_classes_total_bytes` | Medium |
| `group_sync_error` | Indicates errors during group synchronization. May affect cluster consistency in relation to permissions within the cluster. For example, if this increases, it could indicate issues with group membership updates. | `increase(group_sync_error[15m]) > 0` | Medium |
| `grpc_req_panics_recovered_total` | Total number of gRPC request panics recovered. Increasing values indicate gRPC service instability. For example, if this increases rapidly, it may indicate issues with gRPC request handling. | `rate(grpc_req_panics_recovered_total[5m]) > 0` | Medium |
| `haproxy_backend_connection_errors_total` | Total number of connection errors on HAProxy backends. Increasing values indicate issues with backend connectivity. For example, if this increases for a specific backend, it may indicate server or network issues. | `rate(haproxy_backend_connection_errors_total[5m]) > 0` | Medium |
| `haproxy_backend_http_average_response_latency_milliseconds` | Average response latency of HAProxy backend HTTP requests in milliseconds. High values indicate slow backend responses. For example, if this exceeds 500ms for a critical service, it may impact user experience. | `haproxy_backend_http_average_response_latency_milliseconds > 500` | Medium |
| `haproxy_backend_http_responses_total` | Total number of HTTP responses from HAProxy backends. Used to monitor backend request volume. For example, trend analysis is used to determine activity in the cluster. | `rate(haproxy_backend_http_responses_total[5m]) == 0` | Medium |
| `haproxy_backend_up` | Indicates whether HAProxy backends are up (1) or down (0). Values of 0 indicate backend availability issues. For example, if this is 0 for a critical backend, traffic may not be routed correctly. | `haproxy_backend_up == 0` | Critical |
| `haproxy_server_connections_total` | Total number of connections to HAProxy servers. Used to monitor server load. For example, if this increases rapidly, it may indicate a sudden spike in traffic. | `rate(haproxy_server_connections_total[5m]) > 100` | Medium |
| `haproxy_server_downtime_seconds_total` | Total downtime of HAProxy servers in seconds. Increasing values indicate server availability issues. For example, if this increases for a critical server, it may indicate recurring outages. | `increase(haproxy_server_downtime_seconds_total[15m]) > 0` | Medium |
| `haproxy_server_up` | Indicates whether HAProxy servers are up (1) or down (0). Values of 0 indicate server availability issues. For example, if this is 0 for a critical server, it may indicate a server failure. | `haproxy_server_up == 0` | Critical |
| `haproxy_up` | Indicates whether HAProxy is up (1) or down (0). Values of 0 indicate HAProxy service issues. For example, if this is 0, the HAProxy service is not running or not reachable. | `haproxy_up == 0` | Critical |
| `High Container Restart Rate` | Alert for high rate of container restarts. Indicates potential issues with container stability or resource constraints. For example, if containers are restarting frequently, it may indicate memory or CPU issues. | `sum(rate(kube_pod_container_status_restarts_total[5m])) > 10` | Medium |
| `High Error Rate` | Alert for high error rates in API requests or other operations. Indicates service instability or configuration issues. For example, if this alert fires, it may indicate problems with API server or backend services. | `sum(rate(apiserver_request_total{code=~"5.."}[5m])) / sum(rate(apiserver_request_total[5m])) > 0.1` | High |
| `HPA Status Condition Failure` | Indicates a failure in the Horizontal Pod Autoscaler (HPA) status condition. May prevent HPA from scaling workloads correctly. For example, if this condition is failing, HPA may not adjust pod counts based on resource utilization. | `kube_hpa_status_condition{condition="ScalingActive", status="False"} == 1` | Medium |
| `http_client_request_total` | Total number of HTTP client requests. Used to monitor client request volume. For example, if this drops suddenly, it may indicate client connectivity issues. | `rate(http_client_request_total[5m]) == 0` | Medium |
| `instance_request_kind:apiserver_current_inflight_requests:sum` | Sum of concurrent requests to the API server by request kind. High values indicate API server congestion. If this exceeds 100 for mutating requests, the API server may be overloaded. | `instance_request_kind:apiserver_current_inflight_requests:sum{request_kind="mutating"} > 100` | High |
{.is-info}
| `kube_cronjob_spec_suspend` | Indicates whether a CronJob is suspended. Impact: Suspended CronJobs will not run scheduled tasks. If this is True for a critical CronJob, scheduled tasks may not execute. | `kube_cronjob_spec_suspend == "True"` | Medium |
| `kube_deployment_status_condition` | Status conditions of Deployments. Indicates health and readiness of Deployments. If a Deployment shows a False Ready condition, pods may not be available for traffic. | `kube_deployment_status_condition{condition="Available", status="false"} == 1` | Medium |
| `kube_deployment_status_replicas_available` | Number of available replicas in a Deployment. Used to monitor Deployment health. If this is less than the desired number, po{"ds may not be fully available. | `kube_deployment_status_replicas_available{deployment="apiserver"} <3` | Medium |
| `kube_job_status_failed` | Indicates failed Jobs. May indicate issues with Job execution or configuration. If this increases for a critical Job, it may indicate recurring failures. | `kube_job_status_failed == 1` | Medium |
| `kubelet_memory_manager_pinning_errors_total` | Total number of memory pinning errors by the Kubelet memory manager. Indicates issues with memory allocation. If memory pinning fails, workloads requiring guaranteed memory allocation may experience higher latency and decreased performance, especially in NUMA architectures. This metric should ideally always be 0 | `rate(kubelet_memory_manager_pinning_errors_total[5m]) > 0` | Medium |
| `kubelet_pleg_last_seen_seconds` | Time in seconds since the last PLEG (Pod Lifecycle Event Generator) update. High values indicate Kubelet issues. If this exceeds 60 seconds, it may indicate Kubelet is not updating pod status correctly. | `kubelet_pleg_last_seen_seconds > 60` | Medium |
| `kubelet_pod_start_sli_duration_seconds_bucket` | Histogram of pod start latency in seconds. High values indicate slow pod startup times. If the 95th percentile exceeds 30 seconds, pod startup may be slow, affecting service availability. | `histogram_quantile(0.95, sum(rate(kubelet_pod_start_sli_duration_seconds_bucket[5m])) by (le))` | Medium |
| `kubelet_volume_stats_available_bytes` | Available bytes in a volume. Used to monitor volume capacity and detect potential storage issues. If available bytes are low, it may indicate that the volume is running out of space. | `kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.2` | Medium |
| `kubelet_volume_stats_inodes_free` | Number of free inodes in a volume. Low values indicate potential inode exhaustion. If free inodes are less than 5% of total inodes, it may indicate inode exhaustion issues. | `kubelet_volume_stats_inodes_free / kubelet_volume_stats_inodes < 0.05` | Medium |
| `kube_pod_status_ready` | Indicates whether a pod is ready to serve requests. Non-ready pods may not receive traffic. If a critical pod is not ready, it may indicate issues with the pod or its containers. | `kube_pod_status_ready{namespace="default", pod="critical-pod"} == 0` | High |
| `kube_pod_status_unschedulable` | Indicates whether a pod is unschedulable. Unschedulable pods cannot be placed on nodes. If a pod is unschedulable due to resource constraints, it may indicate the need for node scaling. | `kube_pod_status_unschedulable{namespace="default", pod="critical-pod"} == 1` | Medium |
| `kube_running_pod_ready` | Number of running pods that are ready. Used to monitor pod health and availability. If this drops suddenly, it may indicate pod failures or readiness issues. | `kube_running_pod_ready{namespace="default"} < 10` | Medium |
| `kubevirt_hco_hyperconverged_cr_exists` | Indicates whether the Hyperconverged Custom Resource exists. Non-existent CRs may affect hyperconverged functionality. If this is False, it may indicate issues with hyperconverged deployment or configuration. | `kubevirt_hco_hyperconverged_cr_exists == 0` | Critical |
| `kubevirt_hco_system_health_status` | Numeric health status of the Hyperconverged Operator (1 = healthy, 0 = not healthy). A value of 0 indicates issues with hyperconverged components that may affect virtualization functionality. If this metric returns 0, the HCO is reporting a problem requiring investigation. The HyperConverged CR creates corresponding CRs for the operators of all other components related to Compute, storage, networking, templating and scaling within OpenShift Virtualization. | `kubevirt_hco_system_health_status == 0` | Critical |
| `kubevirt_hyperconverged_operator_health_status` | Health status of the Hyperconverged Operator. Non-healthy status may indicate issues with hyperconverged components. If this indicates a degraded status, it may affect hyperconverged functionality. | `kubevirt_hyperconverged_operator_health_status != 0` | High |
| `kubevirt_number_of_vms` | Number of Virtual Machines managed by KubeVirt. Used to monitor VM deployment and scaling. If this drops suddenly, it may indicate VM termination or deployment issues. | `kubevirt_number_of_vms{namespace="default"} < 10` | Medium |
| `kubevirt_ssp_operator_up` | Indicates whether the SSP (Specialized Service Provider) operator, a core component of OpenShift Virtualization, is up and running. If not running (`0`), key virtualization features such as VM templates, common templates, and data import will not function. | `kubevirt_ssp_operator_up == 0` | Critical |
| `kubevirt_virt_api_up` | The number of virt-api pods that are up (not a binary indicator). The virt-api service is the main API endpoint for KubeVirt operations, handling all VM lifecycle requests. Multiple pods provide redundancy and high availability. | `kubevirt_virt_api_up < 1` | Critical |
| `kubevirt_virt_controller_up` | Number of virt-controller pods currently running. The virt-controller manages VM lifecycle operations (creation, migration, deletion). Multiple pods provide redundancy. A value matching your configured replica count (typically 2) indicates healthy operations. If this drops to 0, all VM operations will fail. | `kubevirt_virt_controller_up < 1` | Critical |
| `kubevirt_virt_handler_up` | Number of virt-handler pods currently running. Impact: Each node requires one virt-handler pod to manage VM operations. A value matching node count indicates full coverage. If lower, affected nodes can't manage VMs. | `kubevirt_virt_handler_up < count(kube_node_role{role="worker"})` | Critical |
| `kubevirt_virt_operator_up` | The number of virt-operator pods that are up. If this drops below 1 the operator will cease to function and changes will not be reconciled. | `kubevirt_virt_operator_up < 1` | Critical |
| `kubevirt_vmi_cpu_usage_seconds_total` | Total CPU usage of Virtual Machine Instances (VMIs) in seconds. High values indicate CPU resource constraints. If CPU usage is consistently high, it may indicate the need for VM scaling or resource adjustments. | `rate(kubevirt_vmi_cpu_usage_seconds_total[5m]) > 100` | Medium |
| `kubevirt_vmi_memory_available_bytes` | Available memory for Virtual Machine Instances (VMIs) in bytes. Low values indicate memory constraints. If available memory is low, it may indicate that VMIs are experiencing memory pressure. | `kubevirt_vmi_memory_available_bytes{namespace="default"} < 1e9` | Medium |
| `kubevirt_vmi_memory_pgmajfault_total` | Total number of major page faults for Virtual Machine Instances (VMIs). High values indicate memory pressure. If major page faults are increasing, it may indicate that VMIs are experiencing memory constraints. | `rate(kubevirt_vmi_memory_pgmajfault_total[5m]) > 0` | Medium |
| `kubevirt_vmi_memory_swap_in_traffic_bytes` | Swap-in traffic for Virtual Machine Instances (VMIs) in bytes. High values indicate memory pressure. Swap-in refers to the process of moving data from swap space (usually disk storage) back into the VM's RAM | `rate(kubevirt_vmi_memory_swap_in_traffic_bytes[5m]) > 100` | Medium |
| `kubevirt_vmi_memory_swap_out_traffic_bytes` | Swap-out traffic for Virtual Machine Instances (VMIs) in bytes. High values indicate memory pressure. Sawp-out refers to the process of moving data from ram to swap space. If swap-out traffic is high, it may indicate that VMIs are experiencing memory constraints. | `rate(kubevirt_vmi_memory_swap_out_traffic_bytes[5m]) > 100` | Medium |
| `loki_query_frontend_shards_total` | Number of active query shards in the Loki query frontend. In OpenShift Virtualization, large log volumes from VMs, nodes, and hypervisors demand efficient log query parallelization. Too few shards can bottleneck query performance, making VM troubleshooting or compliance investigations slow. Too many shards can cause overhead and resource waste. Monitoring this helps ensure log query responsiveness for critical virtualization events. | `loki_query_frontend_shards_total` | Medium |
| `loki_runtime_config_last_reload_successful` | Indicates if the last Loki configuration reload was successful (1 for success, 0 for failure). Failed reloads may affect log collection patterns for VM logs, reducing visibility into virtualization problems. | `loki_runtime_config_last_reload_successful == 0` | Medium |
| `loki_store_series_total` | Total number of series in Loki's store. In virtualization environments, the number of log streams grows significantly with each VM, potentially causing high cardinality problems affecting query performance and storage. Monitoring this metric can help identify if VM logging is generating too many unique log streams, which could impact the performance of the entire logging stack. | `loki_store_series_total > 1000` | Medium |
| `loki_tsdb_build_index_last_successful_timestamp_seconds` | Timestamp of the last successful TSDB index build in seconds. Impact: Outdated indexes may affect ability to search VM logs, hampering troubleshooting of virtualization issues. If this timestamp is stale (more than an hour old), recent VM migration or error logs may not be searchable. | `time() - loki_tsdb_build_index_last_successful_timestamp_seconds > 3600` | Medium |
| `loki_write_failures_discarded_total` | Total count of log entries discarded due to write failures. Impact: May result in missing VM logs critical for troubleshooting virtualization issues and maintaining compliance. If increasing rapidly during VM migration activities, important logs about VM state transitions may be missing, making it difficult to diagnose failed migrations. | `increase(loki_write_failures_discarded_total[1h]) > 0` | High |
| `mapi_machinehealthcheck_short_circuit` | Indicates if machine health checks are short-circuited (disabled). When enabled (1), prevents automatic remediation of unhealthy nodes hosting VMs, potentially extending VM outages. During planned maintenance, this might be enabled, but if left enabled accidentally, it could prevent automatic recovery of nodes hosting critical VMs. | `mapi_machinehealthcheck_short_circuit > 0` | Medium |
| `mapi_mao_collector_up` | Indicates if the Machine API Operator collector is up (1) or down (0). When down, affects machine health monitoring for nodes hosting VMs, potentially leaving hardware issues undetected. If down, problems with worker nodes hosting VMs may go undetected. | `mapi_mao_collector_up == 0` | High |
| `mcd_reboots_failed_total` | Total count of failed node reboots during machine config updates. May leave nodes in inconsistent states with partial updates to virtualization components. If a node fails to reboot during an update, VMs scheduled on that node may experience stability issues or incompatibility with the rest of the cluster. | `increase(mcd_reboots_failed_total[1h]) > 0` | High |
| `mco_degraded_machine_count` | Count of machines in a degraded state according to the Machine Config Operator. Degraded machines may host VMs with reduced performance or with outdated virtualization components. If nodes hosting VMs are degraded, virtualization features like live migration may be compromised due to inconsistent configurations. | `mco_degraded_machine_count > 0` | High for Control Plane <br><br> Medium for Workers|
| `mco_unavailable_machine_count` | Count of unavailable machines according to the Machine Config Operator. Unavailable machines cannot host VMs, reducing total virtualization capacity. | `mco_unavailable_machine_count > 0` | High for Control Plane <br><br> Medium for Workers |
| `node_cpu_core_throttles_total` | Total number of CPU throttling events on a node. May indicate CPU contention or thermal issues affecting VM performance and stability. High throttling can cause VM workloads to experience unpredictable performance, especially for latency-sensitive applications, and may indicate that a node is oversubscribed or experiencing cooling problems. | `rate(node_cpu_core_throttles_total[5m]) > 10` | Medium |
| `node_edac_csrow_uncorrectable_errors_total` | Total uncorrectable memory errors at CSROW (Chip-Select Row) level. May indicate hardware memory issues that could lead to VM crashes or corruption. Increasing errors may precede host crashes affecting running VMs, or cause silent data corruption inside VM memory that could affect application integrity. | `increase(node_edac_csrow_uncorrectable_errors_total[1h]) > 0` | Critical |
| `node_edac_uncorrectable_errors_total` | Total uncorrectable memory errors across the system. May indicate hardware memory issues that could cause VM crashes, data corruption, or host failures. Even a single uncorrectable memory error could potentially crash VMs or cause data corruption within VM memory, making this a critical metric to monitor on virtualization hosts. | `increase(node_edac_uncorrectable_errors_total[1h]) > 0` | Critical |
| `node_fibrechannel_link_failure_total` | Total Fibre Channel link failures detected. May affect VM access to storage on SAN, causing I/O errors for VMs using FC-based persistent volumes. If increasing, VMs using Fibre Channel storage may experience I/O errors. | `increase(node_fibrechannel_link_failure_total[1h]) > 0` | High |
| `node_fibrechannel_loss_of_signal_total` | Total Fibre Channel signal loss events. May affect VM access to storage on SAN, causing I/O timeouts for VMs using FC storage. If increasing, VMs using Fibre Channel storage may experience I/O timeouts. | `increase(node_fibrechannel_loss_of_signal_total[1h]) > 0` | High |
| `node_fibrechannel_loss_of_sync_total` | Total Fibre Channel synchronization loss events. May affect VM access to storage on SAN, causing intermittent I/O issues. If increasing, VMs using Fibre Channel storage may experience intermittent I/O issues, leading to application performance degradation or increased latency. | `increase(node_fibrechannel_loss_of_sync_total[1h]) > 0` | High |
| `node_filesystem_avail_bytes` | Available bytes in the node's filesystem. Low values may affect local VM storage, container images, and ephemeral disks used by VMs. If a node runs out of filesystem space, VM creation may fail, ephemeral disks may become full causing VM application errors, and VM image pulling may fail. | `node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"} < 0.10` | High |
| `node_filesystem_free_bytes` | Free space in bytes in the node's filesystem. Critical for VM storage operations, as low free space can prevent VM creation, snapshot operations, and disk image transfers. VM disk I/O performance also degrades as filesystems approach capacity, affecting running workloads. | `node_filesystem_free_bytes{mountpoint="/var"} / node_filesystem_size_bytes{mountpoint="/var"} < 20` | Critical |
| `node_hwmon_temp_crit_celsius` | Critical temperature threshold for hardware components in Celsius. Helps determine thermal headroom before VM workloads are affected by throttling. When planning to deploy VMs with high CPU utilization patterns (ML workloads, database servers), compare current temperature against this critical threshold to ensure adequate thermal margin. | `(node_hwmon_temp_celsius / node_hwmon_temp_crit_celsius) > 0.8` | Medium |
| `node_infiniband_link_downed_total` | Total number of times an InfiniBand link went down. InfiniBand is often used for high-performance VM workloads requiring low-latency networking. Link failures directly impact VM network performance and can cause application errors within VMs. | `increase(node_infiniband_link_downed_total[1h]) > 0` | High |
| `node_infiniband_port_errors_received_total` | Total count of InfiniBand port errors received. Affects network reliability for VM workloads using InfiniBand for high-performance computing or storage access. Even small error rates can degrade performance of latency-sensitive VM applications. If this metric increases for nodes hosting database VMs using InfiniBand for storage access, those VMs may experience intermittent I/O errors leading to database corruption or performance issues. | `rate(node_infiniband_port_errors_received_total[5m]) > 0` | Medium |
| `node_memory_CommitLimit_bytes` | System-wide kernel configuration indicating the theoretical maximum memory allocation limit (physical RAM + swap Ã— overcommit ratio). For OpenShift Virtualization, this metric has minimal impact on actual VM placement decisions, as scheduling is based on physical RAM and memory requests, not the commit limit. | `node_memory_CommitLimit_bytes / (1024*1024*1024)` | Low |
| `node_memory_MemFree_bytes` | Amount of physical RAM left unused by the system. Directly affects VM placement, migration capabilities, and performance under memory pressure. Low free memory can trigger swapping which severely impacts VM performance. Existing VMs may experience performance degradation as the hypervisor struggles to allocate memory pages. <br> <b>NOTE:</b> this does not take into account buffers, cache or overall available memory!| `node_memory_MemFree_bytes / node_memory_MemTotal_bytes` | Medium-Low |
| `node_netstat_Ip6_InOctets` | Total number of IPv6 octets (bytes) received. Measures IPv6 network throughput to nodes hosting VMs. When running dual-stack VMs, unexpected increases may indicate IPv6-specific traffic that could impact network performance. | `rate(node_netstat_Ip6_InOctets[5m])` | Low |
| `node_netstat_Ip6_OutOctets` | Total number of IPv6 octets (bytes) sent. Measures IPv6 network throughput from nodes hosting VMs. For VMs serving content over IPv6, this metric helps identify if network bandwidth is sufficient for the workload or if IPv6 traffic patterns differ from IPv4. | `rate(node_netstat_Ip6_OutOctets[5m])` | Low |
| `node_netstat_IpExt_InOctets` | Total number of IPv4 octets (bytes) received. Measures inbound network throughput to nodes hosting VMs. Critical for monitoring network-intensive VM workloads and identifying potential bandwidth constraints. If this metric approaches physical NIC capacity, VMs may experience network contention, packet drops, or increased latency, especially for VMs running streaming or real-time communication services. | `rate(node_netstat_IpExt_InOctets[5m]) / (1024 * 1024) > 100` | Medium |
| `node_netstat_IpExt_OutOctets` | Total number of IPv4 octets (bytes) sent. Measures outbound network throughput from nodes hosting VMs. Critical for VMs serving content or performing data transfers. | `rate(node_netstat_IpExt_OutOctets[5m]) / (1024 * 1024) > 1250` (for a 10g nic or 3150 for 25G nic)| Medium |
| `node_netstat_Tcp_ActiveOpens` | Total number of active TCP connection openings. Measures the rate of new outbound connections from VMs. Useful for identifying abnormal connection patterns or potential connection storms from VM workloads. A sudden spike in this metric may indicate a VM is performing aggressive outbound connection attempts, which could be a sign of compromised VM, misconfigured application, or DoS attack originating from within a VM. | `rate(node_netstat_Tcp_ActiveOpens[5m]) > 1000` | Medium |
| `node_network_receive_packets_total` | Total count of network packets received. Measures network load in packets rather than bytes. VMs running packet-processing workloads may hit packet processing limits before bandwidth limits. High packet rates can cause CPU saturation on the host, affecting all VMs. | `rate(node_network_receive_packets_total{device!="lo"}[5m]) > 100000` | Medium |
| `node_network_up` | Binary metric indicating if a network device is up (1) or down (0). Network interface failures directly impact VM connectivity. If this metric shows 0 for a network interface used by VM secondary NICs (like SR-IOV or bridge networks), those VMs will lose connectivity on that interface, potentially breaking application clustering or storage connectivity. | `node_network_up{device!="lo"} == 0` | Critical |
| `node_nfsd_connections_total` | Total count of NFS connections. Important if VMs are using NFS-based storage for their disks. NFS connection saturation can cause VM I/O operations to stall, leading to apparent VM freezes. If this metric approaches the maximum supported NFS connections on a node providing NFS storage for VM disks, VMs may experience slow I/O operations or apparent freezes as their disk operations queue. | `node_nfsd_connections_total` | Medium |
| `node_nfsd_rpc_errors_total` | Total count of NFS Remote Procedure Call (RPC) errors. Critical for VMs using NFS-backed storage. RPC errors can cause I/O errors within VMs, leading to filesystem corruption or application failures inside the VM. | `increase(node_nfsd_rpc_errors_total[15m]) > 0` | High |
| `node_role_os_version_machine:cpu_capacity_cores:sum` | Aggregated sum of CPU cores capacity across nodes, broken down by node role, OS version, and machine type. Critical for VM capacity planning and placement, as different classes of nodes may be designated for specific VM workloads. Insufficient CPU capacity directly impacts VM density and performance. | `node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io="worker"}` | High |
| `node_thermal_zone_temp` | Zone temperature in Celsius for node hardware components. If temperatures approach critical thresholds on compute nodes running VMs, performance degradation from thermal throttling may occur, causing inconsistent VM performance or unexpected VM migrations. | `max(node_thermal_zone_temp) by (instance) > 80` | High |
| `node_vmstat_oom_kill` | Count of Out-of-Memory kill events from `/proc/vmstat`. OOM events on nodes running OpenShift Virtualization are catastrophic as they can terminate the virt-launcher pods running VMs, causing immediate VM termination and potential data corruption.  | `increase(node_vmstat_oom_kill[15m]) > 0` | Critical |
| `node_watchdog_bootstatus` | Hardware watchdog boot status indicator. In virtualization environments, hardware watchdog failures may indicate underlying hardware issues on VM host nodes that could lead to unexpected reboots and VM outages. | `node_watchdog_bootstatus > 0` | Medium |
| `node_watchdog_timeleft_seconds` | Represents seconds remaining before the hardware watchdog timer would trigger a reboot if not reset. In properly functioning systems, this value is typically constant as the watchdog is continuously being reset. **This is NOT an indicator of node health problems when stable at a low value.** If this value remains stable across all nodes, this indicates normal operation where the watchdog is being reset regularly by the kernel. Only a decreasing value without reset would indicate a problem. | `rate(node_watchdog_timeleft_seconds[5m])` | Low |
| `openshift_auth_form_password_count_result` | Count of web console login attempts by result. Multiple failed logins from the same source may indicate attempts to access the virtualization console to manipulate VMs or their configurations. | `sum(rate(openshift_auth_form_password_count_result{result="error"}[15m])) > 5` | Medium |
| `openshift:cpu_usage_cores:sum` | Total CPU usage by OpenShift infrastructure components. During VM migration or creation operations, if this value spikes significantly, platform components may be consuming CPU resources needed for VM operations, causing delays or performance issues. | `openshift:cpu_usage_cores:sum / on() group_left() sum(cluster:capacity_cpu_cores:sum) > 0.3` | Medium |
| `openshift_etcd_operator_signer_expiration_days` | Days until the etcd operator signing certificates expire. Certificate expiration could disrupt etcd, which would affect all VM control operations and potentially make the virtualization platform unavailable. | `openshift_etcd_operator_signer_expiration_days < 14` | High |
| `openshift:memory_usage_bytes:sum` | Total memory usage by OpenShift infrastructure components. If memory usage by platform components is high on nodes intended for VM workloads, VM density will be reduced, and memory-intensive VMs may experience performance degradation or fail to be scheduled. | `openshift:memory_usage_bytes:sum / on() group_left() sum(cluster:capacity_memory_bytes:sum) > 0.3` | Medium |
| `ovn_controller_bridge_mappings` | Status metric for OVN controller bridge mappings configuration. In OpenShift Virtualization, bridge mappings are critical for connecting VMs to physical networks, especially for VMs requiring direct layer 2 connectivity. If bridge mappings are misconfigured or unavailable, VMs with attachments to secondary networks or requiring physical network access may lose connectivity or fail to start. | `ovn_controller_bridge_mappings == 0` | High |
| `ovn_controller_ct_zone_commit_95th_percentile` | 95th percentile time taken to commit connection tracking zones in OVN. If this value is high, VMs with high network traffic may experience connection tracking issues, packet drops, or unexpected connection resets, especially during traffic spikes. | `ovn_controller_ct_zone_commit_95th_percentile > 100` | Medium |
| `ovn_controller_if_status_mgr_run_maximum` | Maximum execution time for the OVN interface status manager. If this value is high, detection and recovery of network interface failures for VMs may be delayed, extending network outages for virtualized workloads. | `ovn_controller_if_status_mgr_run_maximum > 200` | Medium |
| `ovn_controller_lflow_run` | Execution time metrics for logical flow processing in OVN. Logical flows control VM network traffic paths, and slow processing delays network policy and route updates. During network policy updates affecting VMs, slow logical flow processing could delay the application of security rules, temporarily leaving VMs with incorrect network access. | `increase(ovn_controller_lflow_run[5m]) > 1000` | Medium |
| `ovn_controller_monitor_all` | Status of OVN controller monitoring processes. If this metric indicates monitoring is failing, changes to VM network configuration may not be detected or applied, leading to inconsistent network behavior for VMs. | `ovn_controller_monitor_all == 0` | High |
| `ovn_controller_southbound_database_connected` | Binary status indicating if OVN controller is connected to the southbound database (1 = connected, 0 = disconnected). If this value is 0, the node's OVN controller cannot communicate with the central OVN database, preventing VM network configuration updates and potentially isolating VMs on that node. | `ovn_controller_southbound_database_connected == 0` | Critical |
| `ovn_controller_txn_error` | Count of OVN controller transaction errors. If transaction errors increase during VM creation or migration, the associated network setup may fail, leaving VMs with incorrect or missing network connectivity. | `increase(ovn_controller_txn_error[5m]) > 0` | High |
| `ovn_db_e2e_timestamp` | End-to-end timestamp information for OVN database operations. If the difference between current time and this timestamp increases, it indicates OVN database operation delays that could slow VM network provisioning or policy updates. | `time() - ovn_db_e2e_timestamp > 70`| Medium |
| `ovnkube_clustermanager_allocated_v4_host_subnets` | Number of IPv4 host subnets allocated in the OVN cluster. Determines availability of IPv4 subnets for VM placement. This should be compared against the expected number of subnets per cluster. | `ovnkube_clustermanager_allocated_v4_host_subnets` | Medium |
| `ovnkube_clustermanager_allocated_v6_host_subnets` |  Number of IPv4 host subnets allocated in the OVN cluster. Determines availability of IPv4 subnets for VM placement. This should be compared against the expected number of subnets per cluster. | `ovnkube_clustermanager_allocated_v6_host_subnets` | Medium |
| `ovnkube_controller_ipsec_enabled` | Binary indicator whether IPsec encryption is enabled for OVN network traffic (1=enabled, 0=disabled). Affects security of VM-to-VM traffic across nodes. Critical for multi-tenant virtualization where network traffic isolation is required. If this metric shows 0 when IPsec should be enabled, VM network traffic may be unencrypted, potentially exposing sensitive data in multi-tenant virtualization environments. | `ovnkube_controller_ipsec_enabled == 0` | High |
| `ovnkube_controller_pod_port_binding_chassis_port_binding_up_duration_seconds_bucket` | Histogram of time taken for OVN port bindings to become active. Directly affects how quickly VM network interfaces become available after VM creation or migration. If the 95th percentile exceeds 5 seconds, VM creation operations may take longer than expected, particularly affecting VM migration time and application availability during migrations. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_pod_port_binding_chassis_port_binding_up_duration_seconds_bucket[5m])) by (le)) > 5` | Medium |
| `ovnkube_controller_resource_add_latency_seconds_bucket` | Histogram of latency for adding network resources. Affects how quickly VM network resources (interfaces, IPs) are provisioned, directly impacting VM creation time. If the 95th percentile exceeds 2 seconds, VM network interfaces may be slow to initialize, affecting VM startup time and orchestration workflows. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_resource_add_latency_seconds_bucket[5m])) by (le, resource)) > 2` | Medium |
| `ovnkube_controller_resource_add_latency_seconds_count` | Count of network resource addition operations. High values indicate network resource churn which may affect OVN controller performance for all VM operations. During large-scale VM creation or migration operations, a high rate can indicate high controller load that might impact other network operations. | `rate(ovnkube_controller_resource_add_latency_seconds_count[5m]) > 10` | Low |
| `ovnkube_controller_resource_add_latency_seconds_sum` | Sum of latencies for adding network resources. Used with count to calculate average latency. High values indicate network control plane performance issues. When divided by the count, an increasing average latency for adding VM network resources may indicate control plane bottlenecks affecting VM deployment speed. | `rate(ovnkube_controller_resource_add_latency_seconds_sum[5m]) / rate(ovnkube_controller_resource_add_latency_seconds_count[5m]) > 1` | Medium |
| `ovnkube_controller_resource_delete_latency_seconds_bucket` | Histogram of latency for deleting network resources. Affects how quickly VM network resources are cleaned up after VM deletion or migration. If the 95th percentile exceeds 2 seconds, resources may not be cleaned up quickly, potentially affecting subsequent VM migrations to the same node or IP address reuse. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_resource_delete_latency_seconds_bucket[5m])) by (le, resource)) > 2` | Medium |
| `ovnkube_controller_resource_delete_latency_seconds_count` | Count of network resource deletion operations. High values indicate network resource cleanup activity which may affect controller performance. | `rate(ovnkube_controller_resource_delete_latency_seconds_count[5m]) > 10` | Low |
| `ovnkube_controller_resource_delete_latency_seconds_sum` | Sum of latencies for deleting network resources. Used with count to calculate average latency. High values indicate network control plane cleanup issues. When divided by the count, an increasing average latency for removing VM network resources may indicate control plane bottlenecks affecting resource cleanup. | `rate(ovnkube_controller_resource_delete_latency_seconds_sum[5m]) / rate(ovnkube_controller_resource_delete_latency_seconds_count[5m]) > 1` | Medium |
| `ovnkube_controller_resource_update_latency_seconds_bucket` | Histogram of latency for updating network resources. Affects how quickly VM network configuration changes take effect, including policy updates or interface reconfiguration. If the 95th percentile exceeds 2 seconds, network policy updates for VMs might be delayed, potentially leaving security gaps during policy transitions. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_resource_update_latency_seconds_bucket[5m])) by (le, resource)) > 2` | Medium |
| `ovnkube_master_libovsdb_disconnects_total` | Total count of disconnections from the OVS database. Database disconnections can disrupt VM networking and prevent network configuration changes from being applied. If this metric increases rapidly, the OVN control plane is experiencing database connectivity issues that may prevent VM network changes from being applied correctly. | `increase(ovnkube_master_libovsdb_disconnects_total[15m]) > 0` | High |
| `ovn_northd_nb_connection_status` | Status of the northbound database connection (1=connected, 0=disconnected). If this metric is 0, the OVN northd service cannot communicate with the configuration database, preventing all VM network changes from being processed. | `ovn_northd_nb_connection_status == 0` | Critical |
| `ovn_northd_ovnsb_db_run_maximum` | Maximum runtime for OVN Southbound database operations. If this value is high, VM network configuration changes may take longer to propagate, affecting the responsiveness of VM network management operations. | `ovn_northd_ovnsb_db_run_maximum > 1` | Medium |
| `ovn_northd_sb_connection_status` | Status of the southbound database connection (1=connected, 0=disconnected). If this metric is 0, the OVN northd service cannot communicate with the implementation database, preventing VM network configurations from reaching the hosts. | `ovn_northd_sb_connection_status == 0` | Critical |
| `ovn_northd_status` | Overall status of the OVN northd service (1=running, 0=not running). If this metric is 0, the OVN northd service is not running, which will prevent all VM network operations including creation, updates, and policy enforcement. | `ovn_northd_status == 0` | Critical |
| `ovs_vswitchd_bridge` | Metric related to OVS bridges in OpenShift, which are foundational for VM networking. In OpenShift, there are typically two critical bridges: `br-int` (handling internal pod/VM traffic) and `br-ex` (gateway for external traffic). These bridges are essential for VM network connectivity - `br-int` connects VM interfaces within the cluster, while `br-ex` enables external access. Bridge failures would isolate VMs from other workloads or external networks. In a healthy OpenShift Virtualization environment, this metric should show both `br-int` and `br-ex` bridges. If either bridge is missing or reports an abnormal state, VMs may have partial or complete network failure, especially affecting external connectivity and inter-node VM communication. | `ovs_vswitchd_bridge{name="br-int"} != 1 or ovs_vswitchd_bridge{name="br-ex"} != 1` | Critical |
| `ovs_vswitchd_interface_tx_dropped_total` | Total number of transmitted packets dropped by OVS interfaces. High drop rates would cause VM applications to experience slow or failed outbound connections, affecting service reliability, especially for VMs serving external clients. | `rate(ovs_vswitchd_interface_tx_dropped_total[5m]) > 100` | High |
| `ovs_vswitchd_ofproto_packet_out` | Count of OpenFlow packet-out operations in OVS. These operations manually inject packets into the switch and abnormal values may indicate OVS controller issues that could affect VM network policy enforcement.| `rate(ovs_vswitchd_ofproto_packet_out[5m]) > 10` | Medium |
| `ovs_vswitchd_packet_in` | Count of packet-in operations where the switch sends packets to the OpenFlow controller for handling. Spikes during VM creation or migration are normal, but sustained high values could indicate network control plane issues that consume resources needed by VMs. | `rate(ovs_vswitchd_packet_in[5m]) > 100` | Medium |
| `ovs_vswitchd_txn_error` | Count of transaction errors in OVS. During network policy updates for VMs, transaction errors could prevent security rules from being applied, leaving VMs with incorrect access controls or connectivity issues. | `increase(ovs_vswitchd_txn_error[15m]) > 0` | High |
| `ovs_vswitchd_upcall_flow_limit_kill` | Count of flows killed due to upcall limits in OVS. If this increases during high VM network activity, it may indicate that the flow table capacity is insufficient for the VM workload, causing connection failures or degraded network performance. | `increase(ovs_vswitchd_upcall_flow_limit_kill[5m]) > 0` | High |
| `prometheus_notifications_dropped_total` | Total count of alert notifications that Prometheus has failed to send to Alertmanager. In OpenShift Virtualization, dropped notifications represent alerts about VM and infrastructure issues that never reach notification systems. Common causes include network issues between Prometheus and Alertmanager, Alertmanager unavailability, or queue overflow due to alert storms.| `increase(prometheus_notifications_dropped_total[15m]) > 0` | High |
| `prometheus_ready` | Binary indicator of whether Prometheus is ready (1) or not (0). If this is 0, all VM monitoring would be unavailable, making it impossible to detect issues with VMs, hosts, or networking components, potentially leading to undetected outages. | `prometheus_ready == 0` | Critical |
| `prometheus_rule_evaluation_failures_total` | Count of Prometheus rule evaluation failures. If VM over-commitment rules fail to evaluate, operators may not receive alerts when hosts become overloaded, potentially causing VM performance degradation before manual detection. | `increase(prometheus_rule_evaluation_failures_total[15m]) > 0` | High |
| `prometheus_sd_dns_lookup_failures_total` | Number of DNS lookup failures during service discovery. DNS resolution failures can prevent discovery of virtualization components, causing gaps in monitoring data for VM workloads. | `increase(prometheus_sd_dns_lookup_failures_total[15m]) > 0` | Medium |
| `prometheus_target_scrape_pool_exceeded_target_limit_total` | Count of times when scrape targets exceeded the configured limit. In large clusters with many VMs, hitting target limits could cause monitoring gaps where some virtual machines have no metrics collected, creating blind spots in operations. | `increase(prometheus_target_scrape_pool_exceeded_target_limit_total[15m]) > 0` | Medium |
| `prometheus_target_scrape_pools_failed_total` | Count of scrape pool failures. Failed scrape pools can result in missing metrics from entire classes of virtualization components. | `increase(prometheus_target_scrape_pools_failed_total[15m]) > 0` | High |
| `prometheus_target_sync_failed_total` | Count of failures when syncing targets from discovery results. Sync failures may result in stale or missing monitoring targets for VM components. | `increase(prometheus_target_sync_failed_total[15m]) > 0` | Medium |
| `prometheus_tsdb_head_truncations_failed_total` | Count of failed TSDB head truncations in Prometheus. If TSDB truncations fail, Prometheus may run out of storage space and crash, causing complete loss of monitoring for VMs, preventing alerting for critical VM issues. | `increase(prometheus_tsdb_head_truncations_failed_total[5m]) > 0` | High |
| `prometheus_tsdb_wal_corruptions_total` | Count of WAL (Write-Ahead Log) corruptions. Corruptions can lead to data loss for recent VM metrics and potential monitoring system instability. | `increase(prometheus_tsdb_wal_corruptions_total[15m]) > 0` | High |
| `reconstruct_volume_operations_errors_total` | Count of errors during volume reconstruction operations. Critical for VM persistent storage reliability as failed operations may affect volume availability for VMs. | `increase(reconstruct_volume_operations_errors_total[15m]) > 0` | High |
| `rest_client_request_duration_seconds_bucket` | Histogram of REST client request durations. Slow API responses may delay critical VM operations like live migrations, potentially extending maintenance windows or causing application downtime when VMs need to be moved. | `histogram_quantile(0.95, sum(rate(rest_client_request_duration_seconds_bucket[5m])) by (le)) > 1` | Medium |
| `scheduler_pod_scheduling_sli_duration_seconds_bucket` | Histogram of scheduling latency for pods. If the 95th percentile exceeds 5 seconds, VM creation operations might take longer than expected, affecting user experience and migration performance. | `histogram_quantile(0.95, sum(rate(scheduler_pod_scheduling_sli_duration_seconds_bucket[5m])) by (le)) > 5` | Medium |
| `scheduler_unschedulable_pods` | Count of pods that can't be scheduled. Unschedulable VM pods prevent new VMs from starting or existing VMs from migrating, directly affecting virtualization platform capacity. | `scheduler_unschedulable_pods{namespace="openshift-virtualization"} > 0` | High |
| `service_ca_expiry_time_seconds` | The Unix timestamp (in seconds) when the OpenShift Service CA certificate will expire. If the Service CA certificate expires, internal TLS communication between OpenShift components-including those managing and exposing VM services-will fail. This can break service-to-service communication, API access, and disrupt VM management, migrations, and monitoring. | `service_ca_expiry_time_seconds - time() < 604800` (alerts if expiry is within 7 days) | Critical |
| `thanos_alert_sender_alerts_dropped_total` | Count of alerts dropped by Thanos alert sender.  Dropped alerts may mean missing critical notifications about VM health or performance issues. | `increase(thanos_alert_sender_alerts_dropped_total[15m]) > 0` | High |
| `thanos_rule_alertmanagers_dns_failures_total` | Count of DNS failures when connecting to Alertmanager. If DNS failures occur when trying to reach the Alertmanager, critical VM failure or performance alerts may not be delivered, leading to delayed incident response. | `increase(thanos_rule_alertmanagers_dns_failures_total[15m]) > 0` | High |
| `thanos_sidecar_prometheus_up` | Indicates if Prometheus is reachable via Thanos sidecar. Critical for long-term metric retention of VM performance data. If `0`, VM historical metrics become inaccessible. | `thanos_sidecar_prometheus_up == 0` | Critical |  
| `vector_checksum_errors_total` | Total number of checksum errors detected by Vector when processing log/event data. Indicates data integrity issues during log ingestion or forwarding. High or increasing values suggest that some log entriesâ€”potentially including those from VM, hypervisor, or cluster eventsâ€”were corrupted or tampered with in transit. | `increase(vector_checksum_errors_total[15m]) > 0` | High |   
| `vector_http_server_handler_duration_seconds_bucket` | HTTP request latency for Vector. High latency delays VM log availability during incidents. | `histogram_quantile(0.95, sum(rate(vector_http_server_handler_duration_seconds_bucket[5m])) by (le)) > 2` | Medium |   
| `workload:cpu_usage_cores:sum` | Aggregated CPU usage across VMs. Sustained >85% usage risks VM throttling. | `sum(workload:cpu_usage_cores:sum) / sum(cluster:capacity_cpu_cores:sum) > 0.85` | Critical |  
| `workload:memory_usage_bytes:sum` | Total memory used by VMs. Combined with swap metrics, identifies memory pressure. | `sum(workload:memory_usage_bytes:sum) / sum(cluster:capacity_memory_bytes:sum) > 0.85` | High |  
| `write:apiserver_request_duration_seconds_bucket:rate1m` | API write latency. High values delay VM lifecycle operations. | `histogram_quantile(0.95, write:apiserver_request_duration_seconds_bucket:rate1m) > 1.5` | High | 