| Name | Description & Example | Example Query | Severity |
|------|----------------------|--------------|----------|
| `active_argocd_instances_total` | Shows the number of Argo CD instances currently managed across the cluster. Unexpected changes may indicate issues with Argo CD operator. For example, if this drops to 0 when you expect active instances, it indicates Argo CD instances are not being properly tracked. | `active_argocd_instances_total == 0` | High |
| `aggregator_unavailable_apiservice` | Number of APIService endpoints that are unavailable. Unavailable APIs can block VM management operations or integration with external platforms. | `aggregator_unavailable_apiservice > 0` | High |
| `alertmanager_cluster_health_score` | Health score of the Alertmanager cluster. Lower values are better, and zero means "totally healthy". Higher values indicate degraded Alertmanager functionality, which could lead to missing or delayed alert notifications. If the score rises above 1, it may indicate network issues between Alertmanager instances or other cluster health problems. | `alertmanager_cluster_health_score > 1` | High |
| `alertmanager_cluster_members` | Number of Alertmanager members in the cluster. Should match expected size. In OpenShift a size of 2 is expected. Lower values indicate degraded alerting HA, risking loss of VM/infrastructure alert delivery. | `alertmanager_cluster_members < 2` | Medium |
| `alertmanager_cluster_refresh_join_failed_total` | Total number of failed attempts to join the Alertmanager cluster. High values indicate cluster instability or misconfiguration, risking alerting HA. | `increase(alertmanager_cluster_refresh_join_failed_total[15m]) > 0` | High |
| `alertmanager_dispatcher_alert_processing_duration_seconds_count` | Number of alert events processed by the dispatcher per second. Used to monitor alert processing throughput; a sudden increase indicates an increasing number of problems observed in the platform | `rate(alertmanager_dispatcher_alert_processing_duration_seconds_count[5m]) == 0` | High |
| `alertmanager_http_concurrency_limit_exceeded_total` | Total HTTP requests rejected due to concurrency limits. Indicates Alertmanager is overloaded, risking dropped or delayed alerts for VM failures. | `increase(alertmanager_http_concurrency_limit_exceeded_total[15m]) > 0` | High |
| `alertmanager_http_request_duration_seconds_count` | 	Cumulative count of HTTP requests handled by Alertmanager, as tracked by the request duration histogram. Impact: This metric is a counter that increases with every HTTP request processed by Alertmanager. A sudden drop to zero in the rate of increase indicates that Alertmanager is not receiving or processing HTTP requests, which could mean the service is down, unreachable, or stalled. | `rate(alertmanager_http_request_duration_seconds_count[5m]) == 0` | Medium |
| `alertmanager_http_requests_in_flight` | Current number of concurrent HTTP requests being processed by Alertmanager. In most clusters, this value is typically very low, reflecting the fact that Alertmanager's API is lightly used except during bursts of alert delivery or configuration reloads. | `alertmanager_http_requests_in_flight > 20` | Medium |
| `alertmanager_marked_alerts` | Number of alerts marked by state (active, suppressed, unprocessed). Abnormal values may indicate alerts not being properly processed. A high number of unprocessed alerts could indicate that Alertmanager is falling behind in processing alerts. | `sum(alertmanager_alerts{state="unprocessed"}) > 10` | Medium |
| `alertmanager_nflog_gossip_messages_propagated_total` | Cumulative count of notification log ("nflog") gossip messages propagated by this Alertmanager instance. This metric tracks the number of notification log updates (such as sent notifications and silences) that have been gossiped to peers in an Alertmanager HA cluster. | `rate(alertmanager_nflog_gossip_messages_propagated_total[5m]) > 0` | High |
| `alertmanager_nflog_query_errors_total` | Counter for the number of notification log queries that failed. Increasing values indicate problems with the notification log system, potentially causing alerts to be lost or duplicated. If this counter increases rapidly, it may indicate issues with the notification log storage or retrieval mechanism. | `rate(alertmanager_nflog_query_errors_total[5m]) > 0.1` | Medium |
| `alertmanager_nflog_query_errors_total` | Total number of notification log query errors. High values indicate issues querying alert state, risking incorrect alert deduplication or suppression. | `increase(alertmanager_nflog_query_errors_total[15m]) > 0` | High |
| `alertmanager_nflog_snapshot_duration_seconds_count` | Cumulative count of notification log (nflog) snapshot operations in Alertmanager. This metric tracks how many times Alertmanager has taken a snapshot of its notification log, which is used for persisting alert state and supporting crash recovery and HA. | `rate(alertmanager_nflog_snapshot_duration_seconds_count[5m]) == 0` | Medium |
| `alertmanager_notification_latency_seconds_bucket` | Histogram of notification delivery latency. High values indicate delays in sending alerts for VM or infra failures, risking slow incident response. | `histogram_quantile(0.95, sum(rate(alertmanager_notification_latency_seconds_bucket[5m])) by (le)) > 5` | High |
| `alertmanager_notification_requests_failed_total` | The total number of failed notification requests. Increasing values indicate that alert notifications are failing to be delivered to configured receivers. If email notifications are failing, this metric would increase with the "integration=email" label. | `rate(alertmanager_notification_requests_failed_total{integration="email"}[5m]) > 0` | High |
| `alertmanager_notifications_failed_total` | Counter showing how many notifications have failed in total. Indicates problems with notification delivery to specific integrations. For example, if Slack notifications are failing, this would increase with the "integration=slack" label. | `increase(alertmanager_notifications_failed_total{integration="slack"}[1h]) > 0` | High |
| `alertmanager_silences_query_errors_total` | Counter for errors encountered when querying the silences database. Increasing values indicate problems with silence management, potentially causing alerts to fire when they should be silenced. If this counter increases, it may indicate corruption in the silences database or other storage issues. | `increase(alertmanager_silences_query_errors_total[1h]) > 0` | Medium |
| `apiextensions_apiserver_validation_ratcheting_seconds_sum` | Cumulative sum of seconds spent by the API server on ratcheting validation for CustomResourceDefinitions (CRDs). Ratcheting validation refers to the process where the API server enforces stricter schema validation rules as CRDs evolve between versions (e.g., from v1beta1 to v1). Excessive time spent here may indicate slow schema upgrades or problematic CRD changes, which can delay or disrupt VM CRD rollouts and upgrades in OpenShift Virtualization. | `increase(apiextensions_apiserver_validation_ratcheting_seconds_sum[15m]) > 5` | Low |
| `apiserver_admission_controller_admission_duration_seconds_bucket` | Histogram of admission controller latencies. High latencies can slow down all API operations, affecting cluster performance. If the 95th percentile latency exceeds 1 second, API operations will be noticeably slower. | `histogram_quantile(0.95, sum(rate(apiserver_admission_controller_admission_duration_seconds_bucket[5m])) by (le))` | Medium |
| `apiserver_admission_webhook_rejection_count` | Alert for high rate of admission controller errors. Indicates problems with admission controllers, potentially preventing resource creation. For example, if ValidatingAdmissionWebhooks are failing, custom resources may not be created properly. | `sum(rate(apiserver_admission_webhook_rejection_count[5m])) / sum(rate(apiserver_admission_webhook_request_total[5m])) > 0.05` | High |
| `apiserver_admission_webhook_rejection_count` | Alert for high rate of admission controller errors. Indicates problems with admission controllers, potentially preventing resource creation. If ValidatingAdmissionWebhooks are failing, custom resources may not be created properly. | `sum(rate(apiserver_admission_webhook_rejection_count[5m])) / sum(rate(apiserver_admission_webhook_request_total[5m])) > 0.05` | High |
| `apiserver_admission_webhook_rejection_count` | Count of admission webhook rejections, labeled by webhook, operation, and error type.  Frequent rejections can block VM creation, migration, or updates. | `increase(apiserver_admission_webhook_rejection_count[15m]) > 0` | Medium |
| `apiserver_audit_requests_rejected_total` | Counter of rejected audit requests. Increasing values indicate audit logging issues, potentially affecting compliance requirements. If this counter increases rapidly, it could indicate audit backend problems or configuration issues. | `rate(apiserver_audit_requests_rejected_total[5m]) > 0` | Medium |
| `apiserver_authorization_decisions_total` | Total number of authorization decisions (allow/forbid). Spikes in denied requests may indicate RBAC misconfiguration, blocking VM operations. | `sum(rate(apiserver_authorization_decisions_total{decision="forbid"}[5m])) > 0` | Medium |
| `apiserver_cache_list_total` | Total cache list operations. High values may indicate heavy API usage, possibly from VM controllers or monitoring. | `rate(apiserver_cache_list_total[5m]) > 100` | Low |
| `apiserver_client_certificate_expiration_seconds_bucket` | Histogram of client certificate expiration times. Imminent certificate expiry can break API access for VM controllers/operators. If you have less than 14 days until expiry and you manage the certs yourself, you will need to take action. | `histogram_quantile(0.01, sum(rate(apiserver_client_certificate_expiration_seconds_bucket[5m])) by (le)) < 1209600` | High |
| `apiserver_crd_conversion_webhook_duration_seconds_bucket` | Histogram of response times (in seconds) for CRD conversion webhooks. High latency in conversion webhooks can delay or block CRD object operations, including those for VM custom resources in OpenShift Virtualization. This can slow down VM creation, updates, or migrations if the conversion webhook is slow or unresponsive. | `histogram_quantile(0.95, sum(rate(apiserver_crd_conversion_webhook_duration_seconds_bucket[5m])) by (le)) > 1` | Medium |
| `apiserver_current_inflight_requests` | Current number of requests being processed. High values indicate API server overload, potentially causing timeouts and degraded performance. If this exceeds the configured max-in-flight limit, new requests will be rejected. | `sum(apiserver_current_inflight_requests) by (request_kind) > 100` | High |
| `apiserver_current_inqueue_requests` | Number of API requests currently queued. High values indicate API server overload, risking delays in VM operations. | `apiserver_current_inqueue_requests > 10` | High |
| `apiserver_delegated_authn_request_duration_seconds_bucket` | Histogram of durations (in seconds) for delegated authentication requests made by the Kubernetes API server. High latency in delegated authentication can delay all API calls that require authentication—including VM lifecycle operations in OpenShift Virtualization.| `histogram_quantile(0.95, sum(rate(apiserver_delegated_authn_request_duration_seconds_bucket[5m])) by (le)) > 1` | Medium |
| `apiserver_delegated_authz_request_duration_seconds_count` | Cumulative count of delegated authorization requests handled by the Kubernetes API server. This metric is most useful when combined with its companion sum metric to calculate average authorization latency. Spikes may signal RBAC or webhook bottlenecks that could delay or block VM operations in OpenShift Virtualization. | `rate(apiserver_delegated_authz_request_duration_seconds_count[5m]) > 100` | Low |
| `apiserver_delegated_authz_request_duration_seconds_sum` | Total time spent on delegated authorization. Used to compute average latency for authorization. | `rate(apiserver_delegated_authz_request_duration_seconds_sum[5m]) / rate(apiserver_delegated_authz_request_duration_seconds_count[5m]) > 1` | Medium |
| `apiserver_flowcontrol_current_executing_requests` | Number of requests currently being executed. High values may indicate API priority and fairness (APF) issues. If a non-critical priority level is consuming too many resources, it could starve critical requests. | `sum(apiserver_flowcontrol_current_executing_requests) by (priorityLevel) > 50` | Medium |
| `apiserver_flowcontrol_current_executing_seats` | Number of seats currently occupied by executing requests. Relates to API server concurrency control. If this approaches the upper limit, it indicates the API server is at maximum capacity. | `sum(apiserver_flowcontrol_current_executing_seats) / sum(apiserver_flowcontrol_upper_limit_seats) > 0.8` | Medium |
| `apiserver_flowcontrol_current_inqueue_requests` | **Current number of API requests waiting in the APF queue (not yet executing), labeled by `flow_schema` and `priority_level`.** High values indicate API server congestion, which can delay VM operations (creation, migration, deletion) as requests wait for available concurrency slots. | `apiserver_flowcontrol_current_inqueue_requests > 10` | High |
| `apiserver_flowcontrol_current_inqueue_seats` | Number of seats occupied by requests currently waiting in queues. Represents the resource consumption of queued requests. High values indicate that queued requests are consuming significant "seat" resources, which can lead to request throttling and delays in VM operations. Each request consumes one or more seats based on its resource requirements. | `apiserver_flowcontrol_current_inqueue_seats > 10` | Medium |
| `apiserver_flowcontrol_demand_seats_average` | Time-weighted average seat demand during the last concurrency borrowing adjustment period. Part of APF's dynamic concurrency borrowing algorithm. Values represent average demand pressure on the API server. | `apiserver_flowcontrol_demand_seats_average > 5` | Medium |
| `apiserver_flowcontrol_demand_seats_high_watermark` | High watermark of seat demand. Indicates peak API pressure, useful for capacity planning. | `apiserver_flowcontrol_demand_seats_high_watermark` | Medium |
| `apiserver_flowcontrol_demand_seats_smoothed` | Smoothed value of seat demand over time. **Impact**: Tracks ongoing API pressure affecting VM operations. | `apiserver_flowcontrol_demand_seats_smoothed > 2` | Medium |
| `apiserver_flowcontrol_demand_seats_sum` | Sum of seat demand across requests. **Impact**: High values indicate API server is under heavy load, risking delays for VM lifecycle events. | `apiserver_flowcontrol_demand_seats_sum > 10` | Medium |
| `apiserver_flowcontrol_priority_level_request_utilization_count` | Indicates request concurrency utilization by priority level. High values may indicate certain priority levels are overloaded. If the "system-node" priority level is highly utilized, node-related operations may be affected. | `apiserver_flowcontrol_priority_level_request_utilization_count{priorityLevel="system-node"} / apiserver_flowcontrol_upper_limit_seats{priorityLevel="system-node"} > 0.9` | Medium |
| `apiserver_flowcontrol_priority_level_seat_utilization_count` | Indicates seat concurrency utilization by priority level. High values may indicate certain priority levels are consuming too many resources. If a workload priority level is using most available seats, it could impact other operations. | `apiserver_flowcontrol_priority_level_seat_utilization_count{priorityLevel="workload-high"} / apiserver_flowcontrol_upper_limit_seats{priorityLevel="workload-high"} > 0.9` | Medium |
| `apiserver_flowcontrol_request_dispatch_no_accommodation_total` | Total number of requests that could not be accommodated due to concurrency limits. Increasing values indicate requests being rejected due to overload. If this increases for critical priority levels, important operations may be failing. | `increase(apiserver_flowcontrol_request_dispatch_no_accommodation_total{priorityLevel="system-cluster-critical"}[5m]) > 0` | High |
| `apiserver_flowcontrol_request_wait_duration_seconds_bucket` | Histogram of request waiting times. High wait times indicate API server congestion. If the 95th percentile wait time exceeds 1 second, API operations will be noticeably delayed. | `histogram_quantile(0.95, sum(rate(apiserver_flowcontrol_request_wait_duration_seconds_bucket[5m])) by (le, priorityLevel))` | Medium |
| `apiserver_flowcontrol_upper_limit_seats` | Maximum number of seats available per priority level. This is a configuration limit that affects concurrency. This is typically a static value but is important for calculating utilization percentages. | `apiserver_flowcontrol_upper_limit_seats` | Low |
| `apiserver_kube_aggregator_x509_insecure_sha1_total` | Count of insecure SHA1 certificates used by aggregated APIs. Insecure certificates in the aggregator can compromise security for VM API extensions and custom resources. SHA1 certificates are deprecated and pose security risks for VM management APIs. | `increase(apiserver_kube_aggregator_x509_insecure_sha1_total[15m]) > 0` | High |
| `apiserver_list_watch_request_success_total:rate:sum` | Rate of successful list/watch API requests aggregated across the cluster. Impact: Critical for VM status monitoring and real-time updates. Failed list/watch operations can cause VM controllers to miss state changes, affecting migration monitoring and status updates. | `apiserver_list_watch_request_success_total:rate:sum` | Low |
| `apiserver_request_aborts_total` | Total count of API requests that were aborted during processing. Request aborts occur when the API server cannot complete request processing due to timeouts, client disconnections, or internal errors. High abort rates indicate API server instability or resource exhaustion. | `rate(apiserver_request_aborts_total[5m]) > 0.1` | High |
| `apiserver_request:burnrate1d` | API request burn rate over 1 day, measured by taking the failed API requests divided by the total number of requests. High values indicate sustained API server load. If this exceeds SLO thresholds, it indicates the API server has been overloaded for an extended period. | `apiserver_request:burnrate1d > 2` | Medium |
| `apiserver_request:burnrate1h` | Error budget burn rate for API server SLO over 1 hour.  High burn rates indicate API server reliability issues that directly affect VM operations reliability and SLA compliance for virtualization workloads. | `sum(apiserver_request:burnrate1h) > (14.4 * 0.01)` | Critical |
| `apiserver_request:burnrate30m` | API request burn rate over 30 minutes, measured by taking the failed API requests divided by the total number of requests. High values indicate medium-term API server load. If this exceeds SLO thresholds, it indicates the API server has been overloaded recently. | `apiserver_request:burnrate30m > 5` | High |
| `apiserver_request:burnrate5m` | API request burn rate over 5 minutes, measured by taking the failed API requests divided by the total number of requests. High values indicate short-term API server load spikes. If this exceeds SLO thresholds, it indicates the API server is currently overloaded. | `apiserver_request:burnrate5m > 10` | Critical |
| `apiserver_request_duration_seconds_bucket` | Histogram of API request durations in seconds. High latency affects all VM operations including creation, migration, and status updates. Slow API responses can cause VM operation timeouts and degraded user experience. | `histogram_quantile(0.99, sum(rate(apiserver_request_duration_seconds_bucket[5m])) by (le)) > 1` | High |
| `apiserver_request_post_timeout_total` | Total number of POST requests that have timed out. Increasing values indicate API server performance issues. If this increases for create operations, new resources may fail to be created. | `increase(apiserver_request_post_timeout_total{verb="create"}[5m]) > 0` | High |
| `apiserver_request_sli_duration_seconds_bucket` | Histogram for Service Level Indicator (SLI) request durations. Used for SLO tracking of API performance. Poor SLI metrics indicate degraded service quality affecting VM operation reliability. The threshold in the query is just an example!| `histogram_quantile(0.99, sum(rate(apiserver_request_sli_duration_seconds_bucket[5m])) by (le)) > 0.5` | High |
| `apiserver_request_sli_duration_seconds_count` | Count of requests used for SLI (Service Level Indicator) calculations. Used for SLO monitoring. This is primarily used for calculating error rates and latencies against SLOs. | `sum(rate(apiserver_request_sli_duration_seconds_count{code=~"5.."}[5m])) / sum(rate(apiserver_request_sli_duration_seconds_count[5m]))` | Medium |
| `apiserver_request_slo_duration_seconds_bucket` | Histogram for Service Level Objective (SLO) request durations. Tracks API performance against defined SLOs. SLO violations indicate service degradation that affects VM operation guarantees. The threshold in the query is just an example!| `histogram_quantile(0.99, sum(rate(apiserver_request_slo_duration_seconds_bucket[5m])) by (le)) > 0.6` | High |
| `apiserver_request_slo_duration_seconds_count` | Count of requests used for SLO (Service Level Objective) calculations. Used for SLO monitoring. Similar to SLI metric, used for calculating compliance with SLOs. | `sum(rate(apiserver_request_slo_duration_seconds_count{code=~"5.."}[5m])) / sum(rate(apiserver_request_slo_duration_seconds_count[5m]))` | Medium |
| `apiserver_request_timestamp_comparison_time_bucket` |Histogram of time spent by the Kubernetes API server comparing timestamps of incoming requests. Timestamp comparison is a step in request processing to ensure correct ordering and consistency, especially important for watch requests and status updates. High overhead in timestamp comparison can add latency to API server responses. | `histogram_quantile(0.95, sum(rate(apiserver_request_timestamp_comparison_time_bucket[5m])) by (le)) > 0.01` | Low |
| `apiserver_request_total` | Total count of API requests processed. High request volumes during VM operations (creation, migration, scaling) help identify capacity constraints and usage patterns. This is also useful to identify abnormal behaviour over time. | `rate(apiserver_request_total[5m])` | Medium |
| `apiserver_selfrequest_total` | Total count of API server requests made by the API server itself (internal self-requests), not by external clients. These are requests generated internally, for example, when the API server needs to fetch or update its own configuration, or when performing background synchronization or cleanup tasks. High rates of self-requests may indicate increased internal activity, which competes with user-initiated requests (like VM creation, migration, or status updates) for API server resources. While usually low, persistent high rates could signal internal issues or misconfiguration. | `rate(apiserver_selfrequest_total[5m]) > 10` | Low |
| `apiserver_storage_data_key_generation_failures_total` | Total number of failed data encryption key generation operations. Increasing values indicate issues with etcd encryption. If this increases, it could indicate problems with the encryption provider or configuration. | `increase(apiserver_storage_data_key_generation_failures_total[1h]) > 0` | Critical |
| `apiserver_storage_db_total_size_in_bytes` | Total size of the etcd database in bytes. As VM count grows, etcd size increases. Approaching the 8GB default limit will make the cluster read-only, preventing all VM operations (creation, migration, deletion). OpenShift > 4.16 has a default size of 8GB.| `apiserver_storage_db_total_size_in_bytes > 7.1e9` | Critical |
| `apiserver_storage_decode_errors_total` | Total count of storage decode errors when retrieving objects from etcd. Decode errors can prevent VM resource retrieval, causing VM status inconsistencies, failed operations, or inability to access VM configuration data. | `increase(apiserver_storage_decode_errors_total[15m]) > 0` | High |
| `apiserver_storage_events_received_total` | Total count of storage events received by the API server from the ETCD. This metric reflects the rate at which the API server is receiving state change notifications from the cluster’s data store. High event rates may indicate heavy cluster activity or frequent configuration updates, which can put pressure on the storage backend. Conversely, a sudden drop or sustained zero rate may signal connectivity problems, storage backend failures, or API server disconnection from the data store. | `rate(apiserver_storage_events_received_total[5m])` | Medium |
| `apiserver_storage_list_total` | Total count of list operations performed against storage. This may be a useful metric to use during intrusion detection. If an entity has gained access to the API, they may issue a lot of lists. | `rate(apiserver_storage_list_total[5m])` | Low |
| `apiserver_storage_size_bytes` | Size of the storage database file physically allocated in bytes. High values may indicate etcd database growth issues. If this approaches the etcd size limit (typically 2GB), it could lead to API server instability. | `apiserver_storage_size_bytes > 7*1024*1024*1024` | High |
| `apiserver_tls_handshake_errors_total` | Total count of TLS handshake errors for API server connections. TLS errors can prevent VM controllers, operators, and management tools from accessing the API, disrupting VM lifecycle operations and monitoring. | `increase(apiserver_tls_handshake_errors_total[15m]) > 0` | High |
| `apiserver_watch_cache_read_wait_seconds_bucket` | Histogram of time spent waiting to read from watch cache. This cache stores recent object states to serve watch requests efficiently. | `histogram_quantile(0.95, sum(rate(apiserver_watch_cache_read_wait_seconds_bucket[5m])) by (le)) > 0.1` | Medium |
| `apiserver_webhooks_x509_insecure_sha1_total` | Total count of times the API server detected insecure SHA1 certificates being used by admission webhooks. Impact: SHA1 certificates are considered cryptographically weak and are deprecated due to security vulnerabilities. When the API server encounters a webhook secured with such a certificate, it logs this event. This poses a security risk to the admission control process, which is critical for VM creation and updates. If a webhook certificate is insecure, it may fail validation, block VM lifecycle operations, or expose the cluster to man-in-the-middle attacks. | `increase(apiserver_webhooks_x509_insecure_sha1_total[15m]) > 0` | High |
| `argocd_app_reconcile_bucket` | Histogram of ArgoCD application reconciliation durations. High values indicate ArgoCD performance issues affecting automated deployment of kubernetes resources. | `histogram_quantile(0.95, sum(rate(argocd_app_reconcile_bucket[5m])) by (le)) > 30` | Medium |
| `argocd_git_request_duration_seconds_bucket` | Histogram of how long it takes ArgoCD to complete requests to Git repositories (in seconds). This metric tracks the time ArgoCD’s Repo Server spends fetching or interacting with Git repos to retrieve application manifests. If these Git operations are slow, it will delay the syncing and deployment of VM configurations or updates in a GitOps workflow. High values mean that ArgoCD is waiting on Git. | `histogram_quantile(0.95, sum(rate(argocd_git_request_duration_seconds_bucket[5m])) by (le)) > 5` | Medium |
| `argocd_git_request_duration_seconds_sum` | Total time spent on ArgoCD Git requests. Used with count to calculate average Git request latency. High values indicate Git performance issues affecting VM GitOps deployment speed. | `rate(argocd_git_request_duration_seconds_sum[5m]) / rate(argocd_git_request_duration_seconds_count[5m]) > 2` | Medium |
| `argocd_redis_request_duration_bucket` | Histogram of how long (in seconds) Redis requests take in ArgoCD. ArgoCD uses Redis as a cache and for internal state management (such as application status and cluster cache). If Redis requests are slow, it can delay application reconciliation, syncing, and status updates—including those for VM deployments managed through GitOps. | `histogram_quantile(0.95, sum(rate(argocd_redis_request_duration_bucket[5m])) by (le)) > 0.1` | Low |
| `authenticated_user_requests` | Count of authenticated requests. Unexpected changes may indicate authentication issues. A sudden drop could indicate authentication system problems. | `rate(authenticated_user_requests[5m])` | Medium |
| `authentication_attempts` | Count of authentication attempts. High failure rates indicate authentication configuration issues or potential security incidents. A high rate of failed attempts could indicate a brute force attack. | `sum(rate(authentication_attempts{result="error"}[5m])) / sum(rate(authentication_attempts[5m])) > 0.5` | High |
| `authentication_duration_seconds_bucket` | Histogram of time spent authenticating API requests (in seconds). All VM operations require authentication. Slow authentication delays VM creation, migration, deletion, and status updates, directly affecting user experience and automation workflows. | `histogram_quantile(0.95, sum(rate(authentication_duration_seconds_bucket[5m])) by (le)) > 1` | Medium |
| `authentication_duration_seconds_sum` | Total time spent on authentication across all requests. When combined with request count, shows average authentication latency. High values indicate authentication bottlenecks that slow down all VM management operations. | `rate(authentication_duration_seconds_sum[5m]) / rate(authentication_duration_seconds_count[5m]) > 0.5` | Medium |
| `cardinality_enforcement_unexpected_categorizations_total` | Cumulative count of metric series that could not be properly categorized for cardinality enforcement. High cardinality means a metric has too many unique label combinations (e.g., due to unique IDs, session tokens, or timestamps as labels). This can overwhelm monitoring systems, causing high memory usage, slow queries, or even data loss. When metrics cannot be categorized, it means the system cannot apply its cardinality controls, increasing the risk that runaway label combinations will degrade monitoring performance.  | `increase(cardinality_enforcement_unexpected_categorizations_total[15m]) > 0` | Low |
| `catalogsource_ready` | Indicates whether Operator Lifecycle Manager catalog sources are ready. Catalog sources provide operators and updates. If not ready, you cannot install, update, or manage OpenShift Virtualization operators, blocking platform maintenance and feature updates. | `catalogsource_ready{name="redhat-operators"} == 0` | Medium |
| `cco_controller_reconcile_seconds_bucket` | Histogram of Cloud Credential Operator reconciliation times (in seconds). CCO manages cloud credentials needed for storage and networking. Slow reconciliation can delay VM operations that require cloud resources like persistent volumes or load balancers. | `histogram_quantile(0.95, sum(rate(cco_controller_reconcile_seconds_bucket[5m])) by (le)) > 10` | Medium |
| `cco_credentials_requests_conditions` | Tracks the status conditions of Cloud Credential Operator credential requests. Indicates issues with cloud provider credentials that could affect cluster functionality. For example, if this metric shows credential requests in a degraded state, cloud resources may be inaccessible. | `cco_credentials_requests_conditions{condition="Degraded", status="True"} > 0` | High |
| `cco_credentials_requests` | Number of credential requests managed by Cloud Credential Operator. High numbers may indicate credential management issues. Failed credential requests can prevent VMs from accessing cloud storage or networking resources. | `increase(cco_credentials_requests[15m]) > 0` | High |
| `certwatcher_read_certificate_errors_total` | Total certificate reading errors from the certificate watcher. Certificate errors can break TLS communications between VM components, operators, and APIs, causing authentication failures and disrupting VM management operations. | `increase(certwatcher_read_certificate_errors_total[15m]) > 0` | High |
| `cluster:apiserver_current_inflight_requests:sum:max_over_time:2m` | Maximum number of concurrent requests to the API server over a 2-minute window. High values indicate API server congestion. For example, if this exceeds 100 for mutating requests, the API server may be overloaded, causing slow responses or failures. | `cluster:apiserver_current_inflight_requests:sum:max_over_time:2m{request_kind="mutating"} > 100` | High |
| `cluster:capacity_memory_bytes:sum` | Total **allocatable** memory across all nodes (not raw hardware capacity). This metric is critical for VM scheduling, as OpenShift Virtualization uses this value minus existing commitments to determine if new VMs can be scheduled. It does not account for memory overcommit configurations, so even if this metric shows 512GB but your VMs have 600GB in memory requests (with overcommit), new VMs may still schedule despite appearing to exceed "capacity." | `cluster:memory_usage_bytes:sum / cluster:capacity_memory_bytes:sum > 0.85` | Medium |
| `cluster:console_auth_login_failures_total:sum` | Total failed login attempts to the OpenShift web console. High failure rates may indicate security issues or user experience problems. Failed logins prevent users from accessing the VM management interface through the web console. | `increase(cluster:console_auth_login_failures_total:sum[15m]) > 10` | Medium |
| `cluster:console_auth_login_successes_total:sum` | Total successful login attempts to the OpenShift web console. Tracks user access to the platform. Sudden drops may indicate authentication system issues preventing users from managing VMs through the web interface. | `rate(cluster:console_auth_login_successes_total:sum[5m]) == 0` | Low |
| `cluster:container_cpu_usage:ratio` | Ratio of actual CPU usage to CPU requests across all containers in the cluster. This metric shows how much CPU your containers (including VM pods) are actually using compared to what they requested from Kubernetes. If the ratio is high (close to or above 1), it means containers are using as much or more CPU than they asked for. A consistently high ratio suggests you may be overcommitting CPU resources or need to adjust your requests and limits for better performance and reliability. | `cluster:container_cpu_usage:ratio > 0.8` | Medium |
| `cluster:control_plane:all_nodes_ready` | Indicates whether all control plane nodes are in a ready state . Non-ready control plane nodes affect cluster stability. <br><br>**Metric values:**<br> - `1` indicates the node is ready. <br>- `0` indicates the node is not ready. <br><br>For example, if this metric is 0, at least one control plane node is not ready, which could affect high availability. | `cluster:control_plane:all_nodes_ready == 0` | Critical |
| `cluster:cpu_usage_cores:sum` | Total CPU cores currently used cluster-wide. Shows how much CPU is being consumed by all workloads, including VMs. High usage can lead to VM throttling or inability to schedule new VMs. | `sum(cluster:cpu_usage_cores:sum) / sum(cluster:capacity_cpu_cores:sum) > 0.8` | High |
| `cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum` | Total storage requested by PVCs per storage provisioner. Ensures enough storage is available for VM disks. High values close to provisioner limits can block VM provisioning or expansion. | `cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum > $(provisioner_capacity)` | High |
| `cluster:master_nodes` | Count of master/control plane nodes in the cluster. Impact: Unexpected changes may indicate node failures. For example, if this drops below the expected number (typically 3), cluster control plane redundancy is compromised. | `cluster:master_nodes == 0` | Medium |
| `cluster_master_schedulable` | Indicates if master nodes are schedulable for workloads. Masters should not run VMs; if true, it risks control plane stability and resource contention. <br><br>**Metric values:**<br>- `1` the control plane is scheduleable<br>- `0` the control plane is not schedulable| `cluster_master_schedulable == 1` | Medium |
| `cluster:memory_usage_bytes:sum` | Total memory **usage** (working set) across all nodes. High values relative to *allocatable* memory indicate potential contention, but OpenShift Virtualization's memory overcommit allows scheduling beyond 100% of allocatable memory. This metric is critical for detecting swap/thrashing scenarios that degrade VM performance. For example, if this reaches 90% of allocatable memory and swap usage is increasing, VMs may experience severe latency due to host memory pressure, even if free memory exists in the overcommit pool.| `cluster:memory_usage_bytes:sum / cluster:capacity_memory_bytes:sum > 0.85` | Medium |
| `cluster:memory_usage:ratio` | Ratio of memory currently used to total physical memory available in the cluster. This metric shows how much of your cluster’s total RAM is actively in use by all workloads, including VMs. A high value (for example, >0.8 or 80%) means you are close to exhausting your cluster’s memory. | `cluster:memory_usage:ratio > 0.8` | High |
| `cluster_monitoring_operator_reconcile_attempts_total` | Number of times the cluster monitoring operator has attempted reconciliation. Frequent attempts may indicate monitoring instability, risking gaps in VM observability and alerting. | `rate(cluster_monitoring_operator_reconcile_attempts_total[5m]) > 5` | Medium |
| `cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits` | Total CPU limits set for pods in each namespace. Ensures VM pods (virt-launcher) have defined CPU limits, preventing noisy neighbor issues and resource contention. | `sum by (namespace)(cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits)` | Medium |
| `cluster:namespace:pod_memory:active:kube_pod_container_resource_limits` | Total memory limits set for pods in each namespace. Ensures VM pods have defined memory limits, reducing risk of OOM kills and ensuring fair resource allocation. | `sum by (namespace)(cluster:namespace:pod_memory:active:kube_pod_container_resource_limits)` | Medium |
| `cluster:network_attachment_definition_enabled_instance_up:max` | Maximum number of enabled network attachment definition instances that are up. Impact: Indicates health of multus CNI attachments. If this is 0 when using secondary networks, pods requiring additional networks may fail to start. | `cluster:network_attachment_definition_enabled_instance_up:max == 0` | High |
| `cluster:network_attachment_definition_enabled_instance_up:max` | Maximum number of enabled network attachment definition instances that are up. Indicates health of multus CNI attachments. For example, if this is 0 when using secondary networks, pods requiring additional networks may fail to start. | `cluster:network_attachment_definition_enabled_instance_up:max == 0` | High |
| `cluster:node_cpu:sum_rate5m` | Total rate of CPU usage (in cores) summed across all nodes in the cluster, averaged over 5 minutes.  This metric tells you how much CPU is actually being used by all workloads—including VMs, pods, and system processes—across your entire cluster. If this value, divided by your total available CPU cores, is high (for example, above 0.9 or 90%), it means your cluster is nearly fully loaded. | `sum(cluster:node_cpu:sum_rate5m) /sum(cluster:capacity_cpu_cores:sum) > 0.8` | High |
| `cluster_operator_conditions` | Represents the health conditions of core cluster operators. Degraded operators can affect overall cluster functionality. For example, if the OpenShift Virtualization operator shows a False Ready condition, virtualization features may be unavailable. <br><br>**Metric values:**<br>- `1` Operator is healthy<br>- `0` Operator is unhealthy.| `cluster_operator_conditions{name="kubevirt-hyperconverged", condition="Available" == 0` | Critical |
| `cluster_operator_payload_errors` | Counts errors encountered during cluster operator payload processing. Increasing values indicate problems with operator updates. For example, if this increases during an upgrade, it may indicate problems applying new operator versions. | `sum(rate(cluster_operator_payload_errors[15m])) > 0` | High |
| `cluster_operator_up` | Indicates if cluster operators are running. Operators (e.g., KubeVirt, CNO) manage VM lifecycle, storage, and networking. If any are down, VM management may be disrupted. | `cluster_operator_up == 0` | Critical |
| `cluster:ovnkube_controller_admin_network_policies_db_objects:max` | Maximum number of admin network policy objects in OVN-Kubernetes. Large numbers may slow down network policy processing, affecting VM network security and connectivity. | `cluster:ovnkube_controller_admin_network_policies_db_objects:max > 10000` | Medium |
| `cluster:ovnkube_controller_admin_network_policies_rules:max` | Maximum number of admin network policy rules in OVN-Kubernetes. Too many rules can increase latency for VM network operations and policy enforcement. | `cluster:ovnkube_controller_admin_network_policies_rules:max > 1000` | Medium |
| `cluster:ovnkube_controller_egress_routing_via_host:max` | Maximum number of egress routes via host in OVN-Kubernetes. High values may indicate suboptimal VM network paths, increasing latency and reducing performance. | `cluster:ovnkube_controller_egress_routing_via_host:max > 100` | Low |
| `cluster_quantile:apiserver_request_duration_seconds:histogram_quantile` | Shows the API server request latency at a given percentile (e.g., 99th percentile). High values mean some API requests (including VM create, update, or delete) are slow, which can delay or disrupt VM operations. | `cluster_quantile:apiserver_request_duration_seconds:histogram_quantile{quantile="0.99"}` | High |
| `cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile` | Shows how long the Kubernetes scheduler takes to make scheduling decisions at a given percentile. High values mean it takes longer to start or migrate VMs, which can slow down scaling or recovery from failures. | `cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile{quantile="0.95"} > 1` | Medium |
| `cluster:route_metrics_controller_routes_per_shard:max` | Maximum number of routes managed by any ingress shard. Too many routes on a single shard can cause networking bottlenecks, affecting how VMs are accessed externally. | `cluster:route_metrics_controller_routes_per_shard:max > 1000` | Low |
| `cluster:telemetry_selected_series:count` | Total number of active metrics time series being collected for monitoring. Too many series can overload monitoring systems, risking loss of VM and cluster observability. | `cluster:telemetry_selected_series:count > 1000000` | Low |
| `cluster:usage:ingress_frontend_bytes_in:rate5m:sum` | Total incoming network traffic (bytes) to all ingress frontends, per 5 minutes. High values may indicate heavy application or VM usage, or possible network saturation. | `cluster:usage:ingress_frontend_bytes_in:rate5m:sum > network_capacity` | Low |
| `cluster:usage:ingress_frontend_bytes_out:rate5m:sum` | Total outgoing network traffic (bytes) from all ingress frontends, per 5 minutes. High values could signal heavy VM or application traffic, which may lead to network congestion. | `cluster:usage:ingress_frontend_bytes_out:rate5m:sum > network_capacity` | Low |
| `cluster:usage:ingress_frontend_connections:sum` | Total number of active connections to all ingress frontends. High connection counts may indicate high demand or potential overload, affecting VM and application accessibility. | `cluster:usage:ingress_frontend_connections:sum > $(connection_limit)` | Low |
| `cluster:usage:openshift:ingress_request_error:fraction5m` | Fraction of ingress requests that resulted in errors over 5 minutes. High error rates mean users may not be able to access VMs or services reliably. | `cluster:usage:openshift:ingress_request_error:fraction5m > 0.05` | High |
| `cluster:usage:openshift:kube_running_pod_ready:avg` | Average fraction of running pods that are in the Ready state across the cluster.  This metric shows the overall health of your workloads, including VMs in OpenShift Virtualization. <br><br>**Metric values:** The value ranges from 0 to 1 <br>- `1`  all running pods are ready<br>- `0` no pods are ready.<br> A value close to 1 means almost all pods are healthy and serving traffic. | `cluster:usage:openshift:kube_running_pod_ready:avg < 0.95` | High |
| `cluster:usage:resources:sum` | Aggregated resource usage across the cluster. Helps identify resource consumption trends. For example, if CPU usage is consistently high, it may indicate the need for cluster scaling. | `cluster:usage:resources:sum{resource="virtualmachineinstances.kubevirt.io"}` | Medium |
| `cluster_version_capability` | Shows which features are enabled (1) or disabled (2) in your OpenShift cluster. This tells you what capabilities your cluster supports, such as whether virtualization features are enabled. <br><br>**Metric values:**<br>- `1` Feature is enabled<br>- `0` Feature is disabled| `cluster_version_capability{name="MachineAPI"}` | Low |
| `cluster_version_payload` | Shows the current version of OpenShift running on your cluster and tracks updates including the applied or pending versions in the cluster. Mostly used during or after an upgrade to ensure all components are at the same version. | `cluster_version_payload{version!="4.16.0"}` | Medium |
| `cluster:virt_platform_nodes:sum` | Total number of nodes that can run virtual machines in your cluster as well as make and model of the servers (if available). | `cluster:virt_platform_nodes:sum` | High |
| `cnv_abnormal` | The metric tracks two specific memory conditions:<ul><li>`memory_working_set_delta_from_request`: The difference between the working set memory and the requested memory</ul></li> <ul><li>`memory_rss_delta_from_request`: The difference between the resident set size (RSS) memory and the requested memory </ul></li>Impact: Abnormal conditions may affect VM functionality. Positive values indicate the component is using more memory than requested, while negative values indicate it's using less than requested.<br><br> Large positive values could indicate memory pressure in your virtualization components. Consistently high values might suggest you need to adjust the resource requests for your virtualization components| `sum by (container) (cnv_abnormal{reason="memory_working_set_delta_from_request"})` | High |
| `cnv:vmi_status_running:count` | Count of running Virtual Machine Instances. Unexpected changes may indicate VM issues. For example, if this drops suddenly, VMs may be failing or being terminated unexpectedly. | `sum by(node) (cnv:vmi_status_running:count)`<br><br>`sum by (guest_os_name)(cnv:vmi_status_running:count)` | Medium |
| `code_handler_job_namespace:lokistack_gateway_http_requests:irate1m` | Rate of HTTP requests to the logging system gateway per minute, by response code. This tracks access to your cluster's logging system. If VM logs aren't being collected properly due to gateway errors, you'll lose visibility into VM performance and troubleshooting information. | `sum(rate(code_handler_job_namespace:lokistack_gateway_http_requests:irate1m{code=~"5.."}[5m])) > 0` | Medium |
| `component_resource:apiserver_request_terminations_total:rate:1m` | Rate of API server request terminations over 1 minute. High values indicate API server overload. If this exceeds normal baseline, the API server is terminating requests due to overload. | `component_resource:apiserver_request_terminations_total:rate:1m > 10` | High |
| `component_resource:apiserver_request_terminations_total:rate:5m` | Rate of API server request terminations over 5 minutes. High values indicate sustained API server overload. If this exceeds normal baseline, the API server is consistently terminating requests. | `component_resource:apiserver_request_terminations_total:rate:5m > 5` | High |
| `console_auth_login_failures_total` | Total number of failed attempts to log into the OpenShift web console. High failure rates could indicate authentication problems that prevent users from accessing VM management tools, or potential security issues. | `increase(console_auth_login_failures_total[15m]) > 10` | Medium |
| `console_auth_login_requests_total` | Total number of login attempts to the OpenShift web console. Shows how much the web console (including VM management interfaces) is being used. Sudden drops might indicate the console is unavailable, preventing users from managing VMs through the web interface. | `rate(console_auth_login_requests_total[5m])` | Low |
| `console_auth_token_refresh_requests_total` | Total number of requests to refresh authentication tokens for the web console. Users need valid tokens to stay logged into the VM management interface. | `increase(console_auth_token_refresh_requests_total[15m]) > 10` | Low |
| `console_usage_total` | Total number of times the OpenShift web console has been used. This metric counts how often users access and interact with the OpenShift web console, including the VM management pages. | `console_usage_total` | Low |
| `container_blkio_device_usage_total` | Total disk I/O operations performed by containers (including VM containers). VMs on OpenShift run inside containers, and this shows how much disk activity they're generating. High values might indicate VMs doing heavy disk work, which could affect performance or indicate storage bottlenecks. To better understand device numbers see the [kernal documentation](https://www.kernel.org/doc/Documentation/admin-guide/devices.txt). | `rate(container_blkio_device_usage_total[5m]) > $(storage_iops_limit)` | Medium |
| `container_cpu_cfs_periods_total` | Total number of CPU scheduling periods that have elapsed for a container. This metric counts how many times the Linux CPU scheduler has checked whether a container (including VMs) can use CPU, based on its CPU limits. Each "period" is typically 100 milliseconds. By itself, this metric tells you how often the system has evaluated CPU usage for the container, but it is most useful when combined with throttling metrics to understand if your containers or VMs are being limited by CPU restrictions. | `rate(container_cpu_cfs_periods_total{container="virt-launcher"}[5m])` | Low |
| `container_cpu_cfs_throttled_periods_total` | Total number of periods that a container was throttled due to CPU limits. High values indicate CPU contention. For example, if a critical container shows high throttling, it may experience performance degradation. | `rate(container_cpu_cfs_throttled_periods_total{namespace="openshift-logging"}[5m]) / rate(container_cpu_cfs_periods_total{namespace="openshift-logging"}[5m]) > 0.25` | Medium |
| `container_cpu_cfs_throttled_seconds_total` | Total time (in seconds) that a container was throttled due to CPU limits. High values indicate CPU contention. For example, if collector pods show high throttling, logging may be affected. | `rate(container_cpu_cfs_throttled_seconds_total{namespace="openshift-logging"}[5m]) > 0.1` | Medium |
| `container_cpu_system_seconds_total` | Total CPU time spent by containers running system/kernel operations. Shows how much CPU your VMs are using for system-level tasks (like file I/O, network operations). High values might indicate VMs are doing intensive system operations that could impact performance. | `rate(container_cpu_system_seconds_total{pod=~"virt-launcher-.*"}[5m])` | Low |
| `containerd_cri_input_bytes_total` | Total bytes received by containerd CRI. Helps monitor container runtime network traffic. For example, sudden spikes may indicate unusual container activity or potential issues. | `rate(containerd_cri_input_bytes_total[5m]) > 1e6` | Low |
| `containerd_cri_output_bytes_total` | Total bytes of logs/output produced by containers managed by containerd. Large output can indicate chatty or misbehaving VMs/containers, and excessive logging may fill up disk space. | `rate(containerd_cri_output_bytes_total[5m]) > $(log_output_threshold)` | Low |
| `container_fs_reads_bytes_total` | Total bytes read from disk by a container. High values mean a VM or container is reading a lot from disk, which could indicate heavy database or application activity. Sudden spikes may signal backup jobs, indexing, or a VM under stress. | `rate(container_fs_reads_bytes_total[5m]) > $(read_bytes_threshold)` | Low |
| `container_fs_reads_total` | Total number of disk read operations performed by a container. Impact: High read counts may indicate disk-intensive workloads or that a VM is under heavy I/O load, which can slow down other VMs sharing the same storage. | `rate(container_fs_reads_total[5m]) > $(read_ops_threshold)` | Medium |
| `container_fs_usage_bytes` | Filesystem usage in bytes per container. High values may indicate disk space issues within containers. For example, if a container's filesystem usage approaches its limit, the container may experience write failures. | `container_fs_usage_bytes / container_fs_limit_bytes > 0.9` | Medium |
| `container_fs_write_seconds_total` | Total time (in seconds) spent writing to disk by a container. High values indicate containers (including VMs) are spending a lot of time on disk writes, which may signal slow disks or I/O bottlenecks affecting VM responsiveness. | `rate(container_fs_write_seconds_total[5m]) > $(write_time_threshold)` | Low |
| `container_fs_writes_total` | Total number of write operations performed by a container. High write counts can indicate heavy logging, database activity, or a misbehaving VM. Too many writes can wear out SSDs or fill up disk space.| `rate(container_fs_writes_total[5m]) > $(write_ops_threshold)` | Medium |
| `container_last_seen` | Timestamp of the last time a container was observed running. Helps detect if a VM or container has crashed or been deleted. If a VM’s container hasn’t been seen recently, it may have failed or been evicted. | `time() - container_last_seen > 300` | Medium |
| `container_memory_failcnt` | Number of memory allocation failures in a container. Indicates memory pressure. For example, if this is increasing, containers are hitting memory limits and failing to allocate memory. | `increase(container_memory_failcnt[5m]) > 0` | High |
| `container_memory_kernel_usage` | Amount of memory used by the kernel within the container. High kernel memory usage in a VM can indicate kernel leaks or abnormal behavior, which may lead to resource exhaustion or instability. | `container_memory_kernel_usage ` | Low |
| `container_memory_rss` | Resident Set Size: the amount of physical memory used by the container. Shows how much RAM a VM or container is actually using. High values can lead to memory pressure on the node, risking VM evictions or slowdowns. | `container_memory_rss` | Low |
| `container_memory_swap` | Amount of swap space used by a container. High values indicate memory pressure. For example, if containers are using swap, it can significantly degrade performance. | `container_memory_swap > 0` | Medium |
| `container_memory_usage_bytes` | Memory usage of a container in bytes. High values relative to limits may lead to OOM kills. For example, if a container's memory usage approaches its limit, it may be terminated by the OOM killer. | `container_memory_usage_bytes / container_memory_working_set_bytes > 0.9` | Medium |
| `container_memory_working_set_bytes` | Amount of memory actively used by the container (not easily reclaimed). This is the “real” memory footprint of a VM or container. High values indicate a VM is using a lot of memory, which can affect node stability and VM performance. | `container_memory_working_set_bytes{namespace="example-ns"}` | High |
| `container_network_receive_bytes_total` | Total bytes received over the network by a container. High network receive rates may indicate busy or heavily accessed VMs (e.g., web servers, databases). Sudden spikes could signal attacks or misconfigured workloads. | `rate(container_network_receive_bytes_total[5m]) > $(network_in_threshold)` | Medium |
| `container_network_receive_packets_total` | Total number of network packets received by a container. High packet counts can indicate network-intensive VMs or possible flooding/attack scenarios. | `rate(container_network_receive_packets_total[5m]) > $(packet_in_threshold)` | Medium |
| `container_network_transmit_bytes_total` | Total bytes sent over the network by a container. High transmit rates may indicate VMs exporting lots of data (e.g., file servers, backups). Sudden increases may signal data exfiltration or backup jobs.| `rate(container_network_transmit_bytes_total[5m]) > $(network_out_threshold)` | Medium |
| `container_oom_events_total` | Total number of OOM (Out of Memory) events for a container. Indicates memory-related container terminations. For example, if this increases, containers are being killed due to memory pressure. | `increase(container_oom_events_total[15m]) > 0` | High |
| `container_runtime_crio_containers_oom_count_total` | Total number of containers killed due to Out Of Memory (OOM) by CRI-O. If VMs are being OOM killed, they are running out of memory and crashing, leading to downtime and data loss. | `increase(container_runtime_crio_containers_oom_count_total[15m]) > 0` | High |
| `container_runtime_crio_containers_oom_total` | Total number of OOM events for containers managed by CRI-O. Indicates memory-related container terminations. For example, if this increases, CRI-O containers are being killed due to memory pressure. | `increase(container_runtime_crio_containers_oom_total[15m]) > 0` | High |
| `container_runtime_crio_image_pulls_failure_total` | Total number of failed image pulls by CRI-O. Failed image pulls can prevent VMs or workloads from starting, leading to service outages or failed VM launches. | `increase(container_runtime_crio_image_pulls_failure_total[15m]) > 0` | High |
| `container_runtime_crio_image_pulls_success_total` | Total number of successful image pulls by CRI-O. Shows that images are being pulled and workloads (including VMs) can start as expected. Sudden drops may indicate registry or network issues. | `increase(container_runtime_crio_image_pulls_success_total[15m])` | Low |
| `container_runtime_crio_operations_latency_seconds_total` | Precomputed quantiles (e.g., 0.5, 0.9, 0.99) of the time taken for CRI-O container runtime operations such as creating, attaching, or checking the status of containers. High values for operations like `CreateContainer` mean that starting VM pods is slow, which can delay VM launches, restarts, or migrations in OpenShift Virtualization. Use the quantile label to focus on the slowest operations (e.g., `quantile="0.99"` for the slowest 1% of requests). | `container_runtime_crio_operations_latency_seconds_total{operation="CreateContainer", quantile="0.99"} > $(crio_latency_threshold)` | High |
| `container_runtime_crio_operations_total` | Total number of operations performed by the CRI-O container runtime (e.g., starting, stopping, pulling images). High or spiking values may indicate many container (including VM pod) actions, which can signal cluster churn or issues with VM lifecycle operations. | `rate(container_runtime_crio_operations_total[5m]) > $(crio_ops_threshold)` | Low |
| `container_runtime_crio_processes_defunct` | Number of defunct (zombie) processes observed by CRI-O. Defunct processes can indicate problems with container or VM cleanup, potentially leading to resource leaks or node instability. | `container_runtime_crio_processes_defunct > $(defunct_proc_threshold)` | Medium |
| `container_spec_memory_limit_bytes` | The memory limit (in bytes) set for each container. Shows the configured memory cap for each VM or container. There are a lot of objects in OpenShift that have a 0 value. Too many will cause the web page to crash. You may wish to hide this by querying `container_spec_memory_limit_bytes >0`. If too low, VMs may be OOM killed; if too high, they may starve other workloads. | `container_spec_memory_limit_bytes > $(memory_limit_bytes)` | Low |
| `container_start_time_seconds` | Timestamp when each container started (in seconds since epoch). Useful for tracking VM/Pod uptime and troubleshooting restarts or unexpected terminations. The example query shows the days a container has been up. Remove the `/24` to show hours instead of days. | `(time() - container_start_time_seconds) /60 /60 /24` | Low |
| `container_threads` | Current number of threads used by each container. High thread counts in a VM or container can indicate heavy workload or runaway processes, possibly leading to resource exhaustion. The threshold will vary across environments based on numerous factors. | `container_threads > $(threads_threshold)` | Low |
| `container_threads_max` | Maximum number of threads allowed for each container. Shows the thread cap for containers. If usage approaches this value, VMs may be unable to spawn new threads, causing failures. | `container_threads / container_threads_max > 0.85` | Medium |
| `controller_runtime_active_workers` | Number of active worker threads in a controller High values may indicate controllers are busy reconciling many changes, which can be normal during scaling or upgrades but may also signal controller bottlenecks. | `controller_runtime_active_workers > $(active_workers_threshold)` | Low |
| `controller_runtime_reconcile_errors_total` | Total number of errors encountered during controller reconciliation loops. High error counts mean controllers (such as for VMs) are failing to process objects, which can cause VM creation, updates, or deletions to fail or stall. | `increase(controller_runtime_reconcile_errors_total[15m]) > $(reconcile_errors_threshold)` | High |
| `controller_runtime_reconcile_panics_total` | Total number of panics during controller reconciliation. Indicates serious issues with Kubernetes controllers. For example, any increase in this metric indicates controllers are experiencing critical failures. | `increase(controller_runtime_reconcile_panics_total[15m]) > 0` | Critical |
| `controller_runtime_reconcile_time_seconds_bucket` | Histogram of time spent reconciling resources in controllers (in seconds). High values mean controllers are taking longer to process VM or other resource changes, which can delay VM operations. | `histogram_quantile(0.95, sum(rate(controller_runtime_reconcile_time_seconds_bucket[5m])) by (le)) > $(reconcile_time_threshold)` | Medium |
| `controller_runtime_reconcile_time_seconds_per_instance_sum` | Total reconciliation time (in seconds) per controller instance. High values indicate some controllers are spending a lot of time processing changes, which may signal performance issues or heavy workloads. | `controller_runtime_reconcile_time_seconds_per_instance_sum > $(reconcile_time_sum_threshold)` | Medium |
| `controller_runtime_reconcile_time_seconds_sum` | Total reconciliation time (in seconds) across all controller instances. Shows the overall time controllers are spending on reconciliation, helping spot cluster-wide controller slowdowns. | `rate(controller_runtime_reconcile_time_seconds_sum[5m]) > $(reconcile_time_sum_rate_threshold)` | Medium |
| `controller_runtime_terminal_reconcile_errors_total` | Total number of terminal errors during controller reconciliation. Impact: Indicates controller failures. If this increases for a specific controller, resources managed by that controller may be in a bad state. | `increase(controller_runtime_terminal_reconcile_errors_total[15m]) > 0` | High |
| `coredns_cache_misses_total` | Total number of DNS queries not found in the CoreDNS cache (cache misses). High values mean DNS queries (including for VMs) are not being served from cache, possibly leading to slower DNS resolution and increased load on upstream servers. | `rate(coredns_cache_misses_total[5m]) > $(dns_miss_threshold)` | Low |
| `coredns_cache_requests_total` | Total number of DNS queries handled by the CoreDNS cache. Shows overall DNS cache usage. Comparing this with cache misses helps you understand DNS cache effectiveness for VM and cluster workloads.| `rate(coredns_cache_requests_total[5m]) > $(dns_cache_req_threshold)` | Low |
| `coredns_dns_request_duration_seconds_bucket` | Histogram showing how long it takes CoreDNS to answer DNS requests, grouped into time buckets. If DNS requests take too long, VMs and applications in your cluster will experience delays when trying to connect to services by name, which can slow down or disrupt VM operations and user experience. | `histogram_quantile(0.99, sum(rate(coredns_dns_request_duration_seconds_bucket[5m])) by (le)) > $(dns_latency_threshold)` | High |
| `coredns_dns_request_duration_seconds_count` | Total count of DNS requests processed. High counts may indicate heavy DNS traffic that could overwhelm CoreDNS, affecting VM connectivity. | `rate(coredns_dns_request_duration_seconds_count[5m]) > $(request_rate_threshold)` | Low |
| `coredns_dns_request_duration_seconds_sum` | Total time spent processing all DNS requests. Combined with count, shows average latency. High values mean overall slow DNS resolution for VMs. | `rate(coredns_dns_request_duration_seconds_sum[5m]) / rate(coredns_dns_request_duration_seconds_count[5m]) > $(avg_latency_threshold)` | Medium |
| `coredns_dns_request_size_bytes_count` | Counts the number of DNS requests received by CoreDNS, grouped by the size of each request (in bytes). Impact: This metric helps you understand the distribution and frequency of DNS request sizes in your cluster. Most DNS requests are small, but a sudden increase in large request sizes could indicate unusual or potentially malicious activity, misconfigured clients, or applications generating abnormally large queries. Large or unexpected DNS requests can put extra load on CoreDNS, potentially slowing down name resolution for VMs and applications, and may signal issues that could impact VM network performance or reliability | `rate(coredns_dns_request_size_bytes_count[5m]) > $(request_size_count_threshold)` | Low |
| `coredns_dns_response_size_bytes_count` | Number of DNS responses by size. Large responses might indicate misconfigured DNS records affecting VM services. | `rate(coredns_dns_response_size_bytes_count[5m]) > $(response_size_count_threshold)` | Low |
| `coredns_dns_responses_total` | Total DNS responses by response code. High error rates (e.g., SERVFAIL) can break VM-to-service communication. In CoreDNS there are often a high number of NXDOMAIN errors due to the way it is configured with `ndots` which cause a domain to be looked up with multiple permutations that ultimately do not exist. | `sum(rate(coredns_dns_responses_total{rcode!="NOERROR"}[5m])) by (rcode) > $(error_rate_threshold)` | Medium to High |
| `coredns_forward_healthcheck_broken_total` | Total number of failed CoreDNS forward health checks. Increasing values indicate DNS forwarding issues. For example, if this increases rapidly, DNS resolution to external domains may fail. | `rate(coredns_forward_healthcheck_broken_total[5m]) > 0` | High |
| `coredns_forward_max_concurrent_rejects_total` | Requests rejected due to upstream limits. May cause DNS failures for VMs if upstream servers are overloaded. | `increase(coredns_forward_max_concurrent_rejects_total[15m]) > 0` | Medium |
| `coredns_health_request_duration_seconds_bucket` | Histogram showing how long it takes CoreDNS to respond to health check requests, grouped into time buckets. This metric helps you see if CoreDNS is responding quickly to health checks. If health check responses become slow, Kubernetes may not detect a failing CoreDNS pod in time, which can delay repair or restart of DNS services. | `histogram_quantile(0.95, rate(coredns_health_request_duration_seconds_bucket[5m])) > $(health_check_threshold)` | Medium |
| `coredns_health_request_failures_total` | Total number of CoreDNS health check failures. Indicates DNS service health issues. For example, if this increases, the CoreDNS service may be experiencing problems affecting DNS resolution. | `rate(coredns_health_request_failures_total[5m]) > 0` | High |
| `coredns_kubernetes_dns_programming_duration_seconds_bucket` | Histogram showing how long it takes CoreDNS to update its DNS records after a change in Kubernetes (like when a new service or pod is created or deleted). | `histogram_quantile(0.95, rate(coredns_kubernetes_dns_programming_duration_seconds_bucket[5m])) > $(k8s_update_threshold)` | Medium |
| `coredns_kubernetes_rest_client_requests_total` | Total number of HTTP requests CoreDNS makes to the Kubernetes API server, labeled by HTTP method, response code, and API server host. CoreDNS relies on the Kubernetes API to stay up-to-date with changes to services, endpoints, and pods. If CoreDNS encounters a high rate of errors (especially 5xx status codes), it may not be able to update its DNS records correctly. | `rate(coredns_kubernetes_rest_client_requests_total{code=~"5.."}[5m]) > 0` | High |
| `coredns_panics_total` | Total number of panics in CoreDNS. Indicates serious issues with DNS service. For example, any increase in this metric indicates CoreDNS is experiencing critical failures. | `increase(coredns_panics_total[15m]) > 0` | Critical |
| `coredns_proxy_request_duration_seconds_count` | Total count of DNS proxy requests handled by CoreDNS. High counts may indicate heavy DNS proxy usage, which could affect DNS performance for VMs and cluster services. | `rate(coredns_proxy_request_duration_seconds_count[5m]) > $(proxy_req_threshold)` | Low |
| `coredns_reload_failed_total` | Total number of times CoreDNS failed to reload its configuration. Reload failures can leave CoreDNS running with outdated or incorrect DNS settings, causing DNS resolution issues for VMs and cluster workloads. | `increase(coredns_reload_failed_total[15m]) > 0` | High |
| `counter_memberlist_msg_dead` | Counts the number of "dead" messages detected by the cluster memberlist protocol, which is used for node discovery and cluster health. When a node in the cluster stops responding or is unreachable, other nodes will mark it as "dead" and send a "dead" message. A high or increasing count of dead messages may indicate node communication problems, network instability, or nodes crashing or being removed. | `increase(counter_memberlist_msg_dead[15m]) > $(dead_msg_threshold)` | Medium |
| `counter_memberlist_tcp_accept` | Total number of TCP connections accepted by the memberlist protocol. Tracks cluster node communication. Sudden drops or spikes may signal network issues affecting VM and pod placement. | `rate(counter_memberlist_tcp_accept[5m]) > $(tcp_accept_threshold)` | Low |
| `cronjob_controller_job_creation_skew_duration_seconds_bucket` | Histogram that measures the delay (in seconds) between when a CronJob is scheduled to run and when the job is actually created. If jobs (such as VM backups or scheduled maintenance) are created late, automation and SLAs can be disrupted. The histogram allows you to see not just the average delay, but also how bad the worst delays are (e.g., the slowest 5%). | `histogram_quantile(0.95, sum(rate(cronjob_controller_job_creation_skew_duration_seconds_bucket[5m])) by (le)) > $(job_skew_threshold)` | Medium |
| `cronjob_controller_job_creation_skew_duration_seconds_sum` | Total time (in seconds) of all job creation delays for cronjobs. High totals indicate persistent delays in job scheduling, which may impact regular VM or cluster maintenance tasks. | `rate(cronjob_controller_job_creation_skew_duration_seconds_sum[5m]) > $(job_skew_sum_threshold)` | Medium |
| `csi_operations_seconds_bucket` | Histogram of how long CSI (Container Storage Interface) storage operations take, grouped by duration buckets (seconds). This metric helps you understand if storage actions—like VM disk attach, detach, creation, or migration—are fast or slow. If storage operations are slow, VMs may be delayed or experience performance problems. | `histogram_quantile(0.95, sum(rate(csi_operations_seconds_bucket[5m])) by (le)) > $(csi_latency_threshold)` | High |
| `csi_operations_seconds_sum` | The total cumulative time (in seconds) spent on all CSI (Container Storage Interface) storage operations, summed across all operations and time. A high or rapidly increasing value means your storage subsystem is spending a lot of time processing requests, which can signal slow disk attach/detach, volume creation, or migration for VMs. | `rate(csi_operations_seconds_sum[5m]) > $(csi_op_sum_threshold)` | High |
| `csv_succeeded` | Indicates whether an Operator Lifecycle Manager (OLM) ClusterServiceVersion (CSV) has succeeded. A value of 1 means the operator is installed and running. | `csv_succeeded == 0` | High |
| `default_storage_class_count` | Number of default storage classes in the cluster. There should only be one default. More than one can cause unpredictable VM disk provisioning behavior. | `default_storage_class_count > 1` | Medium |
| `endpoint_slice_controller_changes` | Counts the number of changes (additions, removals, or updates) processed by the EndpointSlice controller in Kubernetes. Each time a Service’s set of endpoints (the Pods backing the Service) changes, the EndpointSlice controller must update the EndpointSlices. High rates of changes mean there are frequent updates to which Pods are available for Services—this can be due to rapid scaling, frequent pod restarts, or rolling updates. | `rate(endpoint_slice_controller_changes[5m]) > $(slice_change_threshold)` | Medium |
| `endpoint_slice_controller_endpoints_added_per_sync_count` | Counts how many endpoints (Pods or VMs) are added to EndpointSlices during each sync operation by the controller. Large numbers per sync mean many new Pods or VMs are being added at once—this can happen during rapid scaling, mass VM creation, or after a network disruption. | `rate(endpoint_slice_controller_endpoints_added_per_sync_count[5m]) > $(endpoints_added_threshold)` | Low |
| `endpoint_slice_controller_endpoints_desired` | Number of endpoints desired by the EndpointSlice controller. Indicates load on the EndpointSlice controller. For example, if this grows very large, it may indicate a large number of services or endpoints that could affect controller performance. | `endpoint_slice_controller_endpoints_desired > 10000` | Medium |
| `endpoint_slice_controller_endpointslices_changed_per_sync_count` | Counts how many EndpointSlices are changed (created, updated, or deleted) during each sync operation. High values mean the network topology is changing frequently, which can be caused by rapid scaling, rolling updates, or unstable workloads. Frequent changes can increase network traffic, slow down DNS and service discovery, and cause temporary connectivity issues for VMs and applications. | `rate(endpoint_slice_controller_endpointslices_changed_per_sync_count[5m]) > $(slices_changed_threshold)` | Medium |
| `endpoint_slice_controller_syncs` | Counts the total number of sync operations performed by the EndpointSlice controller over time. High sync frequency means the controller is working hard to keep up with changes in the cluster (such as Pods/VMs starting, stopping, or moving). If syncs are happening very frequently, it may indicate instability, a rapid deployment, or a misbehaving workload. | `rate(endpoint_slice_controller_syncs[5m]) > $(syncs_threshold)` | Low |
| `endpoint_slice_mirroring_controller_addresses_skipped_per_sync_sum` | Total number of endpoint addresses skipped during sync operations by the EndpointSlice mirroring controller. This controller maintains backward compatibility by creating legacy Endpoint objects from EndpointSlices. Skipped addresses may indicate configuration issues or resource limits that could affect VM service discovery for older applications that rely on legacy Endpoints. | `rate(endpoint_slice_mirroring_controller_addresses_skipped_per_sync_sum[5m]) > $(skipped_addresses_threshold)` | Low |
| `endpoint_slice_mirroring_controller_changes` | Total number of changes processed by the EndpointSlice mirroring controller.  High change rates may indicate frequent VM or service updates that stress the controller | `rate(endpoint_slice_mirroring_controller_changes[5m]) > $(mirroring_changes_threshold)` | Low |
| `endpoint_slice_mirroring_controller_endpoints_added_per_sync_sum` | Total number of endpoints added during all sync operations by the mirroring controller. Shows how many new VM or pod endpoints are being added to legacy Endpoint objects. High values may indicate rapid scaling or frequent VM creation that could stress the controller and delay service discovery for legacy applications. | `rate(endpoint_slice_mirroring_controller_endpoints_added_per_sync_sum[5m]) > $(endpoints_added_sum_threshold)` | Low |
| `endpoint_slice_mirroring_controller_endpoints_removed_per_sync_bucket` | Histogram of how many endpoints are removed per sync operation by the mirroring controller. Large numbers of removals per sync may indicate mass VM shutdowns, node failures, or scaling events. This can help identify when significant changes in VM availability are happening that might affect service discovery. | `histogram_quantile(0.95, sum(rate(endpoint_slice_mirroring_controller_endpoints_removed_per_sync_bucket[5m])) by (le)) > $(endpoints_removed_threshold)` | Low |
| `endpoint_slice_mirroring_controller_endpoints_sync_duration_bucket` | Histogram of how long it takes the mirroring controller to complete sync operations (in seconds). Slow sync operations can delay updates to legacy Endpoint objects, causing delays in service discovery for applications that haven't migrated to EndpointSlices. This can affect VM connectivity and application reliability. | `histogram_quantile(0.95, sum(rate(endpoint_slice_mirroring_controller_endpoints_sync_duration_bucket[5m])) by (le)) > $(sync_duration_threshold)` | Medium |
| `endpoint_slice_mirroring_controller_endpoints_updated_per_sync_bucket` | Histogram of how many endpoints are updated per sync operation by the mirroring controller. Frequent updates can indicate changing VM or pod states (like readiness changes). High update rates may stress the controller and delay service discovery updates for legacy applications. | `histogram_quantile(0.95, sum(rate(endpoint_slice_mirroring_controller_endpoints_updated_per_sync_bucket[5m])) by (le)) > $(endpoints_updated_threshold)` | Low |
| `ephemeral_volume_controller_create_failures_total` | Total number of failures when creating ephemeral volumes. Ephemeral volumes are used for temporary VM storage needs. Creation failures can prevent VMs from starting properly or cause them to fail during runtime, leading to service outages and data loss for applications using temporary storage. | `increase(ephemeral_volume_controller_create_failures_total[15m]) > 0` | Medium |
| `etcd_cluster_version` | Information about the etcd cluster version running in the cluster. etcd stores all Kubernetes state, including VM configurations. Version information helps with troubleshooting, ensuring compatibility, and planning upgrades. Mismatched versions can cause cluster instability affecting all VM operations. | `etcd_cluster_version` | Low |
| `etcd_debugging_disk_backend_commit_rebalance_duration_seconds_bucket` | Histogram of time spent rebalancing data during etcd disk commits (in seconds). Slow rebalance operations can indicate disk performance issues that may cause API server slowdowns, delayed VM operations, or cluster instability. | `histogram_quantile(0.95, sum(rate(etcd_debugging_disk_backend_commit_rebalance_duration_seconds_bucket[5m])) by (le)) > $(etcd_rebalance_threshold)` | High |
| `etcd_debugging_disk_backend_commit_rebalance_duration_seconds_sum` | Total time spent on etcd disk rebalance operations during commits. High values indicate etcd is spending significant time rebalancing data, which can slow down all cluster operations including VM creation, updates, and deletions. | `rate(etcd_debugging_disk_backend_commit_rebalance_duration_seconds_sum[5m]) > $(etcd_rebalance_sum_threshold)` | High |
| `etcd_debugging_disk_backend_commit_spill_duration_seconds_bucket` | Histogram of time spent on etcd disk spill operations during commits (in seconds). Spill operations occur when etcd's memory buffers are full and data must be written to disk. Slow spills can indicate storage bottlenecks that affect all VM and cluster operations. | `histogram_quantile(0.95, sum(rate(etcd_debugging_disk_backend_commit_spill_duration_seconds_bucket[5m])) by (le)) > $(etcd_spill_threshold)` | High |
| `etcd_debugging_disk_backend_commit_spill_duration_seconds_sum` | Total time spent on etcd disk spill operations during commits. High values indicate etcd is frequently spilling data to disk, suggesting memory pressure or slow storage that can degrade all cluster operations including VM management. | `rate(etcd_debugging_disk_backend_commit_spill_duration_seconds_sum[5m]) > $(etcd_spill_sum_threshold)` | High |
| `etcd_debugging_disk_backend_commit_write_duration_seconds_count` | Total number of disk write operations during etcd commits. High write counts may indicate frequent cluster state changes or storage inefficiency. Excessive writes can wear out SSDs and slow down VM operations that depend on etcd performance. | `rate(etcd_debugging_disk_backend_commit_write_duration_seconds_count[5m]) > $(etcd_write_count_threshold)` | Medium |
| `etcd_debugging_disk_backend_commit_write_duration_seconds_sum` | Total time spent writing to disk during etcd commits. High values indicate slow disk writes, which can cause API server timeouts and delays in all VM operations. Fast, reliable etcd writes are critical for responsive VM management. | `rate(etcd_debugging_disk_backend_commit_write_duration_seconds_sum[5m]) > $(etcd_write_sum_threshold)` | High |
| `etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds_sum` | Measures how long etcd pauses all operations during database compaction. In OpenShift Virtualization, long pauses can delay VM operations like starting, stopping, or migrating VMs because the cluster state becomes temporarily unavailable. If compaction takes 500ms, VM status updates are frozen during this time. | `rate(etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds_sum[5m]) / rate(etcd_debugging_mvcc_db_compaction_pause_duration_milliseconds_count[5m])` | Medium |
| `etcd_debugging_mvcc_db_compaction_total_duration_milliseconds_sum` | Tracks the complete time etcd spends compacting its database to reclaim space from deleted keys. For virtualization workloads, frequent VM lifecycle changes create many deleted keys. Long compaction times indicate storage performance issues | `rate(etcd_debugging_mvcc_db_compaction_total_duration_milliseconds_sum[5m]) > $(total_compaction_threshold)` | Medium |
| `etcd_debugging_mvcc_events_total` | Counts all multi-version concurrency control events in etcd. Each VM state change (created, running, migrating, stopped) generates MVCC events. High event rates indicate heavy VM activity or potential issues with VM controllers continuously updating VM status. Example: 1000 events/minute might indicate VMs constantly restarting. | `rate(etcd_debugging_mvcc_events_total[5m]) > $(mvcc_events_threshold)` | Medium |
| `etcd_debugging_mvcc_index_compaction_pause_duration_milliseconds_bucket` | Provides detailed timing distribution of etcd index compaction pauses. The index helps etcd quickly find VM definitions and status. Long index compaction pauses can make VM queries slow, affecting the web console's VM list or CLI commands like `oc get vms`. Example: If 95th percentile exceeds 100ms, VM operations feel sluggish. | `histogram_quantile(0.95, etcd_debugging_mvcc_index_compaction_pause_duration_milliseconds_bucket) > $(index_pause_threshold)` | Medium |
| `etcd_debugging_mvcc_pending_events_total` | Shows events waiting to be processed by etcd's MVCC system. In virtualization environments, pending events can indicate that VM state changes are backing up, potentially causing delays in VM status updates, scheduling decisions, or live migration coordination. | `etcd_debugging_mvcc_pending_events_total > $(pending_events_threshold)` | Medium |
| `etcd_debugging_mvcc_slow_watcher_total` | Counts watchers that are slow to process etcd events. VM controllers and operators watch for VM state changes. Slow watchers can cause VM operations to appear delayed or inconsistent in the UI, and may indicate controller performance issues or network problems between cluster components. Example: Slow VM controller watchers delay VM startup notifications. | `etcd_debugging_mvcc_slow_watcher_total > $(slow_watchers_threshold)` | Medium |
| `etcd_debugging_mvcc_watcher_total` | Watchers are internal etcd constructs that monitor changes to keys or key ranges. In OpenShift Virtualization, components like virt-controller, virt-handler, and operators rely on watchers to receive real-time updates about VM objects and cluster state. A sudden increase may indicate resource leaks or runaway processes, potentially exhausting etcd resources and degrading cluster performance. A sudden drop may mean critical components are disconnected or failing. | `abs(increase(etcd_debugging_mvcc_watcher_total[5m]))` | Medium |
| `etcd_debugging_raft_terms_total` | Total number of Raft terms seen by this etcd member. Rapid increases indicate frequent leader elections and potential instability. If this increases rapidly over a short period, it indicates cluster instability with frequent leadership changes. | `delta(etcd_debugging_raft_terms_total[15m]) > 5` | High |
| `etcd_debugging_snap_save_total_duration_seconds_bucket` |  Histogram of complete snapshot save operations. Snapshots preserve the entire cluster state including all VM definitions, network configurations, and storage mappings. Slow snapshot saves can indicate storage issues and affect disaster recovery readiness. | `histogram_quantile(0.95, etcd_debugging_snap_save_total_duration_seconds_bucket) > $(snapshot_duration_threshold)` | Medium |
| `etcd_debugging_snap_save_total_duration_seconds_sum `|  Total time spent saving snapshots. This helps identify trends in snapshot performance over time. For virtualization workloads with frequent state changes, increasing snapshot times might indicate growing data size or degrading storage performance affecting backup reliability.| `rate(etcd_debugging_snap_save_total_duration_seconds_sum[1h]) > $(snapshot_rate_threshold)` | Medium |
|`etcd_debugging_store_writes_total`|This metric counts total write operations to the legacy etcd v2 store, which is not used in OpenShift 4.x clusters | `rate(etcd_debugging_store_writes_total[5m]) > $(store_writes_threshold)` | None |
| `etcd_disk_backend_commit_duration_seconds_bucket` | Histogram of latency distribution of commit operations called by the backend. High latencies indicate disk performance issues affecting etcd write operations. For example, if the 99th percentile exceeds 25ms, it may indicate disk performance issues affecting etcd stability. | `histogram_quantile(0.99,sum(rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) by (le))` | High |
| `etcd_disk_backend_commit_duration_seconds_sum` | Cumulative time spent committing to disk. Increasing commit times indicate degrading storage performance that will directly impact VM responsiveness. This is critical for live migration success and VM startup times. | `rate(etcd_disk_backend_commit_duration_seconds_sum[5m]) > $(commit_sum_threshold)` | Critical |
| `etcd_disk_backend_defrag_duration_seconds_sum` | Time spent defragmenting etcd's disk storage. Defragmentation reclaims space from deleted VM records and configurations. Long defrag times might indicate that etcd's storage is heavily fragmented due to frequent VM lifecycle changes. | `increase(etcd_disk_backend_defrag_duration_seconds_count[5m]) > 0` | Medium |
| `etcd_disk_backend_snapshot_duration_seconds_bucket` | Histogram of time to write snapshots to disk. This directly affects backup creation and disaster recovery capabilities for your virtualization environment. Slow snapshot writes might indicate storage issues that could compromise your ability to recover VM configurations and state.  | `histogram_quantile(0.95, etcd_disk_backend_snapshot_duration_seconds_bucket)` | Medium |
| `etcd_disk_backend_snapshot_duration_seconds_sum` | Cumulative time writing snapshots to disk. Trends in this metric help identify degrading storage performance that affects backup reliability. For virtualization workloads, reliable snapshots are essential for maintaining VM state consistency and disaster recovery readiness. Example: Increasing snapshot times over days indicates storage degradation. | `rate(etcd_disk_backend_snapshot_duration_seconds_sum[1h])` | Medium |
| `etcd_disk_wal_fsync_duration_seconds_bucket` | Histogram of latency distribution of fsync operations to the WAL (Write-Ahead Log). High values indicate disk I/O issues affecting etcd performance. If the 99th percentile exceeds 10ms, it may indicate slow disk performance leading to potential leader election issues and slow writes. | `histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])) by (le, instance))` | Critical |
| `etcd_disk_wal_fsync_duration_seconds_sum` | Sum of time spent on fsync operations to the WAL. Increasing values indicate cumulative disk I/O pressure. This metric is used with the count to calculate average fsync duration, which should be monitored for trends. | `rate(etcd_disk_wal_fsync_duration_seconds_sum[5m]) / rate(etcd_disk_wal_fsync_duration_seconds_count[5m])` | Medium |
| `etcd_disk_wal_write_bytes_total` | Total number of bytes written to the etcd Write-Ahead Log (WAL). WAL ensures all changes are safely written to disk before being committed, providing durability for VM and cluster state. High values indicate heavy write activity, which can be normal during frequent VM operations but may signal excessive churn if unexpectedly high. | `rate(etcd_disk_wal_write_bytes_total[5m])` | Medium |
| `etcd_disk_wal_write_duration_seconds_bucket` | Histogram of the duration (in seconds) for WAL write operations. Longer durations mean slower disk I/O, which can delay VM state persistence and cluster operations. If the 95th percentile write duration exceeds 0.1 seconds, etcd may be experiencing disk latency. | `histogram_quantile(0.95, rate(etcd_disk_wal_write_duration_seconds_bucket[5m])) > 0.1` | High |
| `etcd_disk_wal_write_duration_seconds_sum` | Cumulative sum of seconds spent writing to the WAL. Use in combination with `_count` to find the average write duration over a time window. If the average write time increases, it may indicate disk degradation. | `rate(etcd_disk_wal_write_duration_seconds_sum[5m]) / rate(etcd_disk_wal_write_duration_seconds_count[5m])` | High |
| `etcd_lease_object_counts_bucket` | Histogram of the number of objects attached to a single etcd lease. In etcd, a lease is a mechanism to associate a set of keys with a time-to-live (TTL). When the lease expires or is revoked, all attached keys are automatically deleted. This is used for ephemeral or short-lived resources, such as locks, leader elections, or temporary VM-related objects in OpenShift Virtualization. Large numbers of objects per lease may indicate resource leaks  or unusually high churn of ephemeral objects. | `histogram_quantile(0.95, sum(rate(etcd_lease_object_counts_bucket[5m])) by (le))` | Medium |
| `etcd_lease_object_counts_count` | Number of objects attached to leases. High values may indicate resource leaks. For example, if this grows continuously without bounds, it may indicate applications not properly releasing leases. | `etcd_lease_object_counts_count > 1000` | Medium |
| `etcd_lease_object_counts_sum` | Cumulative sum of all objects attached to leases. A growing sum may indicate accumulating temporary resources, which could exhaust etcd memory over time. Example: If this value grows steadily, investigate for resource leaks. | `increase(etcd_lease_object_counts_sum[1h])` | Medium |
| `etcd_mvcc_db_total_size_in_bytes` | Total size of the etcd database in bytes. As this approaches the quota, etcd may stop accepting writes. For example, if this exceeds 80% of the quota (typically 2-8GB), compaction and defragmentation may be needed to prevent etcd from becoming read-only. | `etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes > 0.8` | Critical |
| `etcd_mvcc_db_total_size_in_use_in_bytes` | Logical size (in bytes) of data actively used in the etcd database for a single member. This is the actual amount of data being used by etcd after compaction, not counting free space. If this is much less than the physical size, a lot of disk space is wasted due to fragmentation. When the in-use size is close to the physical size, the database is efficiently packed. This is just the "live" data, not the total file size. | `etcd_mvcc_db_total_size_in_use_in_bytes / etcd_mvcc_db_total_size_in_bytes` | Medium |
| `etcd_mvcc_hash_duration_seconds_bucket` | Histogram of durations for full etcd database hash computations. Hashing is used for consistency checks between etcd members (e.g., during leader changes or disaster recovery). Long hash durations can delay cluster operations and indicate performance issues, especially if the etcd database is large or the underlying disk is slow. This metric will return NaN if no hash operations have been performed since Prometheus started scraping metrics.| `histogram_quantile(0.95, rate(etcd_mvcc_hash_duration_seconds_bucket[5m])) ` | Low |
| `etcd_mvcc_hash_duration_seconds_sum` | 	Cumulative sum of seconds spent on full etcd database hash computations. This metric will return NaN if no hash operations have been performed since Prometheus started scraping metrics, which is normal for most OpenShift clusters. Hash operations are rare and typically only triggered during explicit consistency checks or troubleshooting. Cumulative sum of seconds spent hashing the database. Use with `_count` to calculate average hash duration. If average hash time increases, etcd consistency checks may be slowing down. | `rate(etcd_mvcc_hash_duration_seconds_sum[5m]) / rate(etcd_mvcc_hash_duration_seconds_count[5m]) ` | Low |
| `etcd_mvcc_hash_rev_duration_seconds_bucket` | Histogram of durations for revision-based hash computations. Used to verify consistency at specific revisions (e.g., after VM state changes). Long durations may indicate large or complex state changes.This metric will return NaN if no hash operations have been performed since Prometheus started scraping metrics | `histogram_quantile(0.95, rate(etcd_mvcc_hash_rev_duration_seconds_bucket[5m])) > $(hash_rev_duration_threshold)` | Low |
| `etcd_mvcc_hash_rev_duration_seconds_sum` | Cumulative sum of seconds spent on revision-based hash computations. Use with `_count` for average duration.  Spikes may indicate heavy cluster activity or performance issues. This metric will return NaN if no hash operations have been performed since Prometheus started scraping metrics| `rate(etcd_mvcc_hash_rev_duration_seconds_sum[5m]) / rate(etcd_mvcc_hash_rev_duration_seconds_count[5m])` | Low |
| `etcd_network_client_grpc_received_bytes_total` | Total bytes received by etcd from all clients over gRPC. High values indicate heavy client interaction, which is normal with many VMs or frequent control plane operations. Sudden spikes may signal a misbehaving controller or DDoS. | `rate(etcd_network_client_grpc_received_bytes_total[5m]) ` | Low |
| `etcd_network_client_grpc_sent_bytes_total` | Total bytes sent by etcd to all clients over gRPC. High values indicate heavy response traffic, which may be expected during VM migrations or mass updates. Sudden changes could indicate issues. | `rate(etcd_network_client_grpc_sent_bytes_total[5m])` | Medium |
| `etcd_network_disconnected_peers_total` | Total number of times etcd peers have disconnected. Peer disconnections can disrupt cluster consensus and impact VM state reliability. Frequent disconnects may indicate network or infrastructure issues. | `increase(etcd_network_disconnected_peers_total[1h])` | High |
| `etcd_network_peer_received_bytes_total` | Total bytes received by this etcd member from other peers. Used to monitor inter-node communication health. Drops may indicate network partitions, which can stall VM operations. | `rate(etcd_network_peer_received_bytes_total[5m])` | Medium |
| `etcd_network_peer_round_trip_time_seconds_count` | Total number of RTT samples collected. Useful for ensuring sample size in RTT analysis. Low counts may indicate connectivity issues or idle clusters. | `increase(etcd_network_peer_round_trip_time_seconds_count[1h])` | Medium |
| `etcd_network_peer_round_trip_time_seconds_sum` | Cumulative sum of RTT seconds. Use with `_count` for average peer RTT. Increasing averages may indicate network degradation. | `rate(etcd_network_peer_round_trip_time_seconds_sum[5m]) / rate(etcd_network_peer_round_trip_time_seconds_count[5m])` | High |
| `etcd_network_snapshot_receive_success` | Cumulative count of successful Raft snapshot receives by this etcd member from peers. In healthy clusters, this value is typically 1 (from initial bootstrap) and does not increase unless a peer falls far behind and needs to resync state via snapshot. A steady value of 1 is normal and expected. | `increase(etcd_network_snapshot_receive_success[1h])` | Low |
| `etcd_network_snapshot_receive_total_duration_seconds_bucket` | Histogram of durations (in seconds) for receiving Raft snapshots from peers.Raft snapshots are used when an etcd member falls far behind and must catch up by receiving a full state snapshot from another peer. In OpenShift Virtualization, this is rare and usually only happens after a network partition or severe lag. Long durations may indicate slow disk, network issues, or large cluster state, which can delay recovery and impact VM and cluster operations. If no snapshots are received, this metric may be absent or NaN, which is normal in healthy clusters. | `histogram_quantile(0.95, rate(etcd_network_snapshot_receive_total_duration_seconds_bucket[5m])) > $(snapshot_receive_duration_threshold)` | Medium |
| `etcd_network_snapshot_send_success` | Counter for the total number of successful Raft snapshot sends from this etcd member to peers. This number increases only when a peer falls so far behind that it must receive a full snapshot to catch up. In healthy OpenShift Virtualization clusters, this value is usually 0 or 1 and rarely increases. A sudden increase may indicate network instability or a peer consistently lagging.  If no snapshots have ever been sent, this metric may be absent or NaN, which is normal. | `increase(etcd_network_snapshot_send_success[1h]) > $(snapshot_send_threshold)` | Low |
| `etcd_request_duration_seconds_bucket` | Histogram of request durations (in seconds) for all etcd operations. This metric measures how long etcd takes to process client requests. Monitoring the 95th percentile helps detect performance degradation.  If no requests are processed in the window, result may be NaN. There are a large number of objects in this query, you may want to filter by namespace. | `histogram_quantile(0.95, rate(etcd_request_duration_seconds_bucket{namespace="default"}[5m])) > $(request_duration_threshold)` | Medium |
| `etcd_requests_total` | Total number of client requests received by etcd. This counter tracks all API requests to etcd. A sudden spike may indicate a misbehaving controller, DDoS, or runaway automation. In OpenShift Virtualization, sustained high rates may stress etcd and slow down VM operations. If no requests are received, metric may be NaN, but this is rare in active clusters. There are a large number of datapoints in this metric. You may want to filter by the namespace. | `rate(etcd_requests_total{namespace!="default"}[5m]) > $(requests_rate_threshold)` | Medium |
| `etcd_server_apply_duration_seconds_bucket` | Histogram of durations for applying committed proposals to the etcd state machine. After a change is committed in Raft, it must be applied to the database. High durations can signal slow disk, CPU bottlenecks, or complex transactions, impacting VM state changes and cluster responsiveness. If no proposals are applied, may be NaN. | `histogram_quantile(0.95, rate(etcd_server_apply_duration_seconds_bucket[5m])) > 0.5` | High |
| `etcd_server_apply_duration_seconds_sum` | Cumulative sum of seconds spent applying proposals. Use with the corresponding `_count` metric to get the average apply duration. Increasing averages may indicate etcd is struggling to keep up with workload, which can delay VM operations. | `rate(etcd_server_apply_duration_seconds_sum[5m]) / rate(etcd_server_apply_duration_seconds_count[5m]) > $(avg_apply_duration)` | High |
| `etcd_server_client_requests_total` | Total number of client requests received by type. Rate changes indicate client load patterns. For example, monitoring the rate of write requests can help identify periods of high load on the etcd cluster. | `sum(rate(etcd_server_client_requests_total{type="write"}[5m]))` | Medium |
| `etcd_server_has_leader` | Whether this etcd member has a leader (1) or not (0). Values of 0 indicate split-brain or isolation. If this is 0 for any etcd member, that member cannot process writes and may be network isolated. <br><br>**Metric values:**<br> - `1` ETCD member has a leader<br>- `0` ETCD member is reporting no leader known| `etcd_server_has_leader == 0` | Critical |
| `etcd_server_health_failures` | Total number of health check failures. Tracks how many times etcd failed a health check. Frequent failures can indicate instability, network issues, or resource exhaustion. In OpenShift Virtualization, this can lead to API unavailability and VM disruptions. | `increase(etcd_server_health_failures[1h]) > $(health_failure_threshold)` | Critical |
| `etcd_server_heartbeat_send_failures_total` | Total number of failed Raft leader heartbeats sent. Heartbeats are used to maintain cluster leadership and consensus. Failures can be caused by overload, slow disk, or network issues. Frequent failures can cause leader elections and cluster instability, impacting VM management. If no failures, metric may be zero or absent. | `increase(etcd_server_heartbeat_send_failures_total[1h]) > $(heartbeat_failure_threshold)` | Critical |
| `etcd_server_is_leader` | Indicates if this etcd member is the current leader. Leadership is required for serving write requests. In OpenShift Virtualization, if no leader exists, the cluster cannot process changes, impacting all VM and cluster operations. <br><br>**Metric values:**<br> - `1` This node considers itself to be the ETCD leader<br>- `0` This node does not think it is the ETCD leader.| `sum(etcd_server_is_leader{namespace="openshift-etcd"}) !=1` | Critical |
| `etcd_server_leader_changes_seen_total` | Total number of leader changes observed by this member. Leader changes are normal during upgrades or brief outages but frequent changes indicate instability, which can disrupt VM operations and cause API outages.| `sum(increase(etcd_server_leader_changes_seen_total[1h])) > 3` | High |
| `etcd_server_proposals_applied_total` | Total number of Raft proposals applied to the etcd state machine. In etcd, a proposal is a request to change the cluster state (such as creating, updating, or deleting a VM or resource). Once a proposal is agreed upon via the Raft consensus algorithm, it is "applied" to the etcd database. This metric counts how many such changes have been successfully applied. This metric should steadily increase as the cluster operates. If it stops increasing (i.e., the rate drops to zero), etcd may be unhealthy or stalled. If the cluster is idle or has just started, this metric may be zero or absent, resulting in a NaN for rate queries. | `rate(etcd_server_proposals_applied_total[5m]) < $(min_applied_rate)` | High |
| `etcd_server_proposals_failed_total` | Total number of failed proposals (consensus operations). Increasing values indicate consensus issues. If this increases, it indicates problems with the Raft consensus algorithm, potentially due to network issues or overload. | `increase(etcd_server_proposals_failed_total[15m]) > 0` | High |
| `etcd_server_quota_backend_bytes` | Maximum backend size in bytes etcd can use before triggering an alarm. Defines the limit for etcd database size. This is typically set to 2-8GB. If the database size approaches this value, maintenance is required. | `etcd_server_quota_backend_bytes` | Low |
| `etcd_server_slow_apply_total` | Total number of slow apply operations. Counts how often applying a proposal took longer than a threshold. Frequent slow applies can signal disk or CPU bottlenecks, impacting VM and cluster responsiveness. If no slow applies, may be zero or absent. | `increase(etcd_server_slow_apply_total[1h]) > $(slow_apply_threshold)` | Medium |
| `etcd_server_snapshot_apply_in_progress_total` | Number of snapshot apply operations currently in progress. Applying a snapshot is a blocking operation to restore state. Should be zero during normal operation. Nonzero values indicate recovery or major re-sync, which may temporarily impact cluster responsiveness. If no snapshot applies in progress, may be zero or absent. | `etcd_server_snapshot_apply_in_progress_total > 0` | Medium |
| `etcd_snap_db_save_total_duration_seconds_bucket` | Histogram of durations for saving the etcd database snapshot to disk. Snapshots are used for backup and disaster recovery. Long durations may indicate disk issues or large cluster state, impacting backup reliability. If no snapshots saved, may be absent or NaN. | `histogram_quantile(0.95, rate(etcd_snap_db_save_total_duration_seconds_bucket[5m])) > $(snapshot_save_duration_threshold)` | Medium |
| `etcd_snap_fsync_duration_seconds_bucket` | Histogram of durations for fsync (disk flush) operations during snapshot save. Fsync ensures data is safely written to disk. Long fsync durations may indicate disk latency, risking data durability and backup reliability. If no fsyncs, may be absent or NaN. | `histogram_quantile(0.95, rate(etcd_snap_fsync_duration_seconds_bucket[5m])) > $(fsync_duration_threshold)` | Medium |
| `etcd_snap_fsync_duration_seconds_sum` | Cumulative sum of seconds spent on fsync during snapshot saves. Use with corresponding `_count` for average fsync duration. Increasing averages may indicate deteriorating disk performance. If no fsyncs, may be absent or NaN. | `rate(etcd_snap_fsync_duration_seconds_sum[5m]) / rate(etcd_snap_fsync_duration_seconds_count[5m]) > $(avg_fsync_duration)` | Medium |
| `federate_requests_failed_total` | Total number of failed federation requests. Federation allows one Prometheus server to scrape metrics from another Prometheus server. This is commonly used in OpenShift to aggregate metrics from multiple clusters or to create hierarchical monitoring setups. Failed federation requests mean that monitoring data about your VMs and cluster health may not be properly aggregated. If no federation is configured or no requests have been made, this metric may be absent or NaN. | `increase(federate_requests_failed_total[1h]) > $(federation_failure_threshold)` | Medium |
| `federate_samples` | Total number of samples successfully federated. A sample is a single data point consisting of a metric name, labels, timestamp, and value. This metric indicates how much monitoring data is being successfully transferred between Prometheus instances. If federate_samples drops significantly, you may lose visibility into VM CPU, memory, and disk metrics in your centralized monitoring. If no federation is active, this metric may be absent or NaN. | `rate(federate_samples[5m]) < $(min_federation_samples_rate)` | Medium |
| `filter:apiserver_request_filter_duration_seconds_bucket:rate1m` | API server request filters are middleware components that process incoming requests before they reach the actual API handlers. They perform tasks like authentication, authorization, admission control, and request validation. Slow request filtering can delay VM operations like creation, deletion, live migration, and status updates. High filter durations indicate that the API server is struggling to process requests efficiently. If no API requests are processed in the 1-minute window, this may be NaN. | `histogram_quantile(0.95, filter:apiserver_request_filter_duration_seconds_bucket:rate1m)` | Medium |
| `filter:apiserver_request_filter_duration_seconds_bucket:rate5m` |  Same as the `rate1m` metric above but for 5 minute intervals. | `histogram_quantile(0.95, filter:apiserver_request_filter_duration_seconds_bucket:rate5m) > $(api_filter_duration_long_threshold)` | Medium |
| `flow_schema_priority_level:apiserver_flowcontrol_current_executing_requests:sum` | Current number of API requests being executed, grouped by flow schema and priority level. APF is a Kubernetes feature that manages API server request queuing and execution to prevent resource exhaustion and ensure fair access. Flow schemas classify requests, and priority levels determine their importance. If too many requests are executing simultaneously, new VM creation requests may be delayed or rejected. If no requests are currently executing, this may be zero or absent. | `flow_schema_priority_level:apiserver_flowcontrol_current_executing_requests:sum > $(max_executing_requests)` | Medium |
| `flow_schema_priority_level:apiserver_flowcontrol_current_inqueue_requests:sum` | Current number of API requests waiting in queues, grouped by flow schema and priority level. When the API server receives more requests than it can handle immediately, it places them in queues based on their priority and flow schema. Queued requests wait for their turn to be executed. High queue depths indicate API server backlog. If VM creation requests are queued for minutes, users will experience long delays when deploying new VMs. If no requests are queued, this may be zero or absent. | `flow_schema_priority_level:apiserver_flowcontrol_current_inqueue_requests:sum > $(max_queued_requests)` | Medium |
| `force_cleaned_failed_volume_operation_errors_total` | Total number of errors encountered during forced cleanup of failed volume operations. When volume operations (like mounting, unmounting, or detaching) fail and cannot be resolved normally, Kubernetes may attempt to force-clean these operations to prevent resource leaks and allow recovery. Errors during forced cleanup can leave VMs in inconsistent states, prevent VM deletion, or cause storage resource leaks. | `increase(force_cleaned_failed_volume_operation_errors_total[1h]) > $(volume_cleanup_error_threshold)` | High |
| `force_cleaned_failed_volume_operations_total` | Total number of failed volume operations that required forced cleanup.  High numbers indicate persistent storage issues that can affect VM reliability and data availability. If no volume operations have failed, this may be zero or absent. | `increase(force_cleaned_failed_volume_operations_total[1h]) > $(failed_volume_ops_threshold)` | High |
| `garbagecollector_controller_resources_sync_error_total` | Total number of errors encountered by the garbage collector controller during resource synchronization. This Kubernetes controller automatically deletes objects that are no longer needed.  Garbage collection errors can prevent proper cleanup of VM-related resources (pods, services, volumes), leading to resource leaks and potential cluster instability. Failed cleanup can also prevent VM deletion and cause storage waste.| `increase(garbagecollector_controller_resources_sync_error_total[1h]) > $(gc_sync_error_threshold)` | Low |
| `go_cpu_classes_gc_mark_dedicated_cpu_seconds_total` | Total CPU seconds spent in Go garbage collection mark phase using dedicated CPU resources. The mark phase identifies which objects are still in use. Dedicated CPU means CPU cores exclusively used for garbage collection. High garbage collection CPU usage indicates memory pressure in cluster components (API server, controllers, operators). If the virt-controller spends too much CPU on garbage collection, VM lifecycle operations may become slow. | `rate(go_cpu_classes_gc_mark_dedicated_cpu_seconds_total[5m]) > $(gc_mark_cpu_threshold)` | Low |
| `go_cpu_classes_gc_pause_cpu_seconds_total` | Total CPU seconds spent in Go garbage collection pause phases. During garbage collection, Go applications may pause execution to perform memory cleanup operations. These pauses can temporarily stop request processing. Frequent or long pauses indicate memory management issues. If no GC pauses have occurred, this may be zero or absent. | `rate(go_cpu_classes_gc_pause_cpu_seconds_total[5m]) > $(gc_pause_cpu_threshold)` | Low |
| `go_cpu_classes_gc_total_cpu_seconds_total` | Total CPU time spent on garbage collection. High values indicate excessive GC activity. For example, if this is increasing rapidly, it may indicate memory pressure in the Go runtime. | `rate(go_cpu_classes_gc_total_cpu_seconds_total[5m]) > 1` | Medium |
| `go_cpu_classes_scavenge_assist_cpu_seconds_total` | Total CPU seconds spent in Go garbage collection scavenge assist operations. This is a Go garbage collection mechanism where application goroutines help with memory cleanup when the garbage collector is overwhelmed. It indicates memory pressure. If no scavenge assist has been needed, this may be zero or absent. | `rate(go_cpu_classes_scavenge_assist_cpu_seconds_total[5m]) > $(scavenge_assist_threshold)` | Low |
| `go_cpu_classes_scavenge_background_cpu_seconds_total` |Total CPU seconds spent in Go garbage collection background scavenging. This is a low-priority garbage collection activity that runs in the background to return unused memory to the operating system without impacting application performance. If no background scavenging has occurred, this may be zero or absent. | `rate(go_cpu_classes_scavenge_background_cpu_seconds_total[5m]) > $(scavenge_background_threshold)` | Low |
| `go_cpu_classes_scavenge_total_cpu_seconds_total` | Total CPU seconds spent in all Go garbage collection scavenging activities. This combines all scavenging activities (assist, background, and other scavenging operations) into a single metric representing the total CPU overhead of memory management. This provides an overall view of garbage collection overhead in cluster components. High total scavenging indicates significant CPU resources are being diverted from VM operations to memory management, potentially impacting cluster performance. | `rate(go_cpu_classes_scavenge_total_cpu_seconds_total[5m]) > $(total_scavenge_threshold)` | Medium |
| `go_gc_cycles_total_gc_cycles_total` | Total number of completed Go garbage collection cycles. A garbage collection cycle is a complete pass through memory to identify and free unused objects. Each cycle represents one complete garbage collection operation. While some GC activity is normal, excessive cycles can indicate memory leaks or inefficient memory usage that could impact VM operation performance. | `rate(go_gc_cycles_total_gc_cycles_total[5m]) > $(gc_cycles_rate_threshold)` | Medium |
| `go_gc_duration_seconds` | Current or most recent Go garbage collection duration in seconds. This measures how long garbage collection operations take to complete. Longer durations indicate more work is needed to clean up memory or that the garbage collector is struggling with memory pressure. There are a lot of objects under this metric. You may want to filter by namespace | `go_gc_duration_seconds{namepsace="default"} > $(gc_duration_threshold)` | Medium |
| `go_gc_duration_seconds_sum` | Cumulative sum of all Go garbage collection durations in seconds. This is a counter that accumulates the total time spent in garbage collection across all cycles. Use with the corresponding count metric to calculate average GC duration over time. | `rate(go_gc_duration_seconds_sum[5m]) / rate(go_gc_duration_seconds_count[5m]) > $(avg_gc_duration_threshold)` | Medium |
| `go_gc_gomemlimit_bytes` | Memory limit for the Go runtime in bytes. Defines the ceiling for Go memory usage. This is a configuration setting that helps understand the memory constraints for the process. | `go_gc_gomemlimit_bytes` | Low |
| `go_gc_heap_allocs_bytes_total` | Total bytes allocated on the Go heap across all garbage collection cycles.  The heap is where Go programs allocate memory for objects. This metric tracks the cumulative amount of memory that has been allocated (not necessarily currently in use). High heap allocation rates indicate intensive memory usage in cluster components. While allocation itself isn't problematic, rapidly increasing allocation can indicate memory leaks or inefficient patterns that may eventually impact VM operations through resource exhaustion. | `rate(go_gc_heap_allocs_bytes_total[5m]) > $(heap_alloc_rate_threshold)` | Low |
| `go_gc_heap_goal_bytes` | Current Go garbage collection heap size goal in bytes. This is the target heap size that Go's garbage collector tries to maintain. The GC triggers more frequently as the heap approaches this goal to keep memory usage under control. If the heap goal for API server components grows significantly, it might indicate increased workload or memory leaks. Tis metric should always have a value in running Go applications, so NaN would indicate a problem with metric collection. | `go_gc_heap_goal_bytes > $(heap_goal_threshold)` | Low |
| `go_gc_heap_live_bytes` | Size of the live objects heap in bytes. High values relative to limits indicate memory pressure. For example, if this approaches the total heap size, it may indicate memory leaks or high memory pressure. | `go_gc_heap_live_bytes / go_memory_classes_total_bytes > 0.8` | Medium |
| `go_godebug_non_default_behavior_panicnil_events_total` | Total number of nil panic events. Any non-zero value indicates programming errors. For example, if this increases, it indicates nil pointer dereferences in the code, which should never happen in production. | `increase(go_godebug_non_default_behavior_panicnil_events_total[15m]) > 0` | Critical |
| `go_memory_classes_heap_free_bytes` | The number of bytes in the Go heap that are currently free (not allocated to any object). This represents memory that the Go runtime has reserved for future allocations but is not currently in use. High free heap may indicate over-provisioning or memory fragmentation, while very low values may signal memory pressure. | `go_memory_classes_heap_free_bytes < $(heap_free_min_threshold)` | Low |
| `go_memory_classes_heap_objects_bytes` | The number of bytes in the Go heap that are currently used by live objects. This shows the actual memory footprint of live data in Go processes. A steady increase may indicate a memory leak in a component, while stable values reflect healthy memory usage.| `increase(go_memory_classes_heap_objects_bytes{pod=~"apiserver-.*"}[5m])` | Low |
| `go_memory_classes_metadata_mspan_free_bytes` | mspan is an internal Go runtime structure for tracking memory spans. Free mspan bytes indicate available metadata for future allocations. If this value drops to zero, Go may need to allocate more metadata, possibly impacting performance. Generally, this is a low-level metric, but sudden changes can signal memory management issues. | `go_memory_classes_metadata_mspan_free_bytes` | Low |
| `go_memory_classes_metadata_mspan_inuse_bytes` | Size of mspan structures currently in use in bytes. Indicates memory used for internal Go runtime structures. This is primarily useful for detailed Go runtime memory analysis. | `go_memory_classes_metadata_mspan_inuse_bytes` | Low |
| `go_memory_classes_os_stacks_bytes` | Bytes of memory allocated for goroutine stacks by the Go runtime. Each goroutine (lightweight thread) has its own stack. High values indicate many active goroutines or deep call stacks, which may signal excessive concurrency or stack overflows. If this value spikes, check for runaway goroutine creation in VM controllers or handlers. | `go_memory_classes_os_stacks_bytes` | Low |
| `go_memory_classes_total_bytes` | Total size of Go memory classes in bytes. Indicates overall memory usage by the Go runtime. This helps understand the total memory footprint of the process. | `go_memory_classes_total_bytes` | Medium |
| `go_memstats_alloc_bytes` | Current number of bytes allocated and still in use by the Go application. This is the primary indicator of live heap usage. In OpenShift Virtualization, a steady increase may indicate a memory leak, while sudden drops may indicate garbage collection. | `increase(go_memstats_alloc_bytes[5m])` | Low |
| `go_memstats_alloc_bytes_total` | Cumulative total of bytes allocated by the Go application (including freed memory). This shows the total memory churn over the application's lifetime. | `rate(go_memstats_alloc_bytes_total[5m]) ` | Medium |
| `go_memstats_gc_cpu_fraction` | Fraction of recent CPU time used for garbage collection (GC), as a value between 0 and 1.  High values mean the Go runtime is spending a lot of CPU on GC, which can degrade performance of VM management components. If no GC has occurred recently, this may be NaN. | `go_memstats_gc_cpu_fraction ` | Low |
| `go_memstats_heap_alloc_bytes` | Current heap bytes allocated and in use (same as `go_memstats_alloc_bytes`). This is a snapshot of live heap memory usage. It should correlate with workload activity. | `go_memstats_heap_alloc_bytes ` | Low |
| `go_memstats_heap_inuse_bytes` | Bytes of heap memory in use by Go application (allocated to objects). Indicates the memory actually used by live objects. A high value may reflect high workload or leaks.| `go_memstats_heap_inuse_bytes` | Low |
| `go_memstats_heap_sys_bytes` | Bytes of heap memory obtained from the OS by Go. This is the total heap memory reserved from the OS, not all of which may be in use. A large gap between this and `heap_inuse_bytes` may indicate fragmentation. | `go_memstats_heap_sys_bytes - go_memstats_heap_inuse_bytes` | Low |
| `go_memstats_mallocs_total` | Total number of heap objects allocated since the application started. High allocation rates may indicate inefficient memory usage or high workload. | `rate(go_memstats_mallocs_total[5m])` | Low |
| `go_memstats_next_gc_bytes` | Target heap size (in bytes) at which the next garbage collection will occur. This value helps predict when the next GC will run. Rapid increases may indicate growing workload or memory leaks. | `go_memstats_next_gc_bytes > $(next_gc_max_threshold)` | Low |
| `go_memstats_stack_inuse_bytes` | Bytes of stack memory in use by Go goroutines. High values indicate many active or deeply nested goroutines. Excessive stack usage can lead to memory pressure. | `go_memstats_stack_inuse_bytes > $(stack_inuse_max_threshold)` | Low |
| `go_memstats_sys_bytes` | Total bytes of memory obtained from the OS by Go for all purposes (heap, stacks, metadata, etc). This is the total memory footprint of the Go process. High values may indicate leaks or high workload. | `go_memstats_sys_bytes > $(sys_bytes_max_threshold)` | Medium |
| `go_sched_goroutines_goroutines` | Current number of goroutines (lightweight threads) running in the Go application. High goroutine counts can indicate concurrency, but runaway goroutine creation may signal leaks or code issues. | `go_sched_goroutines_goroutines > $(goroutines_max_threshold)` | Medium |
| `go_sched_pauses_stopping_other_seconds_sum` | Cumulative seconds spent pausing other goroutines during scheduler stops. High values may indicate scheduler contention or performance issues in Go applications, which can delay VM operations. | `increase(go_sched_pauses_stopping_other_seconds_sum[5m])` | Low |
| `go_sched_pauses_total_gc_seconds_bucket` | Histogram of total pause times (in seconds) for all GC-related scheduler pauses. High pause times can cause application stalls, impacting VM management responsiveness. If no GC pauses have occurred, this may be NaN. | `histogram_quantile(0.95, rate(go_sched_pauses_total_gc_seconds_bucket[5m]))` | Medium |
| `go_sched_pauses_total_gc_seconds_sum` | Cumulative sum of all GC-related scheduler pause times in seconds. Increasing values indicate more time spent pausing for GC, which can degrade performance. If no GC pauses have occurred, this may be NaN. | `increase(go_sched_pauses_total_gc_seconds_sum[5m]) ` | Low |
| `go_sched_pauses_total_other_seconds_sum` | Cumulative sum of all non-GC scheduler pause times in seconds. High values may indicate scheduler or runtime contention unrelated to GC. If no such pauses have occurred, this may be NaN. | `increase(go_sched_pauses_total_other_seconds_sum[5m])` | Low |
| `group_kind:apiserver_watch_events_sizes_sum:rate1m` | Sum of the sizes of watch events per API group and kind, per second (1-minute rate).  This metric tracks the total size (in bytes) of objects sent via Kubernetes API watches, grouped by resource type, averaged per second over one minute. Large or rapidly increasing event sizes can indicate high object churn or large objects being watched (e.g., many or large VMs). This can stress API servers and clients, leading to increased memory/network usage and potential latency in VM status updates. | `group_kind:apiserver_watch_events_sizes_sum:rate1m` | Medium |
| `group_kind:apiserver_watch_events_total:rate1m` | Total number of watch events per API group and kind, per second (1-minute rate). Tracks the frequency of API watch events, which are used by controllers and operators to track resource changes. High rates may indicate excessive resource churn (e.g., many VMs being created/deleted), which can overload controllers and degrade cluster responsiveness. | `group_kind:apiserver_watch_events_total:rate1m > 200` | Medium |
| `group_sync_error` | Indicates errors during group synchronization. May affect cluster consistency in relation to permissions within the cluster. For example, if this increases, it could indicate issues with group membership updates. | `increase(group_sync_error[15m]) > 0` | Medium |
| `group_sync_successful_syncs_count` | Total number of successful group sync operations. Tracks how many times group membership (e.g., RBAC groups) have been successfully synced from an external source (like LDAP). If this value is not increasing, group membership may be stale. If no syncs have occurred, this may be NaN. | `increase(group_sync_successful_syncs_count[1h]) == 0` | Medium |
| `group_sync_unsuccessful_syncs_count` | Total number of failed group sync operations. Tracks failures when syncing group membership from external sources. Failures here can prevent users from gaining or losing access to VMs as intended, creating security or operational risks. If no syncs have failed, this may be NaN. | `increase(group_sync_unsuccessful_syncs_count[1h]) > 0` | Medium |
| `grpc_client_handling_seconds_sum` | Cumulative seconds spent handling gRPC client requests. Tracks total time spent processing gRPC client requests (for example, between cluster components or API integrations).  High or increasing values may indicate slow client processing, which could delay VM operations or monitoring. | `rate(grpc_client_handling_seconds_sum[5m]) > 2` | Medium |
| `grpc_req_panics_recovered_total` | Total number of gRPC request panics recovered. Increasing values indicate gRPC service instability. For example, if this increases rapidly, it may indicate issues with gRPC request handling. | `rate(grpc_req_panics_recovered_total[5m]) > 0` | Medium |
| `haproxy_backend_bytes_out_total` | Total bytes sent by the HAProxy backend. Tracks outbound network traffic from backend servers (e.g., API servers, VM endpoints) through HAProxy. High or increasing values indicate heavy data transfer, possibly due to large VM images or high client demand. | `rate(haproxy_backend_bytes_out_total[5m])` | Medium |
| `haproxy_backend_connection_errors_total` | Total number of connection errors on HAProxy backends. Increasing values indicate issues with backend connectivity. For example, if this increases for a specific backend, it may indicate server or network issues. | `rate(haproxy_backend_connection_errors_total[5m]) > 0` | Medium |
| `haproxy_backend_connections_total` | Total number of connections made to the HAProxy backend.  Counts all backend connections (from HAProxy to application servers). High or spiking values may indicate scaling events, client surges, or possible DDoS. | `rate(haproxy_backend_connections_total[5m]) > 100` | Medium |
| `haproxy_backend_current_queue` | Current number of requests waiting in the HAProxy backend queue. Shows how many requests are waiting to be processed by backend servers. Nonzero or growing values indicate backend overload, which can delay VM operations or API requests. If no queuing, may be NaN. | `haproxy_backend_current_queue > 0` | High |
| `haproxy_backend_current_sessions` | Current number of active sessions on the HAProxy backend.  Shows the number of open connections to backend servers. High session counts may indicate heavy usage or possible leaks. If no sessions, this may be NaN. | `haproxy_backend_current_sessions > 500` | Medium |
| `haproxy_backend_http_average_queue_latency_milliseconds` | Average latency (in ms) for requests in the HAProxy backend queue. Measures how long requests wait in the queue before being processed. High latency means backend servers are slow or overloaded, which can delay VM operations and API responses. | `haproxy_backend_http_average_queue_latency_milliseconds > 100` | Medium |
| `haproxy_backend_http_average_response_latency_milliseconds` | Average response latency of HAProxy backend HTTP requests in milliseconds. High values indicate slow backend responses. For example, if this exceeds 500ms for a critical service, it may impact user experience. | `haproxy_backend_http_average_response_latency_milliseconds > 500` | Medium |
| `haproxy_backend_http_responses_total` | Total number of HTTP responses from HAProxy backends. Used to monitor backend request volume. For example, trend analysis is used to determine activity in the cluster. | `rate(haproxy_backend_http_responses_total[5m]) == 0` | Medium |
| `haproxy_backend_max_sessions` | Maximum number of concurrent sessions ever seen on the HAProxy backend. Tracks the peak session count. Useful for capacity planning and identifying scaling needs. | `haproxy_backend_max_sessions > 1000` | Medium |
| `haproxy_backend_response_errors_total` | Total number of error responses from the HAProxy backend. Counts all error responses (e.g., 5xx) returned by backend servers. High error rates indicate backend instability or misconfiguration, which can disrupt VM management and API calls. | `increase(haproxy_backend_response_errors_total[5m]) > 0` | High |
| `haproxy_backend_up` | Indicates whether HAProxy backends are up (1) or down (0). Values of 0 indicate backend availability issues. For example, if this is 0 for a critical backend, traffic may not be routed correctly. <br><br>**Metric values:**<br> - `1` Indicates that this service's backend is up<br>- `0` this service's HA proxy backend is down.| `haproxy_backend_up == 0` | Critical |
| `haproxy_frontend_bytes_in_total` | Total bytes received by the HAProxy frontend (from clients). Tracks inbound network traffic to HAProxy. | `rate(haproxy_frontend_bytes_in_total[5m])` | Low |
| `haproxy_frontend_bytes_out_total` | Total bytes sent by the HAProxy frontend (to clients). Tracks outbound network traffic from HAProxy to clients. | `rate(haproxy_frontend_bytes_out_total[5m])` | Low |
| `haproxy_frontend_connections_total` | Total number of connections made to the HAProxy frontend. Counts all client connections to HAProxy. High or spiking values may indicate increased usage or possible DDoS. | `rate(haproxy_frontend_connections_total[5m]) > 100` | Low |
| `haproxy_frontend_current_session_rate` | Current rate of new sessions per second on the HAProxy frontend. Measures how many new sessions are being established per second. Sudden spikes may indicate scaling events or attacks.| `haproxy_frontend_current_session_rate > 50` | Low |
| `haproxy_frontend_current_sessions` | Current number of active sessions on the HAProxy frontend. Shows the number of open client connections. If this value grows without dropping, check for leaks. | `haproxy_frontend_current_sessions > 500` | Medium |
| `haproxy_frontend_max_sessions` | Maximum number of concurrent sessions ever seen on the HAProxy frontend. Tracks the peak number of frontend sessions. Useful for capacity planning. | `haproxy_frontend_max_sessions > 1000` | Medium |
| `haproxy_process_max_fds` | Maximum number of file descriptors that the HAProxy process can open.File descriptors are handles that programs use to access files, network connections, and other I/O resources. Each client connection to HAProxy consumes one file descriptor. If HAProxy runs out of file descriptors, it cannot accept new connections, which would block VM management operations, console access, and API requests. | `haproxy_process_max_fds` | Low |
| `haproxy_process_virtual_memory_max_bytes` | Maximum virtual memory limit for the HAProxy process in bytes. Virtual memory is the total amount of memory (RAM + swap) that a process can use. If HAProxy hits its memory limit, it may crash or become unresponsive, disrupting all VM management traffic routed through it. This includes API calls for VM lifecycle operations, console connections, and monitoring data. | `haproxy_process_virtual_memory_max_bytes`  | Low |
| `haproxy_server_check_failures_total` | Total number of health check failures for backend servers since HAProxy started.  HAProxy periodically sends requests to backend servers (like API servers) to verify they are healthy and can handle traffic. Failed health checks indicate server problems. | `increase(haproxy_server_check_failures_total[10m]) > 0` | High |
| `haproxy_server_connection_errors_total` | Total number of failed connection attempts to backend servers since HAProxy started. These occur when HAProxy cannot establish a TCP connection to a backend server, often due to server being down, network issues, or server overload. | `increase(haproxy_server_connection_errors_total[5m]) > 0` | High |
| `haproxy_server_connections_total` | Total number of connections to HAProxy servers. Used to monitor server load. For example, if this increases rapidly, it may indicate a sudden spike in traffic. | `rate(haproxy_server_connections_total[5m]) > 100` | Medium |
| `haproxy_server_current_queue` | Current number of requests waiting in the queue for this backend server. When a backend server reaches its maximum concurrent connection limit, HAProxy queues additional requests instead of rejecting them. Queued requests wait for an available connection. High queue depths suggest insufficient backend capacity. | `haproxy_server_current_queue > 0` | Medium |
| `haproxy_server_current_session_rate` | Current rate of new sessions being established per second for this backend server.  A session represents the complete interaction between a client and server, from connection establishment to closure. The session rate indicates how busy the server is.| `haproxy_server_current_session_rate > 50` | Low |
| `haproxy_server_current_sessions` | Current number of active sessions for this backend server. | `haproxy_server_current_sessions > $(server_session_limit * 0.8)` | Low |
| `haproxy_server_downtime_seconds_total` | Total downtime of HAProxy servers in seconds. Increasing values indicate server availability issues. For example, if this increases for a critical server, it may indicate recurring outages. | `increase(haproxy_server_downtime_seconds_total[15m]) > 0` | Medium |
| `haproxy_server_http_average_queue_latency_milliseconds` | Average time in milliseconds that HTTP requests spend waiting in the queue before being processed by the backend server. This measures how long requests wait in HAProxy's queue before being forwarded to a backend server. | `haproxy_server_http_average_queue_latency_milliseconds > 100` | Medium |
| `haproxy_server_http_average_response_latency_milliseconds` | Average time in milliseconds for the backend server to respond to HTTP requests. High response latency indicates backend performance issues. Users will experience slow operations, delayed status updates, and poor overall responsiveness. | `haproxy_server_http_average_response_latency_milliseconds > 1000` | High |
| `haproxy_server_http_responses_total` | Cumulative count of HTTP responses sent by each HAProxy backend server. This metric increases every time a backend server (such as an API server or a VM console endpoint) responds to an HTTP request routed through HAProxy. It measures the overall activity and throughput of backend servers. |`rate(haproxy_server_http_responses_total[5m])` | Low |
| `haproxy_server_max_sessions` | Maximum number of concurrent sessions ever observed for this backend server since HAProxy started. This metric records the peak concurrent session count, which helps understand historical peak load and capacity requirements. | `haproxy_server_max_sessions` | Medium |
| `haproxy_server_response_errors_total` | Total number of HTTP response errors (typically 5xx status codes) from backend servers since HAProxy started.  These are HTTP responses indicating server-side failures, such as 500 Internal Server Error, 502 Bad Gateway, or 503 Service Unavailable. Response errors indicate backend service failures or instability. Users will experience error messages in the web console, and unreliable cluster behavior. Even small increases can indicate serious backend issues.| `increase(haproxy_server_response_errors_total[5m]) > 0` | Critical |
| `haproxy_server_up` | Indicates whether HAProxy servers are up (1) or down (0). Values of 0 indicate server availability issues. For example, if this is 0 for a critical server, it may indicate a server failure. <br><br>**Metric values:**<br>- `1` The HAProxy server is for the backend service is up<br>- `0` the HAProxy server for the backend service is down| `haproxy_server_up == 0` | Critical |
| `haproxy_up` | Indicates whether HAProxy is up (1) or down (0). Values of 0 indicate HAProxy service issues. This is the service that runs HA proxy and and is related to the HA Proxy service running in the container (pid 0). For example, if this is 0, the HAProxy service is not running or not reachable. <br><br>**Metric values:**<br>- `1` The HA Proxy service itself is up.<br>- `0` The HA Proxy system service is down| `haproxy_up == 0` | Critical |
| `horizontal_pod_autoscaler_controller_metric_computation_duration_seconds_bucket` | Histogram of durations (in seconds) for metric computation by the Horizontal Pod Autoscaler (HPA) controller. The HPA controller regularly computes metrics (like CPU or memory usage) to decide if pods should be scaled up or down.  Slow metric computation can delay scaling decisions, causing VMs or workloads to be under- or over-provisioned, impacting performance or resource efficiency. | `histogram_quantile(0.95, rate(horizontal_pod_autoscaler_controller_metric_computation_duration_seconds_bucket[5m])) > 1` | Medium |
| `horizontal_pod_autoscaler_controller_metric_computation_duration_seconds_sum` | Cumulative sum of seconds spent computing HPA metrics. Use with the count metric to calculate average computation time. High averages mean the controller is slow, possibly delaying scaling. | `rate(horizontal_pod_autoscaler_controller_metric_computation_duration_seconds_sum[5m]) / rate(horizontal_pod_autoscaler_controller_metric_computation_duration_seconds_count[5m]) > 1` | Medium |
| `horizontal_pod_autoscaler_controller_reconciliation_duration_seconds_bucket` | Histogram of durations for HPA reconciliation loops. The controller periodically reconciles desired and actual pod counts. Slow reconciliation means scaling actions are delayed, affecting workload responsiveness.  | `histogram_quantile(0.95, rate(horizontal_pod_autoscaler_controller_reconciliation_duration_seconds_bucket[5m])) > 1` | Medium |
| `horizontal_pod_autoscaler_controller_reconciliation_duration_seconds_sum` | Cumulative sum of seconds spent on HPA reconciliations.  High values or increasing trends indicate slow controller performance. | `rate(horizontal_pod_autoscaler_controller_reconciliation_duration_seconds_sum[5m]) / rate(horizontal_pod_autoscaler_controller_reconciliation_duration_seconds_count[5m]) > 1` | Low |
| `http_client_dns_duration_seconds_bucket` | Histogram of DNS lookup durations by HTTP clients in the user workload monitoring namespace.  This metric only appears in `openshift-user-workload-monitoring`, which means it tracks DNS lookups made by **user-defined monitoring components**, not core OpenShift Virtualization infrastructure. This metric does **not** provide insight into DNS performance for VM controllers, operators, or other core virtualization components. It only reflects DNS behavior of custom monitoring workloads. | `histogram_quantile(0.95, rate(http_client_dns_duration_seconds_bucket[5m])) > 0.5` | Very Low |
| `http_client_dns_duration_seconds_sum` | Cumulative DNS lookup time for HTTP clients in user workload monitoring. **Only** tracks DNS lookup time for user-defined monitoring components in `openshift-user-workload-monitoring`, **not** core cluster or VM infrastructure.  **Does not** reflect DNS health for VM operations, API servers, or virtualization controllers. | `rate(http_client_dns_duration_seconds_sum[5m]) / rate(http_client_dns_duration_seconds_count[5m]) > 0.5` | Very Low |
| `http_client_in_flight_requests` | In-flight HTTP client requests from user workload monitoring components only. This metric **only** tracks HTTP requests made by custom monitoring workloads, **not** core OpenShift Virtualization components. Only useful if you have custom monitoring applications that you want to track specifically. | `http_client_in_flight_requests > 100` | Very Low |
| `http_client_request_duration_seconds_sum` | Cumulative HTTP request duration for user monitoring workloads only. Limited to `openshift-user-workload-monitoring` namespace components. **Does Not Track:** Core cluster HTTP traffic, API server performance, or VM-related HTTP operations. | `rate(http_client_request_duration_seconds_sum[5m]) / rate(http_client_request_duration_seconds_count[5m]) > 1` | Very Low |
| `http_client_request_total` | Total number of HTTP client requests. Used to monitor client request volume. For example, if this drops suddenly, it may indicate client connectivity issues. | `rate(http_client_request_total[5m]) == 0` | Medium |
| `http_client_tls_duration_seconds_count` | TLS handshake count for user monitoring workloads only.  Only tracks TLS handshakes made by custom monitoring components in `openshift-user-workload-monitoring`. **No Infrastructure Insight.** Does not reflect TLS performance for core cluster components, VM operations, or infrastructure services. | `increase(http_client_tls_duration_seconds_count[5m]) > 100` | Very Low |
| `http_client_tls_duration_seconds_sum` | Cumulative TLS handshake time for user monitoring workloads only.  Limited to user-defined monitoring components in `openshift-user-workload-monitoring`.**Does Not Indicate:** Core cluster TLS health, API server TLS performance, or infrastructure TLS issues. | `rate(http_client_tls_duration_seconds_sum[5m]) / rate(http_client_tls_duration_seconds_count[5m]) > 0.5` | Very Low |
| `http_inflight_requests` | Current number of in-flight (active) HTTP server requests. High values mean the server is busy, which can delay API responses and VM operations. In OpenShift Virtualization clusters, this metric is typically only emitted by certain monitoring components (e.g., thanos-querier in the openshift-monitoring namespace). It does not reflect in-flight requests for the entire cluster or for core control plane components. | `http_inflight_requests > 100` | Very Low |
| `http_request_duration_seconds_bucket` | Histogram of HTTP server request durations for monitoring/logging components only. This metric is emitted by HTTP servers in the `openshift-monitoring` and `openshift-logging` namespaces, not by core OpenShift or virtualization infrastructure. High percentiles indicate slow responses for monitoring or logging APIs, but do **not** reflect VM management, cluster API, or user workload latency. This metric is **not** a proxy for cluster-wide or virtualization API health. | `histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1` | Low |
| `http_request_duration_seconds_sum` | Cumulative sum of HTTP request durations for monitoring/logging components only. Only covers HTTP servers in `openshift-monitoring` and `openshift-logging`. An increase reflects slower or more numerous requests to monitoring/logging endpoints, not VM or cluster API performance. Not useful for understanding VM, API server, or infrastructure latency. | `rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m]) > 1` | Low |
| `http_request_size_bytes_sum` | Cumulative sum of HTTP request sizes for monitoring/logging components only.  Only measures request sizes for endpoints in `openshift-monitoring` and `openshift-logging`. High values may indicate large requests to monitoring/logging APIs, not cluster or VM operations. | `rate(http_request_size_bytes_sum[5m])` | Very Low |
| `http_response_size_bytes_bucket` | Histogram of HTTP response sizes for monitoring/logging components only.  Only reflects response sizes from endpoints in `openshift-monitoring` and `openshift-logging`.  Large responses may stress network for monitoring/logging, but do **not** reflect VM or cluster API traffic. | `histogram_quantile(0.95, rate(http_response_size_bytes_bucket[5m]))` | Very Low |
| `image_registry_image_stream_tags_total` | Total number of image stream tags in the OpenShift image registry. High tag counts indicate many images/versions, which can impact registry performance and storage. Sudden drops may mean data loss. | `image_registry_image_stream_tags_total > 10000` | Low |
| `image_registry_operator_image_pruner_install_status` | Indicates the installation and operational status of the OpenShift Image Registry Pruner. <br><br>**Metric values:**<br>- `0` Pruner not installed <br>- `1` Pruner suspended (configured but not active) <br>- `2` Pruner enabled (actively managing image pruning). <br><br> The image pruner is responsible for removing unused images from the internal image registry, helping to reclaim storage. If not installed or suspended, unused images will accumulate, risking registry storage exhaustion.| `image_registry_operator_image_pruner_install_status != 2` | Medium |
| `image_registry_operator_storage_reconfigured_total` | Total number of times image registry storage has been reconfigured. Frequent reconfiguration may indicate instability or misconfiguration, which can disrupt VM image pulls. If never reconfigured, may be zero or absent. | `increase(image_registry_operator_storage_reconfigured_total[1h]) > 0` | Low |
| `inflight_requests` | Current number of in-flight (active) HTTP requests, but only for components that explicitly expose this metric.
| `ingress_canary_endpoint_wrong_port_echo` | Tracks misconfigured canary endpoints in Ingress controllers. Canary testing routes a portion of traffic to new service versions. A wrong port indicates misconfiguration that could redirect VM console/API traffic to invalid endpoints. | `ingress_canary_endpoint_wrong_port_echo > 0` | Low |
| `ingress_controller_conditions` | Status conditions of Ingress controllers. Reports health states `Available=True` and `Degraded=True`. Critical for VM console access and API endpoints exposed via Ingress. This emits 0/1 booleans.    <br><br>**Metric values:**<br>- `0` means the contition is false<br>- `1` means the condition is true.| `ingress_controller_conditions{condition="Degraded"} > 0` | High |
| `instance_device:node_disk_io_time_weighted_seconds:rate1m` | The rate of weighted disk I/O time per device, in seconds per second, over a 1-minute window. This metric is the rate of increase of `node_disk_io_time_weighted_seconds_total` (from node_exporter), which is the sum of the time each I/O operation spends in the queue, weighted by the number of concurrent operations. For example, if two I/Os are in progress for an entire second, the counter increases by 2 seconds in that second, so the rate is 2.  High values indicate disk saturation or queuing, meaning the device is handling many concurrent I/O operations. This can cause increased disk latency, which will negatively affect VM disk performance (boot, snapshots, migrations) in OpenShift Virtualization.| `instance_device:node_disk_io_time_weighted_seconds:rate1m > 10` | High |
| `instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile` | Shows distribution of time etcd takes to commit transactions to disk. Fast commits are crucial for VM operations because every VM state change must be durably stored. Slow commits cause VM operations to feel sluggish and can lead to timeouts during VM lifecycle operations. Example: 95th percentile over 10ms indicates storage problems. | `histogram_quantile(0.95, rate(etcd_disk_backend_commit_duration_seconds_bucket[5m])) > 10` | Critical |
| `instance:etcd_mvcc_db_total_size_in_bytes:sum` | This is the actual space allocated on disk for the etcd database, including both live data and unused/free space left after compaction. If this grows rapidly, it may indicate excessive churn (e.g., frequent VM creation/deletion) or that compaction/defragmentation is not keeping up. If it approaches the storage quota, etcd may reject writes, impacting VM and cluster operations.**Example:** If your etcd quota is 8GB and this metric is 7.5GB, you are at risk of cluster instability. | `sum(etcd_mvcc_db_total_size_in_bytes)` | High |
| `instance:node_cpu:rate:sum` | Sum of per-second CPU usage rates across all CPUs on a node. This metric is the sum of the per-second rates of `node_cpu_seconds_total` (excluding "idle" and possibly "iowait") for all CPUs on a node. Each CPU can contribute up to 1 to the sum (since each CPU can be busy for up to 1 second per second). If a node has 8 CPU cores, the maximum possible value is 8. A value of 1.1 means, on average, all CPUs combined are busy for 1.1 seconds per second, which is about 13.75% utilization on an 8-core node (1.1/8 = 13.75%).- This is not a percentage; it is the total "busy seconds per second" summed over all CPUs.  To get a percentage of total CPU usage, divide this value by the number of CPU cores on the node. | `sum(instance:node_cpu:rate:sum) by (instance) / sum(instance:node_num_cpu:sum) by (instance) > 0.8` (alert if CPU utilization >80%) | High |
| `instance:node_load1_per_cpu:ratio` | 1-minute load average divided by CPU cores. Values >1 indicate CPU saturation. Ratio >1.5 causes noticeable VM performance degradation. | `instance:node_load1_per_cpu:ratio > 1.5` | High |
| `instance:node_memory_utilisation:ratio` | Node memory used ÷ total memory. High values risk OOM kills or swapping, which severely impacts VM performance. >90% may trigger VM evictions or freezes. | `instance:node_memory_utilisation:ratio > 0.9` | Critical |
| `instance:node_num_cpu:sum` | Total number of CPU cores (logical CPUs) per node, or summed across all nodes. This metric is a static count of the CPUs available on each node. It is used for capacity planning, resource allocation, and as a denominator for calculating CPU utilization percentages. | `sum(instance:node_num_cpu:sum) by (instance)` | Low |
| `instance_request_kind:apiserver_current_inflight_requests:sum` | Sum of concurrent requests to the API server by request kind. High values indicate API server congestion. If this exceeds 100 for mutating requests, the API server may be overloaded. | `instance_request_kind:apiserver_current_inflight_requests:sum{request_kind="mutating"} > 100` | High |
| `job_controller_terminated_pods_tracking_finalizer_total` | Cumulative counter of finalizer events on terminated pods with the `batch.kubernetes.io/job-tracking` finalizer. This is a **Counter** metric that tracks the total number of events (both "add" and "delete") related to the finalizer `batch.kubernetes.io/job-tracking` on terminated pods (phase=Failed or Succeeded). It increases over time as pods are terminated and finalizers are added/removed. The **rate of increase** indicates job churn and finalizer management activity. High rates may suggest heavy job activity (many pods being created/terminated) or potential issues with finalizer cleanup processes. This is **not** a count of pods currently awaiting cleanup. It's a cumulative total of finalizer events since the controller started. | `rate(job_controller_terminated_pods_tracking_finalizer_total[5m]) > 10` | Low |
| `job_namespace_route_statuscode:loki_request_duration_seconds_count:irate1m` | Loki log request rate by status code. Tracks logging performance for VM-related logs. High 5xx rates indicate logging issues, complicating VM troubleshooting. | `rate(job_namespace_route_statuscode:loki_request_duration_seconds_count:irate1m{status_code=~"5.."}[5m]) > 0.05` | Low |
| `kube_apiserver_clusterip_allocator_available_ips` | The number of available ClusterIP addresses that the Kubernetes API server can allocate for new Services in the cluster. This metric reports the current count of unallocated ClusterIP addresses from the Service CIDR range. Each Service of type `ClusterIP` is assigned a unique IP from this range. If this number drops too low, the cluster will be unable to create new Services, which can break VM console access, service exposure, load balancers, and any other Service-based networking.| `kube_apiserver_clusterip_allocator_available_ips < 100` (alert if available IPs drop below a safe threshold) | Medium |
| `kube_cronjob_info` | Metadata about CronJobs. This can be used to track missing cronjobs for a namespace or by name. | `absent(kube_cronjob_info{cronjob="vm-backup"})` | Low |
| `kube_cronjob_next_schedule_time` | Next scheduled run time for CronJobs. The query by itself displays the exact time of the next run in seconds from the epoch. To get the number of seconds until the next cronjob runs use `time()` | `kube_cronjob_next_schedule_time - time()` | Very Low |
| `kube_cronjob_spec_suspend` | Indicates whether a CronJob is suspended. Impact: Suspended CronJobs will not run scheduled tasks. If this is True for a critical CronJob, scheduled tasks may not execute. | `kube_cronjob_spec_suspend == "True"` | Medium |
| `kube_cronjob_status_active` | Active jobs spawned by CronJobs. High counts may indicate job pileup, risking resource exhaustion for VM hosts. | `kube_cronjob_status_active > 10` | Low |
| `kube_daemonset_status_updated_number_scheduled` | Number of nodes with updated DaemonSet pods. Ensures critical pods (e.g., CNI, monitoring) run everywhere. In order to use this effectively you need to know what the expect value of a specific daemonset is. The value corresponds to the number of nodes the daemon set is expected to run on. | `kube_daemonset_status_updated_number_scheduled{daemonset="virt-handler"}` | Medium|
| `kube_deployment_spec_replicas` | Desired number of pods for a deployment. This metric shows the number of pod replicas you want the deployment to maintain, as set in the deployment’s `.spec.replicas` field. This is your “desired state. | `kube_deployment_spec_replicas{deployment="virt-controller"} != kube_deployment_status_replicas{deployment="virt-controller"}` | High |
| `kube_deployment_spec_strategy_rollingupdate_max_unavailable` | Max unavailable pods allowed during a rolling update. This metric reflects the value of `.spec.strategy.rollingUpdate.maxUnavailable` for a deployment. It controls how many pods can be unavailable during an update before the deployment controller pauses further updates. | `kube_deployment_spec_strategy_rollingupdate_max_unavailable`  | Low |
| `kube_deployment_status_condition` | Status conditions of Deployments. Indicates health and readiness of Deployments. If a Deployment shows a False Ready condition, pods may not be available for traffic. | `kube_deployment_status_condition{condition="Available", status="false"} == 1` | Medium |
| `kube_deployment_status_replicas_available` | Number of available replicas in a Deployment. Used to monitor Deployment health. If this is less than the desired number, po{"ds may not be fully available. | `kube_deployment_status_replicas_available{deployment="apiserver"} <3` | Medium |
| `kube_deployment_status_replicas` | Current number of pods managed by the deployment. This metric shows the actual number of pods that exist for the deployment, as reported in `.status.replicas`. This is your “actual state.” | `kube_deployment_status_replicas{deployment="virt-operator"}` | High |
| `kube_job_failed` | Indicates if a Kubernetes Job has failed.  If no Jobs exist, this may be absent. <br><br>**Metric values:**<br>- `0` Indicates the Job has **not** failed.<br>- `1` indicates the job **has** failed| `kube_job_failed == 1` | Medium |
| `kube_job_status_active` | Number of active pods for each Job. Shows how many pods are currently running for a Job. High values may indicate long-running or stuck Jobs, which can block resources needed for VM operations. | `kube_job_status_active` | Medium |
| `kube_job_status_completion_time` | Completion time of Jobs as a Unix timestamp. Shows when a Job finished. Useful for auditing and troubleshooting VM-related batch operations. | ` kube_job_status_completion_time ` | Low |
| `kube_job_status_failed` | Indicates failed Jobs. May indicate issues with Job execution or configuration. If this increases for a critical Job, it may indicate recurring failures. | `kube_job_status_failed == 1` | Medium |
| `kube_job_status_succeeded` | Number of pods that have completed successfully for each Kubernetes Job. This metric is a gauge that reflects, for each Job, the count of pods that have reached a successful (Succeeded) state. For most Jobs, this will be 1 (the default completions for a Job is 1), but for Jobs configured with higher completions, this value can be greater. <br><br>**Metric values:**<br>- `0`A value of 0 means the Job has not yet succeeded, which could indicate a running, pending, or failing Job.<br>- `1` A value of 1 means the Job has finished successfully.  | `kube_job_status_succeeded == 0` | Medium |
| `kubelet_active_pods` | Current number of pods that the kubelet considers "active" on each node. This metric shows the count of pods that the kubelet is actively managing and considers when making admission decisions for new pods. "Active" pods include those in Running, Pending, and other non-terminal states. Each node has a maximum pod limit (typically 250 depending on configuration). As this number approaches the node's pod limit, new VM pods may fail to schedule| `sum by (node) (kubelet_active_pods) / sum by (node) (kube_node_status_capacity{resource="pods"}) > 0.8` | High |
| `kubelet_certificate_manager_client_expiration_renew_errors` | Total number of errors when renewing client certificates by the kubelet. High values indicate problems with kubelet authentication. | `increase(kubelet_certificate_manager_client_expiration_renew_errors[1h]) > 0` | High |
| `kubelet_certificate_manager_server_ttl_seconds` | Gauge of the shortest time-to-live (TTL) in seconds for the kubelet's server certificate.  This metric reports the remaining lifetime (in seconds) of the kubelet’s serving certificate—the certificate that the kubelet uses to serve HTTPS traffic (for example, to the Kubernetes API server and other clients). If this value drops too low, the certificate is about to expire. An expired or invalid kubelet server certificate will cause the node to lose trust with the cluster, leading to node NotReady status, inability to manage pods (including VM pods), and potentially node eviction. | `kubelet_certificate_manager_server_ttl_seconds < 86400` | High |
| `kubelet_cpu_manager_pinning_errors_total` | Total number of CPU pinning errors by the kubelet CPU manager. High values indicate issues assigning CPUs to pods, which can impact VM performance, especially for workloads needing dedicated CPUs. | `increase(kubelet_cpu_manager_pinning_errors_total[1h]) > 0` | Medium |
| `kubelet_desired_pods` | Number of pods desired by the kubelet on each node. Shows how many pods the scheduler wants to run on the node. If this exceeds node limits, pods (including VMs) may be unschedulable. | `kubelet_desired_pods` | Low |
| `kubelet_evented_pleg_connection_error_count` | **Not Enabled in OpenShift 4.17/4.18** <br><br>Cumulative count of connection errors between the kubelet and the evented Pod Lifecycle Event Generator (PLEG). This metric is a **counter** that increases every time the kubelet fails to establish or maintain a connection to the evented PLEG subsystem. Evented PLEG is a newer (as of Kubernetes v1.26+) mechanism for the kubelet to track pod/container lifecycle events using CRI events, rather than polling. | `increase(kubelet_evented_pleg_connection_error_count[1h]) > 0` | N/A |
| `kubelet_evented_pleg_connection_success_count` | **Not Enabled in OpenShift 4.17/4.18** <br><br>**Total number of successful connections to the evented PLEG.**High values are normal; sudden drops may indicate connectivity problems, risking stale VM pod status.**Example:** If success count stops increasing but errors rise, investigate PLEG health. | `increase(kubelet_evented_pleg_connection_success_count[1h]) == 0` | N/A |
| `kubelet_first_network_pod_start_sli_duration_seconds` | Gauge metric tracking the duration (in seconds) to start the first network pod on each node, excluding image pull time. This is a **per-node metric** that tracks only the **first** network pod startup time on each node. It tracks the time from pod creation to when all containers are started for the **first network pod** on a node. This is an internal metric used to understand node network readiness latency | `kubelet_first_network_pod_start_sli_duration_seconds > 30e9` | Very Low |
| `kubelet_http_inflight_requests` | Current number of HTTP requests being processed by the kubelet HTTP server. This is a **gauge** metric exposed by the kubelet. It reports, in real time, the number of HTTP requests currently being handled by the kubelet’s internal HTTP server. The metric is labeled by HTTP method, path, server_type, and long_running status. If the kubelet is overloaded, VM pods may be slow to start, stop, or migrate, and node health checks may be delayed. Values are typically low (often 0–2) under normal load. Spikes or sustained high values (e.g., >100) are rare and may indicate a problem. This metric **only reflects HTTP requests to the kubelet's own server**, not API server or cluster-wide requests. It does **not** measure kubelet-to-apiserver traffic, so it should not be used to size kubelet QPS or for cluster-wide API load monitoring. | `kubelet_http_inflight_requests`  | Low |
| `kubelet_http_requests_duration_seconds_bucket` | Histogram of HTTP request durations to the kubelet. High percentiles indicate slow kubelet API responses, which can delay pod and VM management. | `histogram_quantile(0.95, rate(kubelet_http_requests_duration_seconds_bucket[5m])) > 1` | Medium |
| `kubelet_http_requests_duration_seconds_count` | Cumulative counter of the total number of HTTP requests handled by the kubelet, broken down by method, path, and server type. This metric is a **counter** that increments by one for every HTTP request the kubelet serves. A high request rate may indicate heavy kubelet API usage. A **sudden drop to zero in the rate** may indicate kubelet failure or network partition. The absolute value by itself is not meaningful for alerting | `sum(rate(kubelet_http_requests_duration_seconds_count[5m])) == 0` | Medium |
| `kubelet_http_requests_duration_seconds_sum` | Cumulative sum of seconds spent handling HTTP requests by the kubelet. Use with count to calculate average request duration. High averages indicate kubelet slowness. | `rate(kubelet_http_requests_duration_seconds_sum{method="GET"}[5m]) / rate(kubelet_http_requests_duration_seconds_count{method="GET"}[5m]) > 1` | Medium |
| `kubelet_image_pull_duration_seconds_bucket` | Histogram of container image pull durations. Measures time taken to pull container images from registries to nodes. Includes VM base images, virt-launcher containers, and other components. High percentiles indicate registry or network bottlenecks. If no image pulls occurred in the window this might be NaN. | `histogram_quantile(0.95, rate(kubelet_image_pull_duration_seconds_bucket[5m])) > 30` | Medium |
| `kubelet_memory_manager_pinning_errors_total` | Total number of memory pinning errors by the Kubelet memory manager. Indicates issues with memory allocation. If memory pinning fails, workloads requiring guaranteed memory allocation may experience higher latency and decreased performance, especially in NUMA architectures. This metric should ideally always be 0 | `rate(kubelet_memory_manager_pinning_errors_total[5m]) > 0` | Medium |
| `kubelet_memory_manager_pinning_requests_total` | Total requests for memory pinning. Count of requests to pin memory pages for guaranteed pods (like VMs needing dedicated memory). High counts indicate many VM startups; failures cause OOM or performance issues. Errors may leave VMs without guaranteed memory. | `increase(kubelet_memory_manager_pinning_requests_total[5m]) > 50` | Medium |
| `kubelet_node_name` | Node identifier label (not a numeric metric). Contextual label for other metrics, identifying which node metrics originate from. Used in queries like `kubelet_pod_start_duration_seconds_bucket{node="worker-0"}`**Note:** Not used in isolation for alerting. | N/A | N/A |
| `kubelet_node_startup_duration_seconds` | Total time for node to become Ready after kubelet start.From kubelet process start to node reporting "Ready" status. This metric does not seem to be very useful a lot of clusters will have a value of 0, while others have a non-zero value for only a few hosts. | `kubelet_node_startup_duration_seconds > 120` | Very Low |
| `kubelet_node_startup_registration_duration_seconds` | Time taken to register node with API server. Measures a subset of node startup focused on API registration. As with `kubelet_node_startup_duration_seconds`, does not seem to have a lot of value in active production clusters. | `kubelet_node_startup_registration_duration_seconds > 30` | Ver Low |
| `kubelet_orphan_pod_cleaned_volumes_errors` | Errors cleaning orphaned pod volumes. Measures failures to remove persistent volumes from terminated pods (like VM disks). Errors leave storage allocated, causing resource leaks and preventing VM deletion. | `increase(kubelet_orphan_pod_cleaned_volumes_errors[1h]) > 0` | Medium |
| `kubelet_pleg_discard_events` | Discarded Pod Lifecycle Events due to full buffers. Events lost when PLEG (Pod Lifecycle Event Generator) can't process them fast enough. Lost events cause stale pod states – VMs may show incorrect status (Running/Stopped). | `increase(kubelet_pleg_discard_events[5m]) > 0` | N/A |
| `kubelet_pleg_last_seen_seconds` | Time in seconds since the last PLEG (Pod Lifecycle Event Generator) update. High values indicate Kubelet issues. If this exceeds 60 seconds, it may indicate Kubelet is not updating pod status correctly. | `kubelet_pleg_last_seen_seconds > 60` | Medium |
| `kubelet_pleg_relist_duration_seconds_bucket` | Time taken for PLEG to list pod states. Duration of periodic pod state checks. Long relists delay VM status updates and cause "PLEG not healthy" errors | `histogram_quantile(0.95, rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) > 5` | High |
| `kubelet_pleg_relist_interval_seconds_bucket` | Interval between PLEG relist operations. This measures how frequently pod states are checked. In OpenShift the default relist time setting is 1 second. This metric will reflect the entire amount of time the action took to run. Long intervals delay detection of VM state changes (start/stop/crash).**Example:** Intervals >300s cause stale VM status.**NaN:** If no relists occurred. | `histogram_quantile(0.95, rate(kubelet_pleg_relist_interval_seconds_bucket[5m])) > 10` | Medium |
| `kubelet_pod_start_duration_seconds_bucket` | Time from pod creation to all containers running. This measures the full pod initialization time, including network setup. This can directly impact VM startup time. High values mean slow VM provisioning. If no pods have started in the window, the query may return NaN. This is normal during idle periods. | `histogram_quantile(0.95, rate(kubelet_pod_start_duration_seconds_bucket[5m])) > 30` | High |
| `kubelet_pod_start_sli_duration_seconds_bucket` | Histogram of pod start latency in seconds. High values indicate slow pod startup times. If the 95th percentile exceeds 30 seconds, pod startup may be slow, affecting service availability. | `histogram_quantile(0.95, sum(rate(kubelet_pod_start_sli_duration_seconds_bucket[5m])) by (le))` | Medium |
| `kubelet_pod_start_sli_duration_seconds_sum` | Cumulative sum of pod start durations (in seconds) for the Service Level Indicator (SLI) of pod startup time. This metric is a **counter** that increases by the duration (in seconds) each time a pod starts. It is part of a histogram family (with `_count` and `_bucket`), allowing you to calculate averages and percentiles for pod startup times. It specifically tracks the time from when a pod is created until it is considered "started" by the kubelet (all containers running and network ready). If the average exceeds 20 seconds, it may indicate systemic issues that need investigation. If no pods start during the query window, the result may be NaN. This is normal during idle periods.| `rate(kubelet_pod_start_sli_duration_seconds_sum[5m]) / rate(kubelet_pod_start_sli_duration_seconds_count[5m]) > 20` | Medium |
| `kubelet_pod_start_total_duration_seconds_sum` | Cumulative sum of total pod start durations (in seconds), covering all phases from scheduling to containers running. This metric is a **counter** that increases by the duration (in seconds) each time a pod completes its full startup lifecycle. It includes all phases: scheduling wait time, image pulling, network setup, and container startup. It is broader than the SLI-specific pod start metric, as it accounts for delays not just at the node/kubelet level but also in the Kubernetes scheduler and API server. High average values can indicate cluster-wide issues such as scheduler delays, API server slowness, image registry/network problems, or node resource contention. If no pods start during the query window, the result may be NaN. This is normal during idle periods.| `rate(kubelet_pod_start_total_duration_seconds_sum[5m]) / rate(kubelet_pod_start_total_duration_seconds_count[5m]) > 45` | Medium |
| `kubelet_pod_status_sync_duration_seconds_bucket` | Time to sync pod status with API server. Measures latency in reporting pod/VMM status to control plane. Delays cause stale VM status in OpenShift console/Kubernetes API. | `histogram_quantile(0.95, rate(kubelet_pod_status_sync_duration_seconds_bucket[5m])) > 3` | Medium |
| `kubelet_pod_status_sync_duration_seconds_sum` | Cumulative sum of time (in seconds) the kubelet spends syncing pod status to the API server.  This metric is a **counter** that increases by the duration (in seconds) each time the kubelet performs a pod status sync operation. <br><br> In active OpenShift and Kubernetes clusters, the rate-based average can spike into the thousands (seconds), especially during periods of low activity or after kubelet restarts. These spikes are often artifacts of how Prometheus calculates rates on counters that reset (e.g., after kubelet restarts), or when there are few sync events in the window, leading to misleadingly high values. If there are very few syncs in the window, the denominator is low, so even a normal sum can yield a large average. These are not necessarily signs of API or network issues, but rather a limitation of the metric and rate calculation method. <br><br> Occasional or brief spikes into the thousands are **normal and expected** and do not indicate a problem. Sustained high averages (not just spikes) may indicate real API/network slowness. For VM state accuracy, focus on persistent trends rather than transient spikes. | `rate(kubelet_pod_status_sync_duration_seconds_sum[5m]) / rate(kubelet_pod_status_sync_duration_seconds_count[5m]) > $(status_sync_latency_threshold)` | Low |
| `kubelet_pod_worker_duration_seconds_sum` | Cumulative sum of time (in seconds) spent by the kubelet's "pod worker" goroutines processing pod state transitions. In the kubelet, a "pod worker" is an internal thread (goroutine) responsible for handling all lifecycle operations for a specific pod. Each pod assigned to a node is managed by its own pod worker. This metric is a **counter** that increases by the amount of wall-clock time (in seconds) each pod worker spends processing pod state transitions. It is not measuring CPU time, but rather the elapsed time spent by all pod workers managing pods on the node. High average processing time per sync can indicate kubelet overload, slow container runtime, network/storage delays, or misbehaving pods. The threashold for this metric should be based on benchmarking your environment. If no pod processing occurred in the window, the result may be NaN. | `rate(kubelet_pod_worker_duration_seconds_sum[5m]) / rate(kubelet_pod_worker_duration_seconds_count[5m]) > 20` | Medium |
| `kubelet_run_podsandbox_errors_total` | Errors creating pod sandboxes. Measures failures to initialize pod runtime environment which includes the creation of Linux namespaces in order for all containers in the same pod to be able to share namespaced resources. This is a counter and may not have data points in your cluster.  | `increase(kubelet_run_podsandbox_errors_total[5m]) > 0` | Critical |
| `kubelet_runtime_operations_errors_total` | Errors in container runtime operations. Measures failures in start/stop/inspect operations for containers (including VM components). Increase indicates runtime instability. | `increase(kubelet_runtime_operations_errors_total[5m]) > 0` | Critical |
| `kubelet_server_expiration_renew_errors` | Cumulative count of errors encountered by the kubelet when attempting to renew its serving (HTTPS) certificate. This metric is a counter that increments each time the kubelet fails to renew its serving certificate. The kubelet serving certificate is automatically rotated before expiry; renewal failures may be caused by API server issues, certificate authority problems, or network disruptions. | `increase(kubelet_server_expiration_renew_errors[24h]) > 0` | High |
| `kubelet_started_containers_errors_total` | Cumulative count of errors the kubelet encountered when starting containers. This metric counts every failure the kubelet experiences when attempting to start a container, including application containers, system pods, and VM-related containers. Persistent or frequent errors may indicate systemic issues with image registries, node resources, configuration, or underlying infrastructure.  | `increase(kubelet_started_containers_errors_total[5m]) > 0` | Critical |
| `kubelet_topology_manager_admission_errors_total` | Cumulative count of errors where the kubelet's Topology Manager failed to admit a pod for scheduling due to topology constraints. The Topology Manager attempts to align CPU, memory, and device allocations for optimal NUMA (Non-Uniform Memory Access) locality. Errors here mean pods (including VM pods) could not be scheduled with the requested hardware topology, potentially degrading VM performance or causing scheduling failures. If no admission errors have occurred, this may be NaN. | `increase(kubelet_topology_manager_admission_errors_total[1h]) > 0` | Medium |
| `kubelet_topology_manager_admission_requests_total` | Cumulative count of pod admission requests evaluated by the kubelet's Topology Manager. This metric tracks how often the Topology Manager is invoked to make allocation decisions. High values indicate frequent scheduling of pods requiring topology alignment (common for performance-sensitive VMs). If no such pods are scheduled, may be NaN. | `increase(kubelet_topology_manager_admission_requests_total[1h]) > $(topology_req_threshold)` | Low |
| `kubelet_volume_stats_available_bytes` | Available bytes in a volume. Used to monitor volume capacity and detect potential storage issues. If available bytes are low, it may indicate that the volume is running out of space. | `kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes < 0.2` | Medium |
| `kubelet_volume_stats_capacity_bytes` | Total storage capacity (in bytes) for each persistent or ephemeral volume attached to a pod.  The value is static for each volume and reflects the total provisioned size, not the current usage. Sudden drops in capacity may indicate storage reconfiguration, volume resizing, or misconfigurations. | `kubelet_volume_stats_available_bytes /kubelet_volume_stats_capacity_bytes  < 0.15` | High |
| `kubelet_volume_stats_inodes_free` | Number of free inodes in a volume. Low values indicate potential inode exhaustion. If free inodes are less than 5% of total inodes, it may indicate inode exhaustion issues. | `kubelet_volume_stats_inodes_free / kubelet_volume_stats_inodes < 0.05` | Medium |
| `kube_node_spec_unschedulable` | Indicates if a node is marked unschedulable (cordoned). <br><br>**Metric Values**<br>- `1` the node is unschedulable<br>- `0` The node is schedulable and uncordoned.<br> Unschedulable nodes do not accept new pods, including VM pods. Used during upgrades, maintenance, or when draining nodes. | `sum(kube_node_spec_unschedulable) > 0` | High |
| `kube_node_status_allocatable` | Amount of allocatable resources (CPU, memory, ephemeral_storage, pods, ) available on each node. {This metric shows what is available for new pods, after system and reserved resources are subtracted. Critical for VM scheduling, as VMs require guaranteed CPU/memory.**Example:** If allocatable memory drops below VM requirements, new VMs cannot be started.**NaN:** If node is not reporting status. | `kube_node_status_allocatable{resource="memory"}`| Medium|  
| `kube_pod_start_time` | Unix timestamp of when each pod was started. Used to track VM pod uptime, detect restarts, or audit pod lifecycle events. Helps identify recent VM launches or unexpected restarts. If a VM pod start time is much more recent than expected, may indicate a crash or migration. | `time() - kube_pod_start_time > $(max_vm_uptime)` | Low |
| `kube_pod_status_phase` | Current lifecycle phase of each pod (Pending, Running, Succeeded, Failed, Unknown). Critical for monitoring VM pod health and progress. Use to detect stuck, failed, or pending VM pods. | `sum by (phase) (kube_pod_status_phase{phase!~"Succeeded|Running"} == 1)` | High |
| `kube_pod_status_ready` | Indicates whether a pod is ready to serve requests. Non-ready pods may not receive traffic. If a critical pod is not ready, it may indicate issues with the pod or its containers. This metric is someone confusing because each pod has a condition of `false`, `true` and `unknown`. Each of these have the following: <br><br>**Metric Values**<br>- `1` Means the condition is true<br>- `0` means the condition is false.  | `kube_pod_status_ready{namespace="default", pod="critical-pod", condition="true" } == 1` | High |
| `kube_pod_status_reason` | Reason for the current status of each pod. Provides detailed context for why a pod is in a given phase (e.g., "CrashLoopBackOff", "ImagePullBackOff", "Evicted"). Essential for troubleshooting VM pod failures. | `kube_pod_status_reason{reason=~"Evicted|ImagePullBackOff"} > 0` | High |
| `kube_pod_status_unschedulable` | Indicates whether a pod is unschedulable. Unschedulable pods cannot be placed on nodes. If a pod is unschedulable due to resource constraints, it may indicate the need for node scaling. | `kube_pod_status_unschedulable{namespace="default", pod="critical-pod"} == 1` | Medium |
| `kube_resourcequota` | Current usage and hard limits for resource quotas in each namespace (limits.cpu, limits.memory, pods, requests.cpu, requests.memory). Tracks how much CPU, memory, storage, and object count is used vs. quota. Prevents over-provisioning and ensures fair resource allocation for VMs and other workloads. | `kube_resourcequota{resource="pods", type="used"} / kube_resourcequota{resource="pods", type="hard"} > 0.9` | Medium |
| `kubernetes_feature_enabled` | Indicates whether specific Kubernetes features are enabled (1) or disabled (0). Used to audit cluster feature gates, such as those affecting virtualization, security, or performance. <br><br>**Metric Values**<br>- `1` The feature is enabled<br>- `0` The feature is disabled.| `kubernetes_feature_enabled == 0` | Low |
| `kube_running_pod_ready` | Number of running pods that are ready. Used to monitor pod health and availability. If this drops suddenly, it may indicate pod failures or readiness issues. | `kube_running_pod_ready{namespace="default"} < 10` | Medium |
| `kube_service_info` | Metadata about each Kubernetes Service, including namespace, name, and type. Used to audit and inventory services exposing VM consoles, APIs, or load balancers. Helps track which services are available and their configuration. | `count by (service) (kube_service_info)` | Low |
| `kube_statefulset_persistentvolumeclaim_retention_policy` | Current retention policy configuration for PVCs created by each StatefulSet. This metric is a **gauge** that emits a value of `1` for every StatefulSet, labeled by its `namespace`, `statefulset`, `when_deleted`, and `when_scaled` retention policy settings. The metric **does not indicate "Delete" or "Retain" by the value**. Instead, the value is always `1` for each StatefulSet/policy combination, and the actual retention policy is reflected in the labels: `when_deleted="Retain"` or `when_scaled="Retain"`.  **Do not use the metric value itself** to determine the policy. Instead, use the `when_deleted` and `when_scaled` labels to audit or alert on risky configurations. <br><br>**Metric values:**<br>- `1` The metric is considered true<br>- `0` The metric is considered false| `kube_statefulset_persistentvolumeclaim_retention_policy{when_deleted="Delete"} == 1`  | High |
| `kube_statefulset_status_replicas_available` | This gauge shows how many replicas in the StatefulSet are currently available to handle requests. During rolling updates, available replicas may temporarily decrease as old VMs are replaced. While "ready" means the VM passes its readiness probe right now, "available" means it has remained ready for a sustained period, indicating genuine stability. This is especially important for stateful workloads where brief ready/not-ready cycles can indicate underlying issues. | `kube_statefulset_status_replicas_available < kube_statefulset_status_replicas`  | High |
| `kube_statefulset_status_replicas` | Current number of pod replicas that exist for each StatefulSet, regardless of their readiness state. During a rolling update, this metric might temporarily show higher numbers as old and new replicas coexist. If nodes fail, this metric might drop below the desired count until new pods are rescheduled. Always compare this with `kube_statefulset_status_replicas_ready` to understand how many of these replicas are actually functional and ready to serve traffic. | `kube_statefulset_status_replicas < kube_statefulset_status_replicas_ready` | High |
| `kube_statefulset_status_replicas_ready` | This gauge reports the count of StatefulSet replicas that are currently passing their readiness checks. These are pods that Kubernetes considers healthy and ready to handle requests. During VM startup (especially with large disk images), replicas may take time to become ready as the VM boots and applications initialize. A pod can be in "Running" state but still not ready if its readiness probe fails. | `kube_statefulset_status_replicas_ready < kube_statefulset_status_replicas`  | High |
| `kube_state_metrics_custom_resource_state_add_events_total` | Cumulative counter of "add" events processed by kube-state-metrics for custom resources managed in the cluster. **The metric is only relevant if kube-state-metrics is specifically configured to collect custom resource state metrics for your CRDs.**  | `increase(kube_state_metrics_custom_resource_state_add_events_total[5m]) > 100` (alert on unusually high custom resource creation rate) | Very Low |
| `kube_state_metrics_custom_resource_state_cache` | Gauge indicating the health and status of the custom resource state cache within kube-state-metrics. Kube-state-metrics maintains an internal cache of custom resource states. This cache stores the current state of all monitored custom resources and is updated through API watch events. The exact values depend on the kube-state-metrics implementation, but generally:  <br><br>**Metric values:**<br>- `1`  means the cahce is healthy<br>- `0` means the cache is empty or has not syncronized.<br><br> **The metric is only relevant if kube-state-metrics is specifically configured to collect custom resource state metrics for your CRDs.**  | `kube_state_metrics_custom_resource_state_cache == 0` (alert when cache is unhealthy) | Very Low |
| `kube_state_metrics_custom_resource_state_delete_events_total` | Cumulative counter of "delete" events processed by kube-state-metrics for custom resources in the cluster.  This counter increments each time a custom resource is deleted and kube-state-metrics processes the deletion event. It provides insight into the rate and volume of resource cleanup activities in your OpenShift Virtualization environment. **The metric is only relevant if kube-state-metrics is specifically configured to collect custom resource state metrics for your CRDs.** | `increase(kube_state_metrics_custom_resource_state_delete_events_total[5m]) > 50`  | Very Low |
| `kubevirt_cdi_clone_pods_high_restart` | Number of CDI (Containerized Data Importer) clone pods that have experienced a high number of restarts. Clone pods are responsible for creating copies of existing DataVolumes (VM disks). Each clone operation creates a temporary pod that performs the disk-to-disk copy operation. High restart counts indicate that disk cloning operations are failing, which directly impacts VM provisioning capabilities. Check for storage latency issues, pod resource limits, and ensure adequate storage provisioner performance. | `kubevirt_cdi_clone_pods_high_restart > 0` | High |
| `kubevirt_cdi_cr_ready` | Indicates whether the CDI (Containerized Data Importer) Custom Resource is in a ready state. This gauge metric indicates whether the CDI Custom Resource has reached a ready state:<br><br>**Metric values:**<br>- `1` = CDI is ready and all components are functional<br>- `0` = CDI is not ready, deployment is in progress, or there are issues<br><br> A non-ready CDI effectively blocks VM provisioning and disk management. <br><br>When CDI is not ready, VM operations that require storage management will fail, including creating VMs from templates, importing images from URLs or registries, and cloning existing VM disks. | `kubevirt_cdi_cr_ready == 0` | High |
| `kubevirt_cdi_datavolume_pending` | Number of DataVolumes currently in pending state within the CDI system. This can happen for several reasons:<br>- Waiting for storage to be provisioned<br>- No suitable nodes available to run the importer pod<br>- Resource constraints preventing pod scheduling<br>- Waiting for dependencies or prerequisites<br><br> | `kubevirt_cdi_datavolume_pending > 5` | Medium |
| `kubevirt_cdi_import_pods_high_restart` | Number of CDI import pods that have experienced a high number of restarts. These restarts can suggest the pod is failing repeatedly and being restarted by Kubernetes, preventing successful completion of the import operation.<br><br>**Common Causes of High Restarts:**<br>- Network connectivity issues to image sources (timeouts, intermittent failures)<br>- Authentication problems with container registries or cloud storage<br>- Corrupted or incompatible source images<br>- Insufficient storage space for the imported image<br>- Storage performance issues causing import timeouts<br>- Resource constraints (CPU, memory) on import pods<br>- Proxy or firewall blocking image downloads<br>- SSL/TLS certificate validation failures<br><br> | `kubevirt_cdi_import_pods_high_restart > 0` | High |
| `kubevirt_cdi_operator_up` | Indicates whether the CDI operator is running and operational. This gauge metric indicates the operational status of the CDI operator:<br><br>**Metric values:**<br>- `1` = CDI operator is running and responding<br>- `0` = CDI operator is down, not responding, or not deployed<br><br>The CDI operator handles deploying and updating CDI controller, API server, and upload proxy amongst other things. | `kubevirt_cdi_operator_up == 0` | Critical |
| `kubevirt_cdi_upload_pods_high_restart` | Number of CDI upload pods that have experienced a high number of restarts. When users upload VM disk images directly to the cluster (rather than importing from external URLs), CDI creates upload pods that receive the uploaded data and write it to the target PersistentVolumeClaim. These restarts suggest the pod is failing repeatedly during upload operations, preventing successful completion of disk image uploads.<br><br>When upload pods have high restart counts, users cannot successfully upload custom VM images, ISO files, or disk images. This blocks workflows that depend on uploading proprietary or custom VM images that aren't available through external sources. | `kubevirt_cdi_upload_pods_high_restart > 0` | Medium |
| `kubevirt_cnao_cr_kubemacpool_aggregated` | The Cluster Network Addons Operator (CNAO) is responsible for deploying and managing networking components that extend OpenShift's networking capabilities for virtualization workloads. KubeMacPool provides MAC address pool management for VMs.<br><br>**Metric values:**<br>- `1` = KubeMacPool CR is healthy and operational<br>- `0` = KubeMacPool CR has issues or is not responding properly | `kubevirt_cnao_cr_kubemacpool_aggregated == 0` | High |
| `kubevirt_cnao_cr_kubemacpool_deployed` | This metric reports the deployment status of KubeMacPool.<br><br>**Common deployment issues:**<br>- Insufficient cluster resources preventing pod startup<br>- Network policies blocking webhook communication<br>- RBAC permission problems<br>- TLS certificate generation failures<br>- Conflicts with existing networking components<br><br>**Metric values:**<br>- `1` = KubeMacPool is successfully deployed and operational<br>- `0` = KubeMacPool deployment failed or is incomplete | `kubevirt_cnao_cr_kubemacpool_deployed == 0` | Critical |
| `kubevirt_cnao_kubemacpool_duplicate_macs` | Number of duplicate MAC addresses detected in the KubeMacPool system managed by CNAO. | `kubevirt_cnao_kubemacpool_duplicate_macs > 0` | Critical |
| `kubevirt_cnao_kubemacpool_manager_up` | Indicates whether the KubeMacPool manager component is running and operational. <br><br>**Metric values:**<br>- `1` = KubeMacPool manager is running and responding<br>- `0` = KubeMacPool manager is down or not responding | `kubevirt_cnao_kubemacpool_manager_up == 0` | Critical |
| `kubevirt_hco_hyperconverged_cr_exists` | Indicates whether the Hyperconverged Custom Resource exists. Non-existent CRs may affect hyperconverged functionality. If this is False, it may indicate issues with hyperconverged deployment or configuration. | `kubevirt_hco_hyperconverged_cr_exists == 0` | Critical |
| `kubevirt_hco_out_of_band_modifications_total` | Cumulative count of out-of-band modifications detected by the HyperConverged Cluster Operator (HCO) on its managed resources. The HCO is the top-level operator for OpenShift Virtualization that manages and coordinates all virtualization components. An "out-of-band modification" (OOB) occurs when someone or something changes these managed resources directly, bypassing HCO's control.<br><br>. This metric may be absent if there are no OOB modifications in your cluster | `increase(kubevirt_hco_out_of_band_modifications_total[1h]) > 0` | Medium |
| `kubevirt_hco_system_health_status` | Numeric health status of the Hyperconverged Operator. A value of 0 indicates issues with hyperconverged components that may affect virtualization functionality. If this metric returns 0, the HCO is reporting a problem requiring investigation. The HyperConverged CR creates corresponding CRs for the operators of all other components related to Compute, storage, networking, templating and scaling within OpenShift Virtualization. <br><br>**Metric Values**<br>- `1` The cluster operator is healthy<br>- `0` The cluster operator is unhealthy| `kubevirt_hco_system_health_status == 0` | Critical |
| `kubevirt_hco_unsafe_modifications` | Current count of unsafe modifications applied to HCO-managed resources through JSON patch annotations. While HCO normally prevents direct modifications to its managed components, it provides a mechanism for advanced users to apply "unsafe modifications" using JSON patch annotations on the HyperConverged CR. <br><br> This metric helps to uncover manual modifications to a cluster that might cause support issues in the event of failures. | `kubevirt_hco_unsafe_modifications > 0` | Medium |
| `kubevirt_hyperconverged_operator_health_status` | Health status of the Hyperconverged Operator. Non-healthy status may indicate issues with hyperconverged components. If this indicates a degraded status, it may affect hyperconverged functionality. | `kubevirt_hyperconverged_operator_health_status != 0` | High |
| `kubevirt_nodes_with_kvm` | Number of cluster nodes that have KVM (Kernel-based Virtual Machine) support available. For OpenShift Virtualization to run VMs efficiently, cluster nodes must have KVM support, which requires both hardware virtualization extensions (Intel VT-x or AMD-V) and the KVM kernel modules loaded. This gauge reports the count of nodes in the cluster that have the `devices.kubevirt.io/kvm` resource available, indicating they can run hardware-accelerated virtual machines. KubeVirt automatically detects KVM availability on each node and advertises it as a node resource. | `kubevirt_nodes_with_kvm < $(min_kvm_nodes)` | Medium |
| `kubevirt_number_of_vms` | Number of Virtual Machines managed by KubeVirt. Used to monitor VM deployment and scaling. If this drops suddenly, it may indicate VM termination or deployment issues. | `kubevirt_number_of_vms{namespace="default"} < 10` | Medium |
| `kubevirt_rest_client_request_latency_seconds_bucket` | Histogram buckets tracking the latency of REST API requests made by KubeVirt components to the Kubernetes API server. It includes all HTTP requests made by KubeVirt components to the Kubernetes API server, broken down by HTTP verb (GET, POST, PUT, PATCH, DELETE) and API endpoint.<br><br> 95th percentile latency consistently above 1 second indicates API server stress. Spikes in latency during VM creation suggest resource contention. Different latencies for different verbs help identify specific bottlenecks<br><br>May return NaN if KubeVirt components are not running or no API requests have been made. | `histogram_quantile(0.95, rate(kubevirt_rest_client_request_latency_seconds_bucket[5m])) > 1` | Medium |
| `kubevirt_rest_client_request_latency_seconds_count` | Total number of REST API requests made by KubeVirt components to the Kubernetes API server. This metric is a counter that increments for every HTTP request made by KubeVirt components to the Kubernetes API server. It provides visibility into the volume of API traffic generated by the virtualization infrastructure.<br>High request rates may indicate excessive API server load while sudden spikes could suggest issues with watch connections or control loops<br><br>**Troubleshooting with request count data:**<br>- High request rates: Check for stuck controllers or excessive polling<br>- Sudden drops: Investigate component connectivity or authentication issues<br>- Irregular patterns: May indicate network issues or API server problems<br> This metric may return NaN if no API requests have been made since startup. | `rate(kubevirt_rest_client_request_latency_seconds_count[5m]) > $(high_api_rate_threshold)` | High |
| `kubevirt_rest_client_request_latency_seconds_sum` | This counter tracks the total amount of time (in seconds) that KubeVirt components have spent waiting for responses from the Kubernetes API server. When combined with the count metric, it enables calculation of average request latency and provides insight into overall API performance impact.<br><br> This metric should always return some value. NaN would indicate a problem.  | `rate(kubevirt_rest_client_request_latency_seconds_sum[5m]) / rate(kubevirt_rest_client_request_latency_seconds_count[5m]) > 0.5` | Medium |
| `kubevirt_ssp_operator_reconcile_succeeded_aggregated` | Aggregated status indicating the overall health of SSP Operator reconciliation across all managed resources. While the individual reconcile metric shows the status of the most recent reconciliation attempt, this aggregated metric provides a comprehensive view of the SSP Operator's overall health across all the resources and components it manages. It considers the cumulative success rate and stability of reconciliation operations over time.<br><br>**Metric values:**<br>- `1` = Overall SSP reconciliation health is good<br>- `0` = Systemic issues with SSP reconciliation<br><br>**NaN:** If SSP Operator is not installed or insufficient data exists for aggregation. | `kubevirt_ssp_operator_reconcile_succeeded_aggregated == 0` | High |
| `kubevirt_ssp_operator_reconcile_succeeded` | Indicates whether the most recent reconciliation cycle of the Scheduling, Scale and Performance (SSP) Operator completed successfully. The SSP Operator deploys and maintains common VM templates, the template validator webhook, console proxy components, and other supporting infrastructure for virtualization workloads.<br><br> Reconciliation failures can lead to VM templates may become unavailable or outdated, template validation may stop working, allowing invalid VM configurations.  In some cases Console access to VMs may be disrupted along with metrics collection for VMs may be affected<br><br>**Metric values:**<br>- `1` = Last reconciliation cycle completed successfully<br>- `0` = Last reconciliation cycle failed.| `kubevirt_ssp_operator_reconcile_succeeded == 0` | High |
| `kubevirt_ssp_operator_up` | Indicates whether the SSP (Specialized Service Provider) operator, a core component of OpenShift Virtualization, is up and running. If not running (`0`), key virtualization features such as VM templates, common templates, and data import will not function. | `kubevirt_ssp_operator_up == 0` | Critical |
| `kubevirt_ssp_template_validator_up` | This metric is a **gauge** that reports the count of template validator webhook instances (typically pods) that are operational and responding to admission requests. In a typical OpenShift Virtualization deployment, the template validator runs as a deployment with multiple replicas for high availability. | `kubevirt_ssp_template_validator_up < 2` | Medium |
| `kubevirt_virt_api_up` | The number of virt-api pods that are up (not a binary indicator). The virt-api service is the main API endpoint for KubeVirt operations, handling all VM lifecycle requests. Multiple pods provide redundancy and high availability. | `kubevirt_virt_api_up < 1` | Critical |
| `kubevirt_virt_controller_leading_status` | Indicates whether a virt-controller pod currently holds the leadership lease and is actively managing VM operations. For high availability, multiple virt-controller pods run simultaneously, but only one can be the "leader" at any time. The leader is responsible for making decisions about VM management, while other pods remain on standby ready to take over if the leader fails.<br><br>**Metric values:**<br>- `1` = This virt-controller pod is the current leader<br>- `0` = This virt-controller pod is not the leader (standby) | `sum(kubevirt_virt_controller_leading_status) == 0` | Critical |
| `kubevirt_virt_controller_ready` | Number of virt-controller pods that are ready and operational.<br><br>**Minimum operational requirements:**<br>- At least 1 ready controller is required for VM operations to function<br>- 2+ ready controllers provide high availability and failover capability<br>- Zero ready controllers means complete VM management failure<br><br>**NaN:** If virt-controller deployment doesn't exist or no pods have been created. | `kubevirt_virt_controller_ready < 2` | Critical |
| `kubevirt_virt_controller_ready_status` | Individual readiness status indicator for each virt-controller pod. While `kubevirt_virt_controller_ready` provides the total count of ready controllers, this metric provides the individual readiness status of each controller pod. This granular view helps identify which specific pods are ready versus not ready, enabling more targeted troubleshooting.<br><br>**Metric values:**<br>- `1` = This specific virt-controller pod is ready<br>- `0` = This specific virt-controller pod is not ready | `kubevirt_virt_controller_ready_status == 0` | High |
| `kubevirt_virt_controller_up` | Number of virt-controller pods currently running. The virt-controller manages VM lifecycle operations (creation, migration, deletion). Multiple pods provide redundancy. A value matching your configured replica count (typically 2) indicates healthy operations. If this drops to 0, all VM operations will fail. | `kubevirt_virt_controller_up < 1` | Critical |
| `kubevirt_virt_handler_up` | Number of virt-handler pods currently running. Impact: Each node requires one virt-handler pod to manage VM operations. A value matching node count indicates full coverage. If lower, affected nodes can't manage VMs. | `kubevirt_virt_handler_up < count(kube_node_role{role="worker"})` | Critical |
| `kubevirt_virt_operator_leading` | Number of virt-operator pods that currently hold leadership status. This number should always be exactly 1. If it is greater than 1, the cluster will experience split-brain symptoms. If the number is 0 all services related to this will stop functioning. | `kubevirt_virt_operator_leading != 1` | Critical |
| `kubevirt_virt_operator_leading_status` | This metric provides the leadership status for each individual virt-operator pod, similar to how the virt-controller leading status works. It helps identify which specific pod is the current leader and enables monitoring of leadership transitions and stability.<br><br>**Metric values:**<br>- `1` = This virt-operator pod is the current leader<br>- `0` = This virt-operator pod is not the leader | `sum(kubevirt_virt_operator_leading_status) != 1` | Critical |
| `kubevirt_virt_operator_ready` | Number of virt-operator pods that are ready and operational. Since the virt-operator is the foundational component that manages all other KubeVirt components, having ready operator pods is essential for overall system health. Alert when ready count drops below expected number (usually 2); Monitor for frequent readiness state changes indicating instability | `kubevirt_virt_operator_ready < 1` | Critical |
| `kubevirt_virt_operator_up` | The number of virt-operator pods that are up. If this drops below 1 the operator will cease to function and changes will not be reconciled. | `kubevirt_virt_operator_up < 1` | Critical |
| `kubevirt_vm_error_status_last_transition_timestamp_seconds` | This gauge metric records the exact time (as a Unix timestamp) when a VirtualMachine transitioned into an error state. Unix timestamps count seconds since January 1, 1970 UTC.<br> This might be NaN if no VMs have ever entered error state, or if the VM has never experienced an error condition since metrics collection began. | `time() - kubevirt_vm_error_status_last_transition_timestamp_seconds < 300` | High |
| `kubevirt_vmi_cpu_system_usage_seconds_total` |  This is a counter that tracks the total time (in seconds) the VMI's vCPUs have spent executing code in system mode (i.e., inside the VM's guest OS kernel with system calls, interrupts, I/O, OS drivers). A healthy VM usually has more user CPU than system CPU. This metric is a counter: it only makes sense when observed as a rate over time. This is a low value metric except under specific circumstances. | `rate(kubevirt_vmi_cpu_system_usage_seconds_total[5m]) / ($num_vcpus) > 0.3`| Low |
| `kubevirt_vmi_cpu_usage_seconds_total` | Total CPU usage of Virtual Machine Instances (VMIs) in seconds. High values indicate CPU resource constraints. If CPU usage is consistently high, it may indicate the need for VM scaling or resource adjustments. | `rate(kubevirt_vmi_cpu_usage_seconds_total[5m]) > 100` | Medium |
| `kubevirt_vmi_cpu_user_usage_seconds_total` | This counter metric accumulates the total CPU time spent by the VMI's virtual CPUs executing user-space applications since the VMI started. It provides insight into how much computational work your applications are actually performing. <br><br>**IMPORTANT NOTE:** These metrics are for measuring the amount of CPU time which is not the same as %CPU usage in OpenShift Console| `sum by (namespace, name) (rate(kubevirt_vmi_cpu_system_usage_seconds_total[5m]) + rate(kubevirt_vmi_cpu_user_usage_seconds_total[5m])) / sum by (namespace, name) (kubevirt_vmi_vcpu_seconds_total) > 0.9` | Medium |
| `kubevirt_vmi_filesystem_capacity_bytes` | This metric reports the total storage capacity of filesystems mounted within the VirtualMachineInstance. Unlike PVC capacity (which is the raw storage allocated to the VM), filesystem capacity represents the usable space available to applications running inside the VM after accounting for filesystem overhead, reserved space, and formatting. This gauge metric shows the total size of all mounted filesystems within the VM, typically including the root filesystem and any additional mounted volumes. The value represents the maximum amount of data that can be stored on the VM's filesystems. A `0` or `None` value often indicates some sort of problem with the QEMU agent. | `kubevirt_vmi_filesystem_capacity_bytes < $(minimum_filesystem_capacity)` | Low |
| `kubevirt_vmi_memory_actual_balloon_bytes` | Current amount of memory (in bytes) that has been reclaimed by the memory balloon device in the VirtualMachineInstance. Memory ballooning is a virtualization technique that allows the hypervisor to reclaim unused memory from a running VM without stopping it. When the host needs more memory for other VMs or processes, it can ask the balloon driver inside a VM to "inflate" by allocating memory that becomes unavailable to applications in the VM. This memory is then available for the host to use elsewhere. When memory pressure decreases, the balloon can "deflate" and return memory to the VM.<br><br>This gauge metric shows the current amount of memory that the balloon device has reclaimed from the VM. A higher value means more memory has been taken away from the VM's applications and is available for host use.<br><br>**Performance implications:**<br>When memory is ballooned away from a VM:<br>- Applications have less memory available<br>- The VM may start using swap space, degrading performance<br>- Memory-intensive applications may experience slowdowns<br>- Overall VM responsiveness can be reduced<br><br>**NOTE:** Small amounts of ballooning (< 10% of VM memory) are typically normal | `kubevirt_vmi_memory_actual_balloon_bytes / kubevirt_vmi_memory_domain_bytes > 0.2`  | Low |
| `kubevirt_vmi_memory_available_bytes` | Available memory for Virtual Machine Instances (VMIs) in bytes. Low values indicate memory constraints. If available memory is low, it may indicate that VMIs are experiencing memory pressure. | `kubevirt_vmi_memory_available_bytes{namespace="default"} < 1e9` | Medium |
| `kubevirt_vmi_memory_domain_bytes` | This gauge metric shows the total memory allocation for the VM as configured in the VM specification. It represents the maximum amount of memory that the VM can use, corresponding to the memory allocation defined in the VirtualMachine resource. This is the memory resource that the VM "sees" and can potentially use for its operations, including the guest operating system, applications, and system processes.<br><br>**Memory domain vs other memory metrics:**<br>- **Domain memory**: Total memory allocated to the VM (this metric)<br>- **Resident memory**: Memory actually being used by the VM<br>- **Balloon memory**: Memory reclaimed by the balloon device<br>- **Available memory**: Domain memory minus balloon memory<br>- **Virtual memory**: Total memory as seen by applications (may exceed resident)<br><br>This metric typically corresponds to the memory request/limit configured in the VirtualMachine specification. It represents the memory that Kubernetes has allocated to the virt-launcher pod that runs the VM. | `kubevirt_vmi_memory_domain_bytes` | Low |
| `kubevirt_vmi_memory_pgmajfault_total` | Total number of major page faults for Virtual Machine Instances (VMIs). High values indicate memory pressure. If major page faults are increasing, it may indicate that VMIs are experiencing memory constraints. | `rate(kubevirt_vmi_memory_pgmajfault_total[5m]) > 0` | Medium |
| `kubevirt_vmi_memory_resident_bytes` | This guage metric is the amount of memory (in bytes) that is currently resident in physical memory for the VirtualMachineInstance. Resident memory, also known as RSS (Resident Set Size), represents the portion of a VM's memory that is currently loaded in physical RAM. This is the memory that is actively being used by the VM and is not swapped out to disk or ballooned away.<br><br>**What this metric measures:**<br>This gauge metric shows the actual amount of physical memory currently being used by the VM. It represents the "working set" of memory that the VM needs to operate efficiently without experiencing swap or page faults.<br><br> Resident memory metrics help determine:<br>- Actual memory requirements vs allocated memory<br>- Opportunities for memory optimization and cost reduction<br>- Node memory utilization patterns<br>- Appropriate memory allocation for similar workloads<br><br>You may want to track whether the amount of memory used is significantly less than what is requested.| `kubevirt_vmi_memory_resident_bytes / kubevirt_vmi_memory_domain_bytes`  | Low |
| `kubevirt_vmi_memory_swap_in_traffic_bytes` | Swap-in traffic for Virtual Machine Instances (VMIs) in bytes. High values indicate memory pressure. Swap-in refers to the process of moving data from swap space (usually disk storage) back into the VM's RAM | `rate(kubevirt_vmi_memory_swap_in_traffic_bytes[5m]) > 100` | Medium |
| `kubevirt_vmi_memory_swap_out_traffic_bytes` | Swap-out traffic for Virtual Machine Instances (VMIs) in bytes. High values indicate memory pressure. Sawp-out refers to the process of moving data from ram to swap space. If swap-out traffic is high, it may indicate that VMIs are experiencing memory constraints. | `rate(kubevirt_vmi_memory_swap_out_traffic_bytes[5m]) > 100` | Medium |
| `kubevirt_vmi_migrations_in_running_phase` | Number of VirtualMachineInstance Live Migrations that are currently in the running phase. This gauge metric shows the count of VMI migrations that are currently in progress (running phase). During migration, the VM continues to run on the source node while its memory and state are transferred to the destination node.<br><br>**Migration phases explained:**<br>A migration typically goes through several phases:<br>- **Pending**: Migration is scheduled but not yet started<br>- **Running**: Active migration in progress (this metric)<br>- **Succeeded**: Migration completed successfully<br>- **Failed**: Migration failed and was aborted<br><br>This may be a useful metric if you are experiencing a high number of live migrations. | `kubevirt_vmi_migrations_in_running_phase` | Low |
| `kubevirt_vmi_migration_succeeded` | Indicates whether the most recent migration of each VMI succeeded (1), failed (0), or has never occurred (absent). It is not a cumulative count. Used to check if any currently running VMs are stuck with failed migrations. <br><br>**Metric values:**<br>- `1` the most recent Live Migration has succeeded<br>- `0` A live migration has either failed or never occurred.| `kubevirt_vmi_migration_succeeded == 0`  | Medium |
| `kubevirt_vm_migrating_status_last_transition_timestamp_seconds` | Timestamp (in seconds since the Unix epoch) of when a VirtualMachine last entered the migrating state. This is a gauge metric, and it reports the most recent time (Unix timestamp format) when a VirtualMachine (VM) transitioned into the "migrating" state. The metric is present and set to a real timestamp value only while the VM is currently in the migrating state. If no VM is actively migrating, the metric's value is either missing (not present) or, in some implementations and environments, appears as 0. In all typical OpenShift Virtualization clusters, you will see zero for this metric for most VMs, most of the time, because migrations are rare/brief and this value is only updated and held during the active migration state.  After the migration completes, the metric is reset; for VMs not actively migrating, its value is either absent (no Prometheus data) or appears as zero due to how the exporter reports unpopulated gauges.| `time() - kubevirt_vm_migrating_status_last_transition_timestamp_seconds > 600` | High |
| `kubevirt_vm_non_running_status_last_transition_timestamp_seconds` | Unix timestamp (seconds since epoch) of when a VirtualMachine last transitioned to a non-running state. VirtualMachines can be in various states: Running, Stopped, Paused, Starting, Migrating, or Error. "Non-running" encompasses all states where the VM is not actively running and serving its intended workload. This includes planned stops, failures, and maintenance states.<br><br>**NOTE:** This metric may persist even if the VM no longer exists. | `time() - kubevirt_vm_non_running_status_last_transition_timestamp_seconds < 300`  | High |
| `kubevirt_vm_resource_requests` | Resource requests configured for VirtualMachines, broken down by resource type (CPU, memory, storage). This gauge metric shows the resource requests configured for each VirtualMachine, labeled by resource type. It reflects the minimum resources that Kubernetes will guarantee to be available for the VM to operate.<br><br>**QoS classes and resource requests:**<br>- **Guaranteed**: Requests equal limits (highest priority)<br>- **Burstable**: Requests less than limits (medium priority)<br>- **Best Effort**: No requests specified (lowest priority)| `kubevirt_vm_resource_requests{resource="memory"}`  | Low |
| `loki_query_frontend_queue_duration_seconds_sum` | Cumulative time (in seconds) that queries have spent waiting in the Loki query frontend queue. When queries are submitted to the Loki query frontend, they may need to wait in a queue before being processed by available queriers. This metric tracks the total amount of time all queries have spent waiting in the queue. This may be NaN if no queries have been processed, the queue is empty, or the metric is not being collected. | `rate(loki_query_frontend_queue_duration_seconds_sum[5m]) / rate(loki_query_frontend_queue_duration_seconds_count[5m]) > 10` | Medium |
| `loki_query_frontend_queue_length` | Current number of queries waiting in the Loki query frontend queue. The query frontend maintains a queue of queries waiting to be processed by available queriers. The queue length indicates how many queries are currently waiting for processing, providing real-time insight into system load and potential bottlenecks.<br><br>**Normal vs problematic queue lengths:**<br>- **0-5 queries**: Normal operation, adequate capacity<br>- **5-20 queries**: Moderate load, monitor for trends<br>- **20-50 queries**: High load, consider scaling<br>- **50+ queries**: Overload condition, immediate attention needed<br><br>This may be NaN if the query frontend is not running, the queue is not configured, or the metric is not being exposed. | `loki_query_frontend_queue_length > 20` | Medium |
| `loki_query_frontend_retries_bucket` | Histogram bucket counting how many times the Query-Frontend had to retry a sub-query before it succeeded. Retries normally occur when a querier or ingester is briefly unavailable. Sudden growth suggests instability in the read path, network hiccups, or an overloaded component. A NaN will appear if no retries have occurred during the time window. | `sum(rate(loki_query_frontend_retries_bucket[5m])) by (le)` | Low |
| `loki_query_frontend_retries_count` | Counter that tracks the total number of retries (all buckets combined). Good high-level SLO indicator for query reliability. Spikes after a platform upgrade or network flap are expected; sustained growth is problematic. | `rate(loki_query_frontend_retries_count[5m]) > 0` | Low |
| `loki_query_frontend_sharding_parsed_queries_total` | Counter incremented each time the Query-Frontend splits an incoming LogQL query for parallel execution (“sharding”). A flat line at 0 signals sharding is disabled; a sudden cliff suggests a mis-spin of query-range settings. | `increase(loki_query_frontend_sharding_parsed_queries_total[10m])` | Very Low |
| `loki_query_frontend_shards_total` | Number of active query shards in the Loki query frontend. In OpenShift Virtualization, large log volumes from VMs, nodes, and hypervisors demand efficient log query parallelization. Too few shards can bottleneck query performance, making VM troubleshooting or compliance investigations slow. Too many shards can cause overhead and resource waste. Monitoring this helps ensure log query responsiveness for critical virtualization events. | `loki_query_frontend_shards_total` | Medium |
| `loki_rate_store_expired_streams_total` | Counter of log streams whose rate records aged out of the automatic-stream-sharding “rate-store” cache. High churn (>10% of `loki_rate_store_streams` per hour) indicates bursty workload generators (e.g., VM live-migration logs). | `rate(loki_rate_store_expired_streams_total[1h])` | Very Low |
| `loki_rate_store_max_stream_shards` | Gauge of the highest shard count that Loki had to assign to a single stream to keep it below `desired_rate`. A rapid increase often flags a noisy neighbour or rogue namespace producing high-cardinality labels. | `max(loki_rate_store_max_stream_shards)` | Low |
| `loki_rate_store_max_unique_stream_rate_bytes` | Same as the previous metric **after** sharding—counts each shard separately. If this value remains above `desired_rate` even after sharding, rate limits will still drop lines. | `loki_rate_store_max_unique_stream_rate_bytes > $(max_per_stream_after_shard)` | Low |
| `loki_rate_store_stream_rate_bytes_bucket` | Histogram of per-stream byte rates. It tracks based on the `lokistack-distributor-*` pods and not the individual VMs themselves. Monitor the 95 percentile bucket during peak VM migration windows to size distributor CPU cores; NaN when a bucket gets no samples. | `histogram_quantile(0.95,sum(rate(loki_rate_store_stream_rate_bytes_bucket[5m])) by (le))` | Low |
|`loki_rate_store_stream_shards_count`| This metric represents the number of shards created through Loki's automatic stream sharding feature. Stream sharding is a mechanism that automatically splits large, high-volume log streams into smaller shards to prevent individual streams from exceeding rate limits. The metric shows how many total shards exist across all streams. A high shard count suggests either very active log sources or potential inefficiencies in labeling strategies. | `loki_rate_store_stream_shards_count > $(shard_count_threshold)` | Medium |
| `loki_rate_store_streams` | This metric tracks the total number of active log streams being processed by Loki's rate store mechanism. A stream is essentially a logical grouping of log entries that share the same set of labels. High stream counts can impact performance and storage requirements, while sudden changes might indicate new workloads, configuration changes, or potential issues. | `loki_rate_store_streams > $(stream_count_threshold)` | Medium |
| `loki_request_duration_seconds_count` | This metric counts the total number of HTTP requests processed by Loki components, providing essential visibility into the query and ingestion workload. This encompasses all API interactions including log ingestion from Promtail/Vector, LogQL queries from Grafana or the OpenShift console, and internal component communications. The metric is tagged with labels like `method` (GET, POST), `route`, and `status_code` for response types. This metric serves as a foundation for calculating request rates, identifying trending patterns, and detecting unusual spikes that might indicate issues or attacks. | `rate(loki_request_duration_seconds_count[5m]) > $(request_rate_threshold)` | Medium |
| `loki_request_duration_seconds_sum` | This metric accumulates the total time spent processing all HTTP requests across all Loki components, measured in seconds. Combined with the count metric, it enables calculation of average response times and performance analysis.  The sum represents cumulative processing time across all request types and components. This metric is crucial for performance monitoring because it reveals system efficiency and helps identify performance degradation. When combined with the count metric you get average response times. Poor performance here directly impacts user experience when troubleshooting VMs or analyzing infrastructure logs. | `rate(loki_request_duration_seconds_sum[5m]) / rate(loki_request_duration_seconds_count[5m])` | High |
| `loki_request_message_bytes_sum` | This metric tracks the cumulative size of all HTTP request payloads received by Loki, measured in bytes. Large message sizes could indicate verbose logging from applications, debug-level logging enabled inappropriately, or bulk log ingestion operations. This metric is essential for network bandwidth planning, storage capacity estimation, and identifying optimization opportunities in logging configurations. | `rate(loki_request_message_bytes_sum[5m]) > $(ingestion_bytes_threshold)` | Low |
| `loki_response_message_bytes_sum` | This metric accumulates the total size of all HTTP response payloads sent by Loki, measured in bytes. Large responses typically result from broad queries, long time ranges, or queries against high-volume log streams from busy VMs or applications. This metric is important for understanding query efficiency and network utilization. Consistently large responses might indicate inefficient queries that retrieve more data than necessary, poorly optimized LogQL queries, or users running expensive operations that should be refined. High response sizes can also impact user experience in the OpenShift console when loading log views. | `rate(loki_response_message_bytes_sum[5m]) > $(response_bytes_threshold)` | Low |
| `loki_ring_member_heartbeats_total` | This metric counts the total heartbeat messages sent by Loki components participating in the consistent hash ring, which is fundamental to Loki's distributed architecture. The ring membership determines how log streams are distributed across ingesters and how queries are routed across the cluster. A healthy cluster should show steady, regular heartbeat activity from all participating components. Abrupt changes in heartbeat patterns often precede more serious issues. | `rate(loki_ring_member_heartbeats_total[5m]) > $(s3_latency_threshold)` | High |
| `loki_runtime_config_last_reload_successful` | Indicates if the last Loki configuration reload was successful . Failed reloads may affect log collection patterns for VM logs, reducing visibility into virtualization problems. <br><br>**Metric Values**<br>- `1` Loki was successfully reloaded<br>- `0` Loki's last reload was not successful.| `loki_runtime_config_last_reload_successful == 0` | Medium |
| `loki_s3_request_duration_seconds_count` | This metric counts the total number of S3 storage requests made by Loki components. This encompasses all S3 operations including chunk uploads from ingesters, chunk retrievals during query processing, index file operations, and metadata management. The request count indicates how actively Loki is using the storage backend and can help identify patterns in storage utilization. When combined with error metrics, this metric provides a picture of storage health. | `rate(loki_s3_request_duration_seconds_count[5m]) > $(s3_request_rate_threshold)` | Medium |
| `loki_s3_request_duration_seconds_sum` | This metric accumulates the total time spent on all S3 storage operations, measured in seconds. Combined with the count metric, it enables calculation of average S3 operation duration and helps identify storage performance trends.  The sum metric helps identify whether storage is becoming a bottleneck in the system. When combined with the count metric, you get average response times for storage operations. In healthy systems, this should remain relatively stable and well below timeout thresholds. Increasing values indicate storage performance degradation, which could result from network issues, storage system overload, misconfigurated storage settings, or resource constraints. | `rate(loki_s3_request_duration_seconds_sum[5m]) / rate(loki_s3_request_duration_seconds_count[5m]) > $(avg_s3_duration_threshold)` | High |
| `loki_store_chunks_downloaded_total` | This metric tracks the total number of log data chunks downloaded from object storage (S3, GCS, etc.) by Loki queriers during query execution. When users query logs through Grafana, the OpenShift console, or LogQL APIs, queriers must download relevant chunks from storage to process the query. High values might indicate frequent queries accessing historical data, inefficient query patterns, or insufficient caching. The download process impacts query latency - more chunks downloaded means longer query times. Monitoring this helps identify query optimization opportunities and storage performance bottlenecks. If this metric shows unexpected spikes, it could indicate users running overly broad queries or caching issues requiring investigation. | `increase(loki_store_chunks_downloaded_total[5m]) > $(chunk_download_threshold)` | Medium |
| `loki_store_chunks_per_batch_sum` | This metric measures the cumulative number of chunks processed per batch operation by Loki's storage layer. Higher batch sizes generally improve throughput by reducing the number of individual storage operations, but may increase memory usage and latency for individual operations. When combined with batch count metrics, it reveals average batch sizes and processing patterns. | `rate(loki_store_chunks_per_batch_sum[5m]) / rate(loki_store_chunks_per_batch_count[5m]) > $(avg_batch_size_threshold)` | Low |
| `loki_store_series_total` | Total number of series in Loki's store. In virtualization environments, the number of log streams grows significantly with each VM, potentially causing high cardinality problems affecting query performance and storage. Monitoring this metric can help identify if VM logging is generating too many unique log streams, which could impact the performance of the entire logging stack. | `loki_store_series_total > 1000` | Medium |
| `loki_tcp_connections_limit` | This metric indicates the maximum number of TCP connections that Loki is configured to handle simultaneously. This is a configuration-driven limit that acts as a safeguard against resource exhaustion and helps maintain system stability under load. In OpenShift Virtualization environments this value is set to `0` meaning that there are no limits on this metric. | `loki_tcp_connections` | N/A |
| `loki_tcp_connections` | This metric shows the current number of active TCP connections established by Loki components. These connections include inter-component communication within the Loki cluster (between distributors, ingesters, queriers), connections to external dependencies (object storage, databases), and incoming connections from log shippers (Vector, Promtail) and query clients (Grafana, OpenShift console). This metric helps identify potential resource exhaustion before it impacts service availability. Sudden increases could signal DDoS attacks, misconfigured clients, or component malfunctions requiring immediate attention. | `loki_tcp_connections > $(high_connection_threshold)` | Low |
| `loki_tsdb_build_index_last_successful_timestamp_seconds` | Timestamp of the last successful TSDB index build in seconds. Impact: Outdated indexes may affect ability to search VM logs, hampering troubleshooting of virtualization issues. If this timestamp is stale (more than an hour old), recent VM migration or error logs may not be searchable. | `time() - loki_tsdb_build_index_last_successful_timestamp_seconds > 3600` | Medium |
| `loki_tsdb_head_series_not_found_total` | This metric counts the total number of times Loki's TSDB (Time Series Database) index failed to find requested log series in the in-memory head segment. The head segment contains the most recent index data before it's flushed to persistent storage. When a query requests data for a series that should exist but isn't found in the head, this counter increments. This can occur during normal operations when series haven't been seen recently, but high values might indicate index corruption, memory pressure, or configuration issues. | `increase(loki_tsdb_head_series_not_found_total[5m]) > $(series_not_found_threshold)` | Medium |
| `loki_tsdb_shipper_query_wait_time_seconds_count` | This metric counts the number of times queries had to wait for TSDB index files to be available from the shipper component. The TSDB shipper manages uploading local index files to object storage and downloading remote index files needed for queries. Wait times occur when requested index files aren't locally available and must be fetched. Frequent waits indicate either insufficient local caching, slow object storage, or query patterns accessing frequently uncached data. Trends in this metric help determine optimal cache sizing and identify when query patterns might benefit from pre-warming strategies. | `rate(loki_tsdb_shipper_query_wait_time_seconds_count[5m]) > $(query_wait_rate_threshold)` | Low |
| `loki_tsdb_shipper_query_wait_time_seconds_sum` | This metric accumulates the total time spent waiting for TSDB index files during query processing. Combined with the count metric, it enables calculation of average wait times and helps identify performance degradation trends. In OpenShift Virtualization, wait times directly impact user experience when accessing logs through Grafana dashboards or the OpenShift console. The average calculation will produce NaN if you have a count of `0` as you cannot divide by `0`. | `rate(loki_tsdb_shipper_query_wait_time_seconds_sum[5m]) / rate(loki_tsdb_shipper_query_wait_time_seconds_count[5m]) > $(avg_wait_time_threshold)` | Medium |
| `loki_tsdb_shipper_tables_download_operation_duration_seconds` | This metric measures the time taken to download complete TSDB index tables from object storage. Index tables contain the structured index data for specific time periods and are essential for query processing. Consistent increases in download times might indicate growing data volumes, storage performance degradation, or network issues.  | `loki_tsdb_shipper_tables_download_operation_duration_seconds > $(download_duration_threshold)` | Low |
| `loki_tsdb_wal_truncation_attempts_total` | This metric counts attempts to truncate (remove old entries from) the TSDB Write-Ahead Log (WAL). The WAL is a crucial component that ensures data durability by recording all write operations before they're committed to the main index. Regular truncation attempts indicate healthy WAL management, while failed attempts can lead to disk space exhaustion and ingestion failures. Failed truncation attempts can quickly lead to storage exhaustion, causing Loki to reject new log entries and potentially losing important VM and infrastructure logs. Monitoring truncation frequency and success rates helps ensure reliable log ingestion. Sudden changes in truncation patterns might indicate storage issues, configuration problems, or ingestion load changes requiring investigation. | `increase(loki_tsdb_wal_truncation_attempts_total[5m]) < $(min_truncation_attempts)` | High |
| `loki_write_failures_discarded_total` | Total count of log entries discarded due to write failures. Impact: May result in missing VM logs critical for troubleshooting virtualization issues and maintaining compliance. If increasing rapidly during VM migration activities, important logs about VM state transitions may be missing, making it difficult to diagnose failed migrations. | `increase(loki_write_failures_discarded_total[1h]) > 0` | High |
| `loki_write_failures_logged_total` | This metric tracks the total number of write operations that failed and were logged by Loki. Write failures can occur during log ingestion, index updates, or storage operations. Write failures are serious issues that can result in log data loss, affecting monitoring, troubleshooting, and compliance capabilities. Common causes include storage system failures, network issues, resource exhaustion, or configuration errors. Occasional failures might indicate underlying issues that could worsen over time.| `increase(loki_write_failures_logged_total[5m]) > 0` | Critical |
| `machine_cpu_sockets` | This metric indicates the number of physical CPU sockets present in the OpenShift node hardware. | `machine_cpu_sockets < $(expected_socket_count)` | Low |
| `machine_dimm_capacity_bytes` | This metric shows the total memory capacity of all DIMM (Dual In-Line Memory Module) modules installed in the OpenShift node, measured in bytes. This represents the raw physical memory available. It also includes the type of Ram (DDR3/4/5 etc)| `machine_dimm_capacity_bytes < $(min_memory_capacity)` | Low |
| `machine_dimm_count` | This metric represents the total number of physical DIMM modules installed in the OpenShift node.  | `machine_dimm_count < $(expected_dimm_count)` | Very Low |
| `machine_nvm_capacity` | This metric indicates the capacity of Non-Volatile Memory (NVM) modules in the system, measured in bytes and typically labeled by NVM mode (memory mode or app direct mode). NVM technologies like Intel Optane provide persistent memory capabilities that bridge the gap between traditional RAM and storage.  | `machine_nvm_capacity` | Very Low |
| `mapi_current_pending_csr` | Gauge showing how many node certificate-signing requests (CSRs) are **currently** waiting for approval. A growing count can delay new worker nodes or VM hosts from joining, impacting cluster capacity. | `mapi_current_pending_csr > $(csr_pending_threshold)` | High |
| `mapi_machinehealthcheck_short_circuit` | Indicates if machine health checks are short-circuited (disabled). When enabled (1), prevents automatic remediation of unhealthy nodes hosting VMs, potentially extending VM outages. During planned maintenance, this might be enabled, but if left enabled accidentally, it could prevent automatic recovery of nodes hosting critical VMs. | `mapi_machinehealthcheck_short_circuit > 0` | Medium |
| `mapi_machine_items` | This is a guage that reports the number of `Machine` API objects that the Machine API Operator (MAO) currently sees. Each `Machine` represents a node lifecycle-managed by MAO—physical, cloud VM, or bare-metal. In OpenShift Virtualization clusters this directly maps to the nodes that host KVM workloads, so a drop indicates possible node loss or etcd drift. | `mapi_machine_items != $(expected_machine_count)` | Low |
| `mapi_machine_set_status_replicas_available` | Counts replicas in the *Available* condition (machine is running, joined the cluster, and passes readiness probes). A MachineSet typically backs a worker pool such as `worker-0`. | `mapi_machine_set_status_replicas_available{namespace="openshift-machine-api",name=~".*worker-0"} != mapi_machine_set_status_replicas{namespace="openshift-machine-api",name=~".*worker-0"}` | Critical |
| `mapi_machine_set_status_replicas_ready` |  Similar to **available**, but *Ready* only reflects kubelet node‐ready status, not higher-level health. Use this when you want an early warning that nodes are present but not yet hosting workloads. | `mapi_machine_set_status_replicas_ready{namespace="openshift-machine-api"} < mapi_machine_set_status_replicas_available{namespace="openshift-machine-api"}` | Medium |
| `mapi_mao_collector_up` | Indicates if the Machine API Operator collector is up (1) or down (0). When down, affects machine health monitoring for nodes hosting VMs, potentially leaving hardware issues undetected. If down, problems with worker nodes hosting VMs may go undetected. <br><br>**Metric Values**<br>- `1` The Machine API Opertaor is up<br>- `0` The Machine API Operator collector is down.| `mapi_mao_collector_up == 0` | High |
| `mapi_max_pending_csr` | Constant gauge published by the Cluster Machine Approver. It is the maximum number of pending node certificate-signing requests (CSRs) allowed before the approver stops auto-signing. Administrators set this to throttle malicious or runaway bootstrap loops. | `mapi_current_pending_csr > mapi_max_pending_csr` | High |
| `mcc_drain_err` | Counter that increments every time the Machine Config Controller (MCC) cannot drain a node during an update. Drains are mandatory before NIC, kernel or kubelet changes that reboot the host. Repeated errors block upgrades and leave VMs on out-of-date kernels. | `increase(mcc_drain_err[5m]) > 0` | High |
| `mcd_kubelet_state` | Counter that increments when the Machine Config Daemon (MCD) finds the kubelet unhealthy. A rising count means the VM host’s kubelet may not register pods, impacting both VMs and container workloads. | `increase(mcd_kubelet_state[5m]) > 2` | High |
| `mcd_pivot_errors_total` | Counter for errors that occur while the MCD tries to *pivot* the OS to a new container image during upgrades (e.g., 4.18 → 4.19). Any increment can leave nodes stuck between OS versions, preventing virtualization patches. | `increase(mcd_pivot_errors_total[5m]) > 0` | Critical |
| `mcd_reboots_failed_total` | Total count of failed node reboots during machine config updates. May leave nodes in inconsistent states with partial updates to virtualization components. If a node fails to reboot during an update, VMs scheduled on that node may experience stability issues or incompatibility with the rest of the cluster. | `increase(mcd_reboots_failed_total[1h]) > 0` | High |
| `mcd_update_state` | Logs success or failure of configuration updates and the corresponding errors. The expected value is `rendered-master/rendered-worker-XXXX`. If the update fails, an error is present.  | `mcd_update_state{config!~".*rendered.*"}` | High |
| `mco_degraded_machine_count` | Count of machines in a degraded state according to the Machine Config Operator. Degraded machines may host VMs with reduced performance or with outdated virtualization components. If nodes hosting VMs are degraded, virtualization features like live migration may be compromised due to inconsistent configurations. | `mco_degraded_machine_count > 0` | High for Control Plane <br><br> Medium for Workers|
| `mco_machine_count` | Gauge equal to the number of nodes across all pools that the Machine Config Operator believes it manages. Sudden drops imply etcd desync or critical node loss. | `mco_machine_count < $(min_expected_nodes)` | Medium |
| `mco_state` | Machine Config Operator state transition timestamp. Reports the last time the MCO changed its status, as a UNIX epoch timestamp (seconds since 1970-01-01T00:00:00Z). This is not a health code. Alerting on this value directly does not convey health, but you can use it to detect when state has not changed in a long time, which may suggest a stuck update. For real MCO health status, use ClusterOperator conditions instead. | `mco_state` | Very Low |
| `mco_unavailable_machine_count` | Count of unavailable machines according to the Machine Config Operator. Unavailable machines cannot host VMs, reducing total virtualization capacity. | `mco_unavailable_machine_count > 0` | High for Control Plane <br><br> Medium for Workers |
| `metricsclient_request_retrieve` | Tracks successful Prometheus HTTP *retrievals* performed by the in-cluster `metrics-client` component (used by console \& dashboards). Spikes can indicate dashboard storms; drops to zero mean UI graphs will show *No Data*. | `rate(metricsclient_request_retrieve[5m]) < $(min_retrieve_rate)` | Low |
| `metricsclient_request_send` | Counts attempts by `metrics-client` to *send* metrics (for example, to Telemeter or external systems). A growing gap between **send** and **retrieve** often signals egress issues. | `rate(metricsclient_request_send[5m]) ` | Low |
| `metrics_server_api_metric_freshness_seconds_bucket` | Histogram of metric age delivered by Metrics Server. The Metrics Server powers HPA and VM resource graphs. Each bucket shows how stale the latest sample was when retrieved via the K8s metrics-server API. Values >60 s can break CPU \& memory auto-scaling for VMs and pods. | `histogram_quantile(0.99, rate(metrics_server_api_metric_freshness_seconds_bucket[5m])) > 60` | High |
| `metrics_server_kubelet_last_request_time_seconds` | Tracks the Unix timestamp of the last successful scrape the Metrics Server made against each node’s kubelet. If this value is stale, node-level CPU/memory data stops updating, which can break Horizontal Pod Autoscaler (HPA) calculations for pods and virtual machines. | `time() - metrics_server_kubelet_last_request_time_seconds > 120` | High |
| `metrics_server_manager_tick_duration_seconds_bucket` | Histogram counting how long the Metrics Server’s reconciliation “tick” loop takes. Longer ticks delay metric freshness. Look for the 99th-percentile bucket to ensure the controller is not overloaded. | `histogram_quantile(0.99, rate(metrics_server_manager_tick_duration_seconds_bucket[5m])) > 2` | Medium |
| `metrics_server_manager_tick_duration_seconds_count` | Counter of tick executions. Use alongside the histogram buckets to compute averages or detect missed cycles (no increments). | `increase(metrics_server_manager_tick_duration_seconds_count[10m]) == 0` | Very Low |
| `metrics_server_storage_points` | This gauge reflects the current number of time-series metric "samples" retained in the Metrics Server's in-memory storage cache. Each "point" is a timestamped observation for a specific metric (such as CPU usage for a VM or pod). A sudden drop in this value often signals a significant event: cache eviction, Metrics Server restart/crash, configuration reload, or an underlying memory-pressure event. This eviction will force the Metrics Server to drop old data and repopulate the cache only with new scrapes, potentially causing clients (like the autoscaler or the user console) to see sparse or "stale" resource data. | `delta(metrics_server_storage_points[5m]) < -100` | Low |
| `net_conntrack_dialer_conn_established_total` | This metric counts every successful outbound network connection established by a node or pod, as observed by the Linux kernel’s connection tracking (conntrack) in the Netfilter subsystem. In the context of OpenShift Virtualization, every time a VM, virt-launcher pod, or node process initiates a TCP/UDP connection out (e.g., DNS lookup, connecting to API servers, internal app traffic), a corresponding increment is recorded here—unless blocked by firewall, security group, or policy. Monitoring this metric helps quickly distinguish between complete network outage, policy misconfiguration, or selective reachability problems that can severely impact VM functionality or lifecycle operations (e.g., migration, image download). | `increase(net_conntrack_dialer_conn_established_total[5m])` | Medium |
| `net_conntrack_listener_conn_accepted_total` | Tracks the total number of TCP/UDP connection attempts accepted by processes listening on each node, as logged by conntrack. Every incoming SSH, API, or service port open is reflected here if successfully handed to a listener. A healthy OpenShift Virtualization cluster with active workloads should have regular traffic accepted by node, pod, and VM listeners. Sudden drops may reveal pod/node unavailability, application crash, firewall rule changes, or DoS protection triggers. | `increase(net_conntrack_listener_conn_accepted_total[5m])` | Medium |
| `network_attachment_definition_instances` | This metric exposes the current number of interfaces ("attachments") created using NetworkAttachmentDefinitions (NADs). More specifically this counts the number of pods in the cluster that have the `k8s.v1.cni.cncf.io/networks` annotation. Abrupt drops or sharp fluctuations in this metric strongly indicate catastrophic CNI misconfiguration, operator failure, or the accidental deletion of NAD CRDs—any of which can sever east-west (VM-to-VM) or north-south (VM-to-external) connectivity and cause mass outages. Proactive monitoring of this value ensures infrastructure changes do not unexpectedly break live or provisioned workloads. | `network_attachment_definition_instances` | Medium |
| `node_authorizer_graph_actions_duration_seconds_count` | Counts the total number of node-authorization (RBAC) checks performed, typically as nodes access pods or volume resources. This metric helps establish baseline activity for authorization checks. Identifies shifts in workload dynamics or flags changes in policy activity, allowing proactive analysis of authorization layer health. | `increase(node_authorizer_graph_actions_duration_seconds_count[5m])` | High |
| `node_authorizer_graph_actions_duration_seconds_sum` | This metric aggregates the total seconds spent evaluating node authorization actions over a period. When divided by the corresponding `count`, it gives the mean authorization latency. Sustained increases mean elevated response times from the control plane (often due to etcd performance, controller load, or RBAC graph complexity). This directly affects VM lifecycle events such as disk attach/detach, pod/VM creation, and even cluster-level operations like draining or upgrading nodes.  NaN may appear if there are too few samples in the selected range (common if activity is low or right after a component restart). | `rate(node_authorizer_graph_actions_duration_seconds_sum[5m]) / rate(node_authorizer_graph_actions_duration_seconds_count[5m]) > 1` | Medium |
| `node_boots_total` | Increments every time a node OS or kubelet reboots (planned upgrades, crash recovery, infra events). Tracking this counter allows early identification of instability (e.g., faulty kernel updates, underlying hardware faults, looped reboots). Surges after routine upgrades are expected; unscheduled spikes may require immediate investigation to maintain VM SLAs. | `increase(node_boots_total[1h])` | High |
| `node_boot_time_seconds` | Exposes the latest Unix timestamp when each node last booted. Use this as correlative evidence—pinpointing which nodes have rebooted recently, and to check that node boot events (expected or not) synchronously completed. Overly long uptime can also surface missing critical patches or infrastructure drift. When correlated with VM IO/timeouts, helps prove node root-cause. | `(time() - node_boot_time_seconds) / 60 /60 /24` | Low |
| `node_collector_evictions_total` | Increments every time a pod is evicted by Kubernetes due to resource pressure (memory, disk, PID).  For virtualization, eviction of VM pods (`virt-launcher`) interrupts or kills running VMs, and persistent resource pressure may degrade service or violate SLA. High rates here are among the most critical signals that a cluster is over-subscribed or not properly allocating node resources to VMs. Tracks both baseline cluster capacity health and the side-effects of events like upgrades or image pushes. | `increase(node_collector_evictions_total[10m])` | Critical |
| `node_collector_update_node_health_duration_seconds_bucket` | Measures, per bucket, how long the collector takes to update a node's health status, reflecting end-to-end propagation time (including cloud provider or cluster API). High percentiles indicate lag in reporting node condition changes upstream, creating a “blind spot” in HA features. This can delay VM failover, live-migration, or cluster capacity rebalancing—key risks in cloud and hybrid virtualization. | `histogram_quantile(0.99, rate(node_collector_update_node_health_duration_seconds_bucket[5m])) > 1` | Medium |
| `node_collector_zone_size` | Tracks how many nodes currently reside in each specific fault domain/zone. Ensures resilience for VM high availability and migration strategies. Sudden drops point to a zone outage—or removal/cordoning—threatening ability to maintain anti-affinity rules, migrate VMs, or meet public cloud SLAs. Critical for diverse workload or compliance needs. | `min by(zone) (node_collector_zone_size)` | Low |
| `node_context_switches_total` | This metric counts the number of times the Linux kernel switches between processes or threads (context switches) per node. High switching means a node is under CPU contention, with the kernel rapidly juggling work between many processes—this is typical with dense container or VM workloads. In OpenShift Virtualization, abnormal context switch rates are warning signs for "noisy neighbor" effects (oversubscription), kernel thread contention, hardware interrupts. | `rate(node_context_switches_total[5m])` |Medium|
| `node_cpu_core_throttles_total` | Total number of CPU throttling events on a node. May indicate CPU contention or thermal issues affecting VM performance and stability. High throttling can cause VM workloads to experience unpredictable performance, especially for latency-sensitive applications, and may indicate that a node is oversubscribed or experiencing cooling problems. | `rate(node_cpu_core_throttles_total[5m]) > 10` | Medium |
| `node_cpu_seconds_total` |  This metric tracks the total time (in seconds) each CPU core has spent operating in each mode since the node booted. High `iowait` can mean storage problems, while high `system` can signal kernel pressure from heavy disk/network/interrupt load. Use this metric for granular performance triage and trend dashboards. | `rate(node_cpu_seconds_total{mode!="idle"}[5m])` | Medium |
| `node_disk_discarded_sectors_total` | Cumulative count of disk sectors discarded (TRIM/UNMAP operations) on block devices. This tracks all filesystem or application requests to mark disk sectors as unused. Especially relevant on SSD/NVMe due to wear-leveling. High discard rates may indicate heavy VM churn (deleting disks or snapshots) or thin-provisioned storage regularly cleaned up. A sudden spike may correlate with mass VM deletions or a misbehaving storage operator. Monitoring helps detect storage backend issues, storage leaks, or misconfigured workloads. | `increase(node_disk_discarded_sectors_total[10m])` | Medium |
| `node_disk_flush_requests_time_seconds_total` | Sum of seconds waiting on disk flush requests (fsync, cache flush, barrier writes).  Flushing ensures all dirty buffers are safely written, critical for data durability and transactional consistency. Long or frequent flush times may degrade VM response (e.g., databases in VMs may stall), risk application data loss, or signal storage system saturation. Consistent monitoring helps storage design and profiling for IO-intensive guests. | `rate(node_disk_flush_requests_time_seconds_total[5m]) ` | Medium |
| `node_disk_io_now` | Count of disk IOs currently in progress for each device. Tells you how many read/write operations are outstanding “right now.” High sustained values mean the disk is overloaded or saturated, and can cause extreme latency for VM workloads (queueing delays, application timeouts). Very low values are expected during idle periods. Trend over time to distinguish between bursts and chronic contention. | `node_disk_io_now` | Low |
| `node_disk_io_time_seconds_total` | Total seconds where IO was in progress on the disk. Measures IO device busy time as a cumulative sum; the higher this grows, the more that disk was continuously handling IO. Key metric to target for performance tuning, backlog root-cause in VM complaints about disk latency. | `rate(node_disk_io_time_seconds_total[5m])` | Medium |
| `node_disk_io_time_weighted_seconds_total` | This metric is a cumulative, increasing counter that reflects both (a) how long the disk device is busy performing IO, and (b) the *intensity* of how many IO operations are simultaneously occurring on the disk. Rather than just measuring how many seconds the disk has been busy, this "weighted" metric multiplies each moment the disk is busy by **the number of outstanding requests at that moment**. For example: if a disk has 1 outstanding operation for 2 seconds, then 10 operations overlapping for 1 second, the counter would increase by 2 + 10 = 12 seconds. This detects not just slow disks, but *severe parallel disk contention* where VMs fight for storage—manifesting as slow boot, delayed app responses, or even timeouts. Conversely, a steady busy time but surging weighted busy time means the disk queue is backing up: the disk cannot keep up with host/VM demand. Compare this metric to raw busy time and IO queue metrics to pinpoint not just if the disk is slow, but whether *queue depth* (parallelism) or IO scheduler efficiency is the underlying cause.  | `rate(node_disk_io_time_weighted_seconds_total[5m])` | Medium |
| `node_disk_read_time_seconds_total` | Cumulative time spent servicing disk read operations. This metric is an increasing counter that tracks the total wall-clock time (in seconds) the storage device has been actively servicing read IO requests since the node booted or the metric reset. It's important to understand that this is **not** the time spent waiting for reads to complete, but rather the actual time the disk controller, drive heads, or SSD flash was busy reading data and transferring it back to the requesting process. By comparing this metric to `node_disk_write_time_seconds_total`, you can identify whether performance issues are read-bound (high read time, normal write time) or write-bound.<br><br> Compare with `node_disk_reads_completed_total` to calculate average read service time, and with `node_disk_io_now` to see if reads are queueing up. | `rate(node_disk_read_time_seconds_total[5m])` | Medium |
| `node_disk_write_time_seconds_total` | Cumulative seconds spent actively writing to disk. This metric is an increasing counter that records the total time (in seconds) the storage device controller has been occupied serving write requests since the node boot or last metrics reset. Every time data is written the disk spends measurable service time completing each IO. High or surging write time means the disk is under heavy write demand, common when VMs are:<br> a) persisting checkpoints or snapshots, <br>b) running high-write applications (e.g., databases, intensive file logging, CI/CD artifacts)<br> c) during live migrations where full disk state may be checkpointed to persistent storage.<br><br>  | `rate(node_disk_write_time_seconds_total[5m])` | Medium |
| `node_disk_written_bytes_total` | Total bytes written to storage since boot/reset. This counter grows as any data is written to block devices, whether by VM guests, node services, or OpenShift processes. It represents the sum of all successful write payloads. Monitoring the total written bytes is crucial for multiple reasons: <br>a) Wear tracking—especially<br>  b) Trend analysis for capacity planning<br> c) Identifying workload anomalies<br><br> In clusters hosting mission-critical or long-running VMs, this is a primary signal for forecasting disk replacement, enforcing quotas, or preemptively moving write-heavy workloads to more durable storage classes.| `increase(node_disk_written_bytes_total[1h])` | Medium |
| `node_edac_correctable_errors_total` | Count of memory ECC (Error-Correcting Code) correctable errors detected per node.Modern server hardware corrects most soft memory errors, but persistent high values signal failing DIMMs or unstable hardware, which threaten all running VMs with possible double-bit (uncorrectable) escalation or silent data corruption. Essential for node health and selecting safe VM placement. | `increase(node_edac_correctable_errors_total[24h])` | High |
| `node_edac_csrow_correctable_errors_total` | Pinpoints which memory bank/chiplet is experiencing soft errors. Not all errors are equal—a failing CSROW may impact only certain CPUs/VMs, allowing more targeted proactive maintenance. Use to guide hardware replacement or schedule safe VM migration off a “risky” node. | `increase(node_edac_csrow_correctable_errors_total[24h])` | High |
| `node_edac_csrow_uncorrectable_errors_total` | Total uncorrectable memory errors at CSROW (Chip-Select Row) level. May indicate hardware memory issues that could lead to VM crashes or corruption. Increasing errors may precede host crashes affecting running VMs, or cause silent data corruption inside VM memory that could affect application integrity. | `increase(node_edac_csrow_uncorrectable_errors_total[1h]) > 0` | Critical |
| `node_edac_uncorrectable_errors_total` | Total uncorrectable memory errors across the system. May indicate hardware memory issues that could cause VM crashes, data corruption, or host failures. Even a single uncorrectable memory error could potentially crash VMs or cause data corruption within VM memory, making this a critical metric to monitor on virtualization hosts. | `increase(node_edac_uncorrectable_errors_total[1h]) > 0` | Critical |
| `node_entropy_available_bits` | Available kernel entropy for random number generation (e.g., `/dev/random`). Shows the number of random bits ready for use by cryptographic and security systems. | `node_entropy_available_bits` | Low |
| `node_fibrechannel_dumped_frames_total` | Frames discarded by the Fibre Channel (FC) host adapter. “Dumped” means the adapter had to drop the frame before transmission, usually due to buffer overflow or link negotiation failure. In VM environments using FC storage (SAN LUNs for VM disks), dumped frames translate to I/O retries and latency spikes. Monitor per‐HBA to catch zoning, cable, or SFP issues. | `increase(node_fibrechannel_dumped_frames_total[5m]) > 0` | High |
| `node_fibrechannel_error_frames_total` | Frames received with general protocol errors (framing, encoding, header problems). Indicates link noise or mis-matched speed/duplex. Even small but growing counts can corrupt VM disk writes or cause multipath failover storms. | `increase(node_fibrechannel_error_frames_total[5m]) > 0` | Critical |
| `node_fibrechannel_invalid_crc_total` | CRC-failed Fibre Channel frames. Each FC frame carries a cyclic-redundancy check; non-zero increments reveal physical-layer corruption (bad optics, dirty connectors, bending fiber). Persistent growth mandates cable/SFP replacement to avoid silent data corruption in VM volumes. | `rate(node_fibrechannel_invalid_crc_total[5m]) > 0` | Critical |
| `node_fibrechannel_invalid_tx_words_total` | Invalid 10-bit transmission characters detected. FC uses 8b/10b encoding; invalid “words” suggest severe signal integrity problems. When this counter climbs, expect link resets, path down events, and heavy I/O retry storms visible inside guest OSes. | `increase(node_fibrechannel_invalid_tx_words_total[5m]) > 0` | Critical |
| `node_fibrechannel_link_failure_total` | Total Fibre Channel link failures detected. May affect VM access to storage on SAN, causing I/O errors for VMs using FC-based persistent volumes. If increasing, VMs using Fibre Channel storage may experience I/O errors. | `increase(node_fibrechannel_link_failure_total[1h]) > 0` | High |
| `node_fibrechannel_loss_of_signal_total` | Total Fibre Channel signal loss events. May affect VM access to storage on SAN, causing I/O timeouts for VMs using FC storage. If increasing, VMs using Fibre Channel storage may experience I/O timeouts. | `increase(node_fibrechannel_loss_of_signal_total[1h]) > 0` | High |
| `node_fibrechannel_loss_of_sync_total` | Total Fibre Channel synchronization loss events. May affect VM access to storage on SAN, causing intermittent I/O issues. If increasing, VMs using Fibre Channel storage may experience intermittent I/O issues, leading to application performance degradation or increased latency. | `increase(node_fibrechannel_loss_of_sync_total[1h]) > 0` | High |
| `node_fibrechannel_nos_total` | Number of “Not Operational Sequence” (NOS) events. NOS is sent when a receiver detects loss of sync; the link must recover before traffic resumes. Each event equals a momentary storage path outage; frequent NOS indicates failing optics or incompatible HBAs/switch ports. | `increase(node_fibrechannel_nos_total[10m]) > $(nos_event_threshold)` | Critical |
| `node_filefd_allocated` | Allocated kernel file descriptors. Linux limits open files per node; every socket, pipe, and disk file consumes one FD. VM launchers, QEMU processes, and SDN components open thousands. Close to the system cap, new pods/VMs fail. Track percentage of `node_filefd_allocated / node_filefd_maximum`. | `node_filefd_allocated / node_filefd_maximum > 0.90` | High |
| `node_filesystem_avail_bytes` | Available bytes in the node's filesystem. Low values may affect local VM storage, container images, and ephemeral disks used by VMs. If a node runs out of filesystem space, VM creation may fail, ephemeral disks may become full causing VM application errors, and VM image pulling may fail. | `node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"} < 0.10` | High |
| `node_filesystem_files_free` | Free inodes remaining. Used with the above metric to compute inode exhaustion risk. Sharp drops during CI/CD or logging bursts warrant action before virtual disks or container layers hit “no space on device (inodes)” errors. | `node_filesystem_files_free / node_filesystem_files < 0.10`|	Medium|
| `node_filesystem_files` | Total inodes available per filesystem. Inodes cap how many files you can create even if space is free. VM image layers, container logs, and overlayfs tmpdirs burn inodes quickly. Low headroom stops pod & VM logs, temp file creation, and package installs. | `node_filesystem_files_free / node_filesystem_files < 0.10` | Medium |
| `node_filesystem_free_bytes` | Free space in bytes in the node's filesystem. Critical for VM storage operations, as low free space can prevent VM creation, snapshot operations, and disk image transfers. VM disk I/O performance also degrades as filesystems approach capacity, affecting running workloads. | `node_filesystem_free_bytes{mountpoint="/var"} / node_filesystem_size_bytes{mountpoint="/var"} < 20` | Critical |
| `node_hwmon_temp_crit_celsius` | Critical temperature threshold for hardware components in Celsius. Helps determine thermal headroom before VM workloads are affected by throttling. When planning to deploy VMs with high CPU utilization patterns (ML workloads, database servers), compare current temperature against this critical threshold to ensure adequate thermal margin. | `(node_hwmon_temp_celsius / node_hwmon_temp_crit_celsius) > 0.8` | Medium |
| `node_infiniband_excessive_buffer_overrun_errors_total` | Receiver buffer overrun errors. This counter tracks the total number of times the InfiniBand Host Channel Adapter (HCA) on the node failed to deliver incoming data to system memory fast enough—meaning receive buffers filled up before the NIC DMA engine could process them. When this happens, packets are dropped at the hardware layer before the OS or any VM ever sees them. Each overrun forces upper-layer retransmits, increasing latency, and causing loss of throughput. Overruns are usually caused by overly aggressive workload bursts, poorly tuned queue sizes, IRQ/SRIOV interrupt bottlenecks, or hardware that is not keeping up  | `increase(node_infiniband_excessive_buffer_overrun_errors_total[5m]) > 0` | High |
| `node_infiniband_link_downed_total` | Total number of times an InfiniBand link went down. InfiniBand is often used for high-performance VM workloads requiring low-latency networking. Link failures directly impact VM network performance and can cause application errors within VMs. | `increase(node_infiniband_link_downed_total[1h]) > 0` | High |
| `node_infiniband_link_error_recovery_total` | Count of link error recovery events. This metric increments every time the HCA detects a link error and successfully performs a recovery, restoring the link to an operational state. Recovery is a disruptive transient. Traffic is interrupted, latency spikes, and at worst, active RDMA operations are aborted. Consistent increases often precede full link flaps. High rates signal deteriorating cable, fiber, switch ports, SFPs/DACs, or environmental interference. | `increase(node_infiniband_link_error_recovery_total[5m])` | High |
| `node_infiniband_local_link_integrity_errors_total` | Local link integrity error counter. This accumulates every time the node’s HCA detects a failed integrity check on the inbound InfiniBand link. Even a small, rising count almost always points to physical connector issues. On RoCE (Ethernet-based RDMA), it can also reflect VLAN tagging errors or interference from broadcast/multicast storms.  `rate(node_infiniband_local_link_integrity_errors_total[5m]) > 0` | High |
| `node_infiniband_port_constraint_errors_received_total` | Inbound constraint error counter. This metric counts violations reported by the remote InfiniBand fabric (the switch or another HCA) when frames were received from your node that failed to meet fabric requirements. Such violations degrade or block RDMA traffic, break multi-host VM replication, and can cause fabric congestion. A climbing value is almost always a consequence of recent firmware/drivers, human error in fabric policy, or incompatibility across vendor hardware.| `increase(node_infiniband_port_constraint_errors_received_total[5m]) > 0` | Medium |
| `node_infiniband_port_constraint_errors_transmitted_total` | Outbound constraint error counter. This counts the times your node attempted to send frames which the InfiniBand switch or another node rejected due to violating fabric configuration. Incrementing this often means local configuration drift: an OS/fabric update or misaligned driver version. Most fabrics will tolerate a few constraint violations, but frequent offenders may trigger switch-side port shutdown or fabric quarantine. | `increase(node_infiniband_port_constraint_errors_transmitted_total[5m]) > 0` | Medium |
| `node_infiniband_port_discards_transmitted_total` | Outbound packet discards. Measures the number of packets the node’s HCA discarded before sending—this happens when requested operations are illegal , credits are exhausted, or when other hardware protections are triggered. High discard rates mean actual application or storage operations are silently failing or backing off. Often remedied by flow-control tuning, increasing fabric credit pools, or finding and fixing congestion points. | `increase(node_infiniband_port_discards_transmitted_total[5m]) > 0` | Medium |
| `node_infiniband_port_errors_received_total` | Total count of InfiniBand port errors received. Affects network reliability for VM workloads using InfiniBand for high-performance computing or storage access. Even small error rates can degrade performance of latency-sensitive VM applications. If this metric increases for nodes hosting database VMs using InfiniBand for storage access, those VMs may experience intermittent I/O errors leading to database corruption or performance issues. | `rate(node_infiniband_port_errors_received_total[5m]) > 0` | Medium |
| `node_infiniband_port_receive_remote_physical_errors_total` | Remote physical errors on IB port. This counter tracks when the InfiniBand (IB) port on the node receives a packet flagged with a remote “physical error” indication by a switch or end device. Causes include fiber/cable faults, bad optics, or severe environmental noise. Errors here can break or degrade RDMA traffic used by VM disk/state transfers. Even intermittent increases warn you of underlying link instability, risking packet loss, retransmits, or full interface outages. | `increase(node_infiniband_port_receive_remote_physical_errors_total[5m]) > 0` | High |
| `node_infiniband_port_receive_switch_relay_errors_total` | Switch relay error counter. Tracks the number of packets received with switch relay error status, typically due to misrouted, misframed, or fabric-congestion events at the IB switch. These errors often indicate switch config drift, congestion, or physical pathing errors in high-performance fabrics underlying OpenShift Virtualization networks.  | `increase(node_infiniband_port_receive_switch_relay_errors_total[5m]) > 0` | Medium |
| `node_infiniband_port_transmit_wait_total` | Cumulative transmit-wait (blocked send) time. Tracks the time (in seconds) an IB port spent unable to transmit because of insufficient destination credit. In busy VM environments, or when a remote HCA/switch is congested/failing, transmit wait rises and the VM traffic is throttled. High values signal either network saturation or misconfigured credit pools and result in high-latency (slow migration/storage/VM access). | `rate(node_infiniband_port_transmit_wait_total[5m])` | Medium |
| `node_infiniband_symbol_error_total` | Cumulative symbol errors detected on IB port. Symbol errors occur when bit streams on the fiber are corrupted due to electrical/optical problems, dirty connectors, bad cables or high EM/RF noise. Any increase can precede widespread packet errors, failing links, or “flapping” ports. Persistent symbol errors warrant immediate hardware cleaning or replacement. | `increase(node_infiniband_symbol_error_total[5m]) > 0` | High |
| `node_infiniband_vl15_dropped_total` | Dropped management packets on IB virtual lane 15. VL15 is reserved for IB subnet management. Dropped VL15 packets may indicate switch overload, fabric storm, or controller crash. Management packet loss can delay or break network auto-healing, address assignment, or fabric topology discovery. | `increase(node_infiniband_vl15_dropped_total[5m]) > 0` | Medium |
| `node_ipvs_incoming_packets_total` | **NOTE:** IPVS is not the default in OpenShift. This is likely to have not value. Incoming IPVS packet counter (Linux IP load balancing). Tracks the total number of packets received by the kernel’s IP Virtual Server (IPVS), which powers kube-proxy in IPVS mode. Drops/spikes in this metric can expose network outages, DDoS, or kube-proxy misconfiguration, risking application or VM connectivity. | `rate(node_ipvs_incoming_packets_total[5m])` | Very Low |
| `node_ipvs_outgoing_bytes_total` | **NOTE:** IPVS is not the default in OpenShift. This is likely to have not value. Total bytes sent out by IPVS. Measures all data bytes transmitted by the IPVS kernel load-balancer, a proxy for all traffic sent to backend pods and VMs.  | `rate(node_ipvs_outgoing_bytes_total[5m]) > $(ipvs_out_bps_warn)` | Very Low |
| `node_load1` | System load over 1 minute: Measures the average number of runnable and uninterruptible processes, indicating demand for CPU time over the past minute. Because load averages count processes regardless of the number of CPUs, dividing the raw load by the number of CPU cores normalizes the value to a per-CPU scale. This normalized metric reveals true CPU usage pressure: when the ratio exceeds 1, the node cannot schedule all runnable processes immediately, meaning work is queued, causing latency for VMs and containers. .  | `node_load1 / on(instance) instance:node_num_cpu:sum` | Medium |
| `node_md_state` | Linux MD (RAID) status indicator. This metric emits one time series per array per possible RAID state, each uniquely labeled with a `state` tag: e.g., `active`, `inactive`, `check`, `recovering`, `resync`. <br><br>**Metric Values**<br>- `1` Indicates the current state of a given MD Array is *active*<br>- `0` the indicated state(s) are inactive<br><br> For OpenShift Virtualization, any state other than `active` can indicate underlying storage issues—an inactive array, a syncing or recovering array, or one performing a check may reduce I/O throughput or reliability for any VM backed by that volume. Monitoring these labels allows accurate, state-specific alerting and historical dashboards. | `node_md_state{state!="active"} == 1` | High |
| `node_memory_AnonPages_bytes` | Anonymous memory pages in bytes. This metric represents the total amount of physical RAM currently occupied by "anonymous pages"—areas of memory not backed by any file, such as dynamically allocated stack and heap for processes. In Linux, these pages are created through system calls like `malloc` or `mmap`, and only become realized in RAM when actually used by a process. For KVM/QEMU virtual machines, `AnonPages` is the main measure of guest VM memory consumption truly resident in RAM, not memory that is cached, buffered, or mapped to files. High values lead to memory pressure, increased swap activity, and raise the risk of the Linux kernel triggering Out-Of-Memory (OOM) kills—resulting in VM or pod evictions. | `(node_memory_AnonPages_bytes / node_memory_MemTotal_bytes > 0.85)` | High |
| `node_memory_Cached_bytes` | File system cache memory usage. Shows RAM spent caching files and directory metadata. Higher values typically indicate a healthy cache. Suddenly dropping cache, or high memory pressure paired with low cache, may slow VMs or signal impending OOM. Used to identify inefficiencies in disk IO or hint at memory under- or over-provisioning. | `node_memory_Cached_bytes` | Low |
| `node_memory_CommitLimit_bytes` | System-wide kernel configuration indicating the theoretical maximum memory allocation limit (physical RAM + swap × overcommit ratio). For OpenShift Virtualization, this metric has minimal impact on actual VM placement decisions, as scheduling is based on physical RAM and memory requests, not the commit limit. | `node_memory_CommitLimit_bytes / (1024*1024*1024)` | Low |
| `node_memory_Committed_AS_bytes` | Committed “address space” across all node processes. Sums all memory committed via malloc/mmap, even if not immediately resident in RAM. Reflects the potential peak memory demand if every process used all it pledged. In environments with KSM or memory overcommit (common in VM clusters), it tracks how much risk you have of eventually running out of RAM if all VMs/applications “touch” their entire allocation. Surges here forewarn real OOM conditions, even before RAM is exhausted. | `node_memory_Committed_AS_bytes / node_memory_MemTotal_bytes > 1.1` | High |
| `node_memory_Dirty_bytes` | Bytes in RAM modified, not yet written back to disk ("dirty pages"). These dirty pages accumulate when applications or the OS write to files or storage-backed memory, but those changes haven’t been physically flushed to disk by the kernel’s writeback process yet. In OpenShift Virtualization, VMs and pods often drive substantial disk IO leading to naturally fluctuating dirty memory. Moderate, transient dirty bytes are normal. Persistent or growing dirty bytes, however, indicate that disk writeback can’t keep up, often due to storage bottlenecks, saturated disks, or misbehaving workloads. This can cause VM IO timeouts, high latency, and (if the node crashes) possible data loss for any not-yet-persisted files. | `rate(node_memory_Dirty_bytes[5m])` | Low |
| `node_memory_HugePages_Total` | Total number of configured HugePages. HugePages are pre-allocated at boot for large-memory VMs or workloads (like databases) to reduce page table overhead and improve TLB efficiency. KubeVirt/VM pods requiring hugepages (for low-latency or high-performance) will fail to schedule if not enough pages are reserved. This metric lets you confirm cluster tuning and ensure you meet your VM workload specifications. Drops can reflect node tuning problems, misconfig, or competing workloads fragmenting RAM and causing VM launches to fail. | `node_memory_HugePages_Total` | Low |
| `node_memory_MemAvailable_bytes` | Amount of memory available for new processes, in bytes. This is the best single gauge of how much RAM is actually available for applications, containers, and VMs, accounting for both truly free memory and memory that can be quickly reclaimed (from buffers, page cache, and slabs). If MemAvailable drops too low, the risk of swapping or Out-Of-Memory (OOM) kills increases. | `node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes < 0.10` | High |
| `node_memory_MemFree_bytes` | Amount of physical RAM left unused by the system. Directly affects VM placement, migration capabilities, and performance under memory pressure. Low free memory can trigger swapping which severely impacts VM performance. Existing VMs may experience performance degradation as the hypervisor struggles to allocate memory pages. <br> <b>NOTE:</b> this does not take into account buffers, cache or overall available memory!| `node_memory_MemFree_bytes / node_memory_MemTotal_bytes` | Medium-Low |
| `node_memory_Mlocked_bytes` | Number of bytes of memory locked (mlocked) into RAM by applications. mlocked memory cannot be swapped out, even under memory pressure. Secure workloads (cryptography, etc.) may mlock pages to prevent sensitive data from hitting disk. Excessive mlock use reduces flexible RAM, raising OOM risk for VMs. | `node_memory_Mlocked_bytes` | Low |
| `node_memory_PageTables_bytes` | Bytes of RAM currently consumed by process page tables. Every process, pod, and VM gets its own set of page tables, which map virtual memory addresses (what apps see) to physical RAM (what the node provides). The more processes or the bigger the memory allocations. High rates of growth may signal pathological changes—like a memory leak in a VM, or a job that’s forking uncontrollably. | `rate(node_memory_PageTables_bytes[10m])` | Low |
| `node_memory_SecPageTables_bytes` | Bytes used for secondary page tables. These are additional memory structures for address translation, sometimes used in advanced memory or virtualization scenarios. Large, persistent growth may indicate heavy paging or specific kernel memory features. | `rate(node_memory_SecPageTables_bytes[15m]) ` | Low |
| `node_memory_Shmem_bytes` | Shared memory usage in bytes. This metric shows the total size of all shared memory (shm) segments currently held in node RAM. Shared memory is a Linux mechanism allowing multiple processes (including containers and VMs) to access the same physical memory region. VM guest agents, applications inside VMs, or sidecar containers may rely on shared memory regions for efficient data exchange. High or rapidly growing Shmem can signal:<br> (a) many active VMs using in-memory databases, <br>(b) large shared-memory buffers for application-level caching <br> (c) accidental or malicious apps filling up tmpfs-backed volumes<br><br> Persistent high usage increases the risk of node OOM, as shared memory is not easily reclaimed unless processes free it. | `(node_memory_Shmem_bytes / node_memory_MemTotal_bytes) > 0.40` | Medium |
| `node_memory_Slab_bytes` | Kernel slab allocator memory in bytes. The slab allocator manages caches of frequently-used kernel objects, such as dentry/inode structures, network buffers, pipes, and VM/process descriptors. Slab memory should fluctuate with node workload, IO, and process churn. If the slab grows steadily and persistently high (especially if it approaches 15–20% of total node RAM), it may indicate: <br>(a) intense aggregate IO or process churn, <br>(b) kernel tuning needed <br> (c) exceptionally rare, a "slab leak" from a kernel module bug or driver.<br><br> Slab memory, while reclaimable, can crowd out available RAM for VMs. | `node_memory_Slab_bytes / node_memory_MemTotal_bytes > 0.15` | Medium |
| `node_memory_SUnreclaim_bytes` | Unreclaimable slab memory (bytes). This is the part of the slab allocator that the kernel cannot reclaim under memory pressure. In a healthy node, unreclaimable slab is a small fraction of total slab. If `SUnreclaim` climbs and stays high, free memory for pods and VMs is reduced, possibly due to kernel code or driver bugs, leaking kernel objects (rare), or pathological system workload. Monitored alongside total slab, it helps distinguish between normal cache and persistent kernel "pinning." | `(node_memory_SUnreclaim_bytes / node_memory_Slab_bytes) > 0.6` | Medium |
| `node_memory_Unevictable_bytes` | Memory the kernel cannot reclaim (“unevictable” memory), in bytes. Includes mlocked pages and memory in special filesystems like ramfs, as well as any pages “stuck” due to kernel bugs or long-lived pinning. High unevictable usage shrinks the scheduling headroom for new pods/VMs and raises the risk of forced OOM kills. This can happen if processes abuse mlock, ramfs is over-used, or VMs/pods misconfigure in-memory storage. | `(node_memory_Unevictable_bytes / node_memory_MemTotal_bytes) > 0.05` | Medium |
| `node_memory_VmallocUsed_bytes` | Kernel virtual memory (`vmalloc`) usage in bytes. This metric exposes the total bytes allocated using the kernel’s `vmalloc` allocator. Unlike normal RAM allocations, `vmalloc` is for dynamically allocating large or fragmented memory regions required by some kernel modules, device drivers, and certain hardware buffers. Modern Linux kernels use `vmalloc` for PCI device memory mappings, some storage/network controller buffers, or kernel modules that cannot use physically contiguous memory. While rare in OpenShift Virtualization, persistent growth or sudden spikes in `vmalloc` usage could signal a misbehaving kernel module, buggy device driver, or hardware feature that allocates excessive virtual memory, risking kernel instability or memory leaks. For most VM and pod workloads, `vmalloc` should remain low and stable. Sudden growth is a reason to inspect dmesg/kern.log and identify problematic drivers or recent node hardware changes. | `rate(node_memory_VmallocUsed_bytes[10m]) > 40000000` | Low |
| `node_memory_Writeback_bytes` | RAM currently being “written back” to disk (writeback cache). This metric tracks how much data is staged in RAM, queued by the kernel’s writeback mechanism, waiting to be flushed to physical storage. In OpenShift Virtualization, VMs and node workloads frequently write to disk; the kernel buffers these writes to improve performance, but the cache must be cleared at intervals to keep data durable and free up RAM. Brief bursts are normal following heavy writes, but persistently high or rising writeback indicates storage is not keeping up—often a sign of saturated disks, failing storage, storage controller queue exhaustion, or NFS/file backend issues. This is a leading indicator for disk bottlenecks that will cause applications and especially VMs to stall or experience IO timeouts. | `(rate(node_memory_Writeback_bytes[5m]) > 1000000)` | Medium |
| `node_memory_Zswapped_bytes` | **NOTE: Zswap not enabled in OpenShift 4.19 or older.**<br> Bytes currently held in compressed swap (zswap) cache. Zswap is an in-kernel compressed swap cache which tries to hold recently swapped-out pages in RAM (compressed) before resorting to writing them to traditional disk swap. Growing or persistently high `Zswapped` hints the node has insufficient RAM for its active workload: VMs, pods, or node processes are exceeding available memory, forcing the kernel to compress and swap out pages to eke out more headroom. If this metric continues to rise, you are likely to see actual swap IO to disk (which can devastate VM and pod latency), or soon after, OOM kills if memory runs out entirely. Spikes can also reveal runaway memory use or unexpectedly high memory consumption by VMs and workloads. | `(node_memory_Zswapped_bytes / node_memory_MemTotal_bytes) > 0.05` | Medium |
| `node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate` | Aggregated rate of container CPU usage (across namespace, pod, container). This metric captures how much CPU time is being burned, per namespace/pod/container, averaged over a small window. Useful for identifying hot pods/VMs, balancing scheduling, and detecting resource contention. Consider normalizing by total available node CPU to get % utilization. | `sum by (namespace,pod) (rate(container_cpu_usage_seconds_total{container!=""}[5m]))` | Medium |
| `node_namespace_pod_container:container_memory_rss` | Resident Set Size (RSS) for containers/pods/VMs. RSS measures the actual non-swapped physical memory used by each running container, pod, or VM. High RSS usually means a "hot" process, but can also surface memory leaks, process bloat, or misconfigured VM/container memory limits. Excessive RSS per pod or VM risks node-level memory exhaustion and precedes Out-Of-Memory (OOM) kills.  | `sum by(node, namespace, pod) (container_memory_rss{container!=""})/on(node) group_left() label_replace(node_memory_MemTotal_bytes, "node", "$1", "instance", "(.+)")` | Medium |
| `node_namespace_pod_container:container_memory_working_set_bytes` | Working set bytes for containers/pods/VMs. This metric captures the amount of resident, non-evictable memory that is actually in use inside each container, pod, or VM. An increasing working set shows real active memory pressure: as the sum approaches node total RAM, resource competition spikes, scheduling becomes tight, and the risk of pod or VM eviction rises. | `sum by(node, namespace, pod) (container_memory_working_set_bytes{container!=""}) / on(node) group_left() label_replace(node_memory_MemTotal_bytes, "node", "$1", "instance", "(.+)") ` | High |
| `node_netstat_Icmp6_InMsgs` | **NOTE: IPv6 not used in OpenShift 4.18 or earlier by default** Total incoming ICMPv6 (IPv6 "ping", error, and control) messages. This metric counts every ICMPv6 message received by the node—covering pings, neighbor discovery, router advertisements, and error/status traffic for IPv6. This metric is  relevant when VMs or cluster services use IPv6 networking. Unexpected drops may indicate external connectivity issues for VMs using IPv6. | `rate(node_netstat_Icmp6_InMsgs[5m])` | Very Low |
| `node_netstat_Icmp_InMsgs` | Total incoming ICMP (IPv4) messages. Includes all types: ping (echo), destination unreachable, TTL expired, etc. Used for network diagnostics and core IP-layer error reporting. Increases with VM health checks, kubelet status probes, or SDN tunneling events. Sudden drops or irregularities may reflect broken L3, security group policies, or platform-level route faults impacting VM visibility. | `rate(node_netstat_Icmp_InMsgs[5m])` | Low |
| `node_netstat_Ip6_InOctets` | Total number of IPv6 octets (bytes) received. Measures IPv6 network throughput to nodes hosting VMs. When running dual-stack VMs, unexpected increases may indicate IPv6-specific traffic that could impact network performance. | `rate(node_netstat_Ip6_InOctets[5m])` | Low |
| `node_netstat_Ip6_OutOctets` | Total number of IPv6 octets (bytes) sent. Measures IPv6 network throughput from nodes hosting VMs. For VMs serving content over IPv6, this metric helps identify if network bandwidth is sufficient for the workload or if IPv6 traffic patterns differ from IPv4. | `rate(node_netstat_Ip6_OutOctets[5m])` | Low |
| `node_netstat_IpExt_InOctets` | Total number of IPv4 octets (bytes) received. Measures inbound network throughput to nodes hosting VMs. Critical for monitoring network-intensive VM workloads and identifying potential bandwidth constraints. If this metric approaches physical NIC capacity, VMs may experience network contention, packet drops, or increased latency, especially for VMs running streaming or real-time communication services. | `rate(node_netstat_IpExt_InOctets[5m]) / (1024 * 1024) > 100` | Medium |
| `node_netstat_IpExt_OutOctets` | Total number of IPv4 octets (bytes) sent. Measures outbound network throughput from nodes hosting VMs. Critical for VMs serving content or performing data transfers. | `rate(node_netstat_IpExt_OutOctets[5m]) / (1024 * 1024) > 1250` (for a 10g nic or 3150 for 25G nic)| Medium |
| `node_netstat_Tcp_ActiveOpens` | Total number of active TCP connection openings. Measures the rate of new outbound connections from VMs. Useful for identifying abnormal connection patterns or potential connection storms from VM workloads. A sudden spike in this metric may indicate a VM is performing aggressive outbound connection attempts, which could be a sign of compromised VM, misconfigured application, or DoS attack originating from within a VM. | `rate(node_netstat_Tcp_ActiveOpens[5m]) > 1000` | Medium |
| `node_netstat_TcpExt_ListenOverflows` | Number of connection attempts that exceeded OS listen queue capacity. This event occurs when the kernel SYN queue is full. Sustained ListenOverflows might signal a DDoS, application bug, or mis-tuned sysctl settings leading to severe service or VM access degradation. | `increase(node_netstat_TcpExt_ListenOverflows[5m]) > 0` | High |
| `node_netstat_TcpExt_TCPSynRetrans` | TCP SYN retransmissions. Each increment means a SYN (connection start request) from a client or pod/VM was not acknowledged by the server, so it was retried. This reflects network packet loss or high latency on the path. Spikes may coincide with SDN fabric issues, physical link loss, or node CPU/network pressure. A high SYN retrans rate can result in slow VM services, user complaints, or failed liveness checks. | `increase(node_netstat_TcpExt_TCPSynRetrans[5m])` | Critical |
| `node_netstat_TcpExt_TCPTimeouts` | TCP connections timing out due to unacknowledged packets. This metric increments when the OS has waited too long for a TCP retransmission and gives up on the connection. This is a high-priority signal. | `increase(node_netstat_TcpExt_TCPTimeouts[5m]) > 10` | Critical |
| `node_netstat_Tcp_InErrs` | Input TCP errors excluding retransmissions/timeouts. These are lower-level protocol errors, such as malformed segments, bad flags, unsupported options, or failed checksums. Increase often means external network problems, faulty hardware, buggy kernel/driver, or malicious scanning traffic. VMs may observe connection drops, failed data transfers, or degraded throughput when this metric rises. | `increase(node_netstat_Tcp_InErrs[5m]) > 0` | Medium |
| `node_netstat_Tcp_PassiveOpens` | Total passive TCP opens (server-side connection accepts). This metric increases when an application (such as a VM, pod, or service) accepts a new incoming TCP connection. Low values may indicate service down or underutilization; spikes align with traffic bursts or attacks. | `rate(node_netstat_Tcp_PassiveOpens[5m])` | Low |
| `node_netstat_Udp6_InErrors` |  **NOTE: IPv6 not used in OpenShift 4.18 or earlier by default.** Errors decoding or processing incoming IPv6 UDP packets on the node. This metric counts every time the kernel receives a UDPv6 datagram but cannot deliver it to an application. These errors can translate directly to dropped syslog messages, lost DNS queries, or gaps in UDP-based application communication. | `increase(node_netstat_Udp6_InErrors[5m]) > 0` | Very Low |
| `node_netstat_Udp6_RcvbufErrors` | **NOTE: IPv6 not used in OpenShift 4.18 or earlier by default.** UDPv6 receive buffer overrun errors. Increments when a UDPv6 datagram is dropped due to lack of receive buffer space, usually under sustained high traffic or mis-sized buffers. | `increase(node_netstat_Udp6_RcvbufErrors[5m]) > 0` | Very Low |
| `node_netstat_Udp_InDatagrams` | Total received UDP datagrams (IPv4). Counts all valid UDP packets delivered to sockets. Sudden changes may reflect SDN changes, service pauses, or application behavior shifts at the node/VM level. | `rate(node_netstat_Udp_InDatagrams[5m])` | Low |
| `node_network_carrier_changes_total` | Total number of network carrier transitions (up/down) detected for each device. This counter tells you how many times the interface link state changed. In heavily loaded OpenShift nodes, an increasing value here means network instability, flap, or failing cables/nics. | `increase(node_network_carrier_changes_total[1h])` | High |
| `node_network_carrier_down_changes_total` | Count of link-down transitions. Specifically tracks each time the network interface lost link. In OpenShift, this means “outage event for at least one interface.” Repeated increases mean failing hardware, bad cabling, or misbehaving switches, showing up as VM disconnects, pod restarts, or temporary black-holing. | `increase(node_network_carrier_down_changes_total[1h]) > 0` | Critical |
| `node_network_carrier` | Network link carrier status. This is a gauge (0 or 1) for each network interface/device on a node: `1` if the link is physically up (carrier detected), `0` otherwise. This directly reports the “is the cable plugged in, is the port active?” answer for every physical and virtual interface. <br><br>**Metric Values**<br>- `1` The cable is plugged in and the carrier is detected<br>- `0` Not cable (carrier) detected<br><br>It is possible that there are some unused ports in the VM hosts that you might want to exclude by using `node_network_carrier{device!="eno12409np1"}` in the query. | `min by(instance, device) (node_network_carrier) == 0` | Critical |
| `node_network_receive_bytes_total` | Cumulative data received (bytes) on each network device. This is an ever-increasing counter; reflects the total bytes received since boot. In OpenShift Virtualization, it is the best measure for actual ingress network load—across management, VM, and tenant traffic. | `sum by(instance, device)(rate(node_network_receive_bytes_total[5m]))` | Low |
| `node_network_receive_errs_total` | Cumulative receive errors (hardware/driver) per device. Reflects “hard” problems: overrun buffers, DMA failures, or CRC errors from broken links. Exposes physical or driver issues invisible to higher layers. In VM clusters, high values equate to packet drops, lost API calls, and guest workload flakiness. Alert on error rates, not just count. | `rate(node_network_receive_errs_total[5m]) / rate(node_network_receive_bytes_total[5m]) > 0.01` | Critical |
| `node_network_receive_fifo_total` | Receive FIFO buffer errors per network device. This metric is a cumulative counter representing how many times a network interface's hardware receive FIFO (First-In-First-Out) buffer has overrun on a node. FIFO overruns happen when incoming packets arrive faster than the network interface or kernel can process or move to memory—resulting in dropped packets at the hardware level, before the OS or applications (including VMs and pods) see them. Any sustained increase, especially on production or VM-bearing interfaces, is a major warning. Occasional single increments can occur briefly during reboots or very short-lived spikes. | `increase(node_network_receive_fifo_total[5m]) > 0` | High |
| `node_network_receive_frame_total` | Receive frame (alignment) errors. This metric counts the total number of received packets on network interfaces that have framing or alignment errors. Such errors usually mean packets have incorrect structure caused by physical-layer faults like damaged cables, electrical interference, loose or corroded connectors, or failing network interface hardware. Reliable and low-latency networking is critical. Even a few frame errors can signal deeper network integrity problems that will lead to packet loss. | `increase(node_network_receive_frame_total[5m]) > 0` | High |
| `node_network_receive_packets_total` | Total count of network packets received. Measures network load in packets rather than bytes. VMs running packet-processing workloads may hit packet processing limits before bandwidth limits. High packet rates can cause CPU saturation on the host, affecting all VMs. | `rate(node_network_receive_packets_total{device!="lo"}[5m]) > 100000` | Medium |
| `node_network_speed_bytes` | Configured link speed, in bytes per second. How fast the interface can theoretically transmit—e.g., 1Gbps = 125,000,000 bytes/sec. Use this to check that key interfaces are running at their expected speeds (no accidental negotiation to very low speeds). For VM and storage networks, mismatches cause silent bottleneck and congestion; spotted here. Negative or zero often means the interface is down or not negotiated. | `node_network_speed_bytes/125000000` | Medium |
| `node_network_transmit_drop_total` | Packets dropped before transmission. This tracks each time a packet was queued for outbound send, but dropped by the device/NIC before leaving the host (usually due to buffer exhaustion). Increases here hint at packet loss, potential starvation for guest VM networks, or SDN saturation. | `rate(node_network_transmit_drop_total[5m]) > 0` | High |
| `node_network_transmit_errs_total` | Total transmit errors (hardware/driver) per device. Signals low-level hardware or driver problems that prevent packets from being sent (CRC errors, collisions, DMA errors, cabling trouble). Best monitored as a ratio to transmitted traffic. | `rate(node_network_transmit_errs_total[5m]) / rate(node_network_transmit_bytes_total[5m]) > 0.01` | Critical |
| `node_network_up` | Binary metric indicating if a network device is up (1) or down (0). Network interface failures directly impact VM connectivity. If this metric shows 0 for a network interface used by VM secondary NICs (like SR-IOV or bridge networks), those VMs will lose connectivity on that interface, potentially breaking application clustering or storage connectivity. <br><br>**Metric Values**<br>- `1` The network interface is up<br>- `0` The network interface is down| `node_network_up{device!="lo"} == 0` | Critical |
| `node_nf_conntrack_entries` | Current number of active conntrack entries (network flows) being tracked by the Linux kernel. Conntrack tracks all ongoing network connections (TCP, UDP, some ICMP, etc.) traversing the node. High entries indicate many active pod/VM connections, load balancer sessions, or service endpoints. Exceeding safe levels means new connections may be dropped, causing app or pod failures. Utilization typically scales with RAM. | `(node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75` | High |
| `node_nf_conntrack_entries_limit` | Maximum configurable limit of conntrack entries for the node. This sets the cap on how many tracked connections the OS supports. | `(node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75` | High |
| `node_nf_conntrack_stat_drop` | Number of packets dropped due to conntrack table exhaustion. If more flows arrive than can be tracked, further connections are immediately dropped. This signals imminent cluster/network outages—new connections from pods, VMs, or services may fail unpredictably. Surges in this metric require immediate investigation. | `increase(node_nf_conntrack_stat_drop[5m]) > 0` | Critical |
| `node_nf_conntrack_stat_insert` | Connection tracking table inserts. Each increment means a new connection was just added. High rates = lots of new connections starting, which happens with chatty microservices, heavy web APIs, or rapid pod churn. | `rate(node_nf_conntrack_stat_insert[5m])` | Medium |
| `node_nf_conntrack_stat_insert_failed` | Failed attempts to insert new conntrack entries. This counts every time the kernel tried to create a new entry but couldn't, usually due to table being full or system stress. Persistent values here map to failing pod/VM connections, application timeouts, or failed service endpoints. Must remain zero in healthy clusters. | `increase(node_nf_conntrack_stat_insert_failed[5m]) > 0` | High |
| `node_nfs_connections_total` | Cumulative number of client connections to NFS servers (e.g., persistent storage for pods/VMs). High values indicate lots of NFS traffic—common for shared storage in OpenShift. Watch for relationship with pod/VM restarts and performance. | `rate(node_nfs_connections_total[5m])` | Low |
| `node_nfsd_connections_total` | Total count of NFS connections. Important if VMs are using NFS-based storage for their disks. NFS connection saturation can cause VM I/O operations to stall, leading to apparent VM freezes. If this metric approaches the maximum supported NFS connections on a node providing NFS storage for VM disks, VMs may experience slow I/O operations or apparent freezes as their disk operations queue. | `node_nfsd_connections_total` | Medium |
| `node_nfsd_file_handles_stale_total` | This is a counter that is always increasing representing the number of “stale file handles” seen by NFS server. This occurs when a file referenced by a handle (open file) is removed/renamed/unavailable on the server, causing failures for pods/VMs still accessing it. High/rapid growth implies storage maintenance, poor pod termination order, or share corruption, causing I/O errors or stuck workloads. | `increase(node_nfsd_file_handles_stale_total[5m])` | Medium |
| `node_nfsd_reply_cache_hits_total` | NFS reply cache “hits”; a reply that was able to answer a client’s duplicate request fast, avoiding rework. High values = effective caching and efficient NFS server operation, especially with retransmitting clients or network flappiness (which is common in dense Kubernetes clusters). | node_nfsd_reply_cache_hits_total | Very Low |
| `node_nfsd_reply_cache_misses_total` | Count of NFS server reply cache misses, by pool or server. A "reply cache miss" means the requested reply was not found in the cache, so the NFS server must recompute or re-execute the operation. Sudden bursts in misses may be triggered by NFS server failover/restarts, elevated network loss or retransmits between nodes and storage, overloaded cache size, or temporary overload conditions. Regularly observe baselines for your cluster and set thresholds for abnormal growth. | `increase(node_nfsd_reply_cache_misses_total[5m])` | Medium |
| `node_nfsd_requests_total` | Total number of NFS operations processed. Direct measure of NFS server load. Watch for surges/cliffs, which reflect pod churn, backup/restore, or heavy VM usage. This has `method`, `proto` and `instance` columns that may be useful for filtering | `rate(node_nfsd_requests_total[5m])` | Low |
| `node_nfsd_rpc_errors_total` | Total count of NFS Remote Procedure Call (RPC) errors. Critical for VMs using NFS-backed storage. RPC errors can cause I/O errors within VMs, leading to filesystem corruption or application failures inside the VM. | `increase(node_nfsd_rpc_errors_total[15m]) > 0` | High |
| `node_nfsd_server_threads` | Number of active NFS server worker threads on the node. Each thread processes one NFS operation at a time. Default in OpenShift 4.18: For RHEL and related Linux distributions (which underpin OpenShift 4.18 nodes), the default number of NFS server threads is 8, unless explicitly tuned. Some storage providers such as Portworx, may opt to tune this via their own commands outside of OpenShift (such as `pxctl cluster options update --sharedv4-threads`.  | `node_nfsd_server_threads` | Low |
| `node_nfs_requests_total` | Total NFS operations processed on the node. This tracks how many file system requests (reads, writes, metadata lookups) have been processed by the NFS server since boot. This metric shows the level of storage traffic. High or rising rates may indicate heavy backup jobs, bursting VM disk activity, or storage bottlenecks. This has `method`, `proto` and `instance` columns that may be useful for filtering | `rate(node_nfs_requests_total[5m]) > $(nfs_request_threshold)` | Low |
| `node_nfs_rpc_retransmissions_total` | Number of NFS RPC request retransmissions. This counter grows when the node’s NFS client/server must re-send a request, usually due to a lost or slow packet. Consistently increasing values indicate network congestion, storage server slowness, or unreliable connectivity between VMs/pods and their NFS mounts. Spikes may signal impending application I/O errors. | `increase(node_nfs_rpc_retransmissions_total[5m]) > 0` | High |
| `node_procs_blocked` | Number of processes currently blocked, waiting for I/O completion (disk or network). This gauge metric indicates how many user processes or system daemons are stuck in an uninterruptible state because an operation on disk, storage, or network has not finished. High or sustained values suggest the node is experiencing heavy disk or network I/O pressure, hardware contention, or degraded performance from underlying storage. | `node_procs_blocked` | Medium |
| `node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile` | Pod Lifecycle Event Generator (PLEG) relist latency for the kubelet. This metric measures the quantile (commonly the 99th percentile) latency of the PLEG "relist" operation. The kubelet uses PLEG to poll the container runtime for all pod and container state changes. Relisting occurs ~every second. Elevated values often point to an overloaded node, high container churn, or runtime lags. | `histogram_quantile(0.99, rate(kubelet_pleg_relist_duration_seconds_bucket[5m])) > 5` | Medium |
| `node_role_os_version_machine:cpu_capacity_cores:sum` | Aggregated sum of CPU cores capacity across nodes, broken down by node role, OS version, and machine type. Critical for VM capacity planning and placement, as different classes of nodes may be designated for specific VM workloads. Insufficient CPU capacity directly impacts VM density and performance. | `node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io="worker"}` | High |
| `node_role_os_version_machine:cpu_capacity_sockets:sum` | Total CPU socket count for a given (role, OS, machine type) tuple. Has columns `label_node_hyperthread_enabled`, `label_node_role_kubernetes_io_master`. This is a socket count, not a core count. | `node_role_os_version_machine:cpu_capacity_sockets:sum` | Very Low |
| `node_schedstat_waiting_seconds_total` | Total time tasks spent waiting for CPU scheduling. This metric accumulates the number of seconds that tasks (processes, threads) on the node have spent in the scheduler queue, waiting for an available CPU core. Persistent or rising rates reflect that the node’s CPUs are not able to keep up with demand. Tracking the *rate* of waiting seconds, rather than just the total, allows early detection of contention trends—alerting before average CPU usage maxes out, letting you shift or add capacity proactively. | `sum by(instance) (rate(node_schedstat_waiting_seconds_total[5m]))` | Medium |
| `node_selinux_enabled` | Binary gauge if SELinux is enabled (1) or not (0). Ensures nodes are running with the expected kernel security posture. If unexpectedly disabled, VMs/pods may be exposed to privilege escalation or data exfiltration risks. <br><br>**Metric Values**<br>- `1` SELinux is Enabled <br>- `0` SELinux is **disabled** | `node_selinux_enabled == 0` | Critical |
| `node_sockstat_TCP6_inuse` | Open/active TCP6 sockets (IPv6) in use by the kernel. Surges or persistent high counts can signal memory leaks, socket exhaustion, or failed connection cleanup by VMs/pods. | `node_sockstat_TCP6_inuse` | Very Low |
| `node_sockstat_TCP_alloc` | Total allocated (used and in process of opening) TCP sockets. A key high-water mark for tracking kernel networking demands. Trends help diagnose early stages of connection exhaustion or DDOS before failures manifest. Also helps to identify nodes that have the most active network connections. | `node_sockstat_TCP_alloc` | Low |
| `node_sockstat_TCP_inuse` | Active, fully in-use TCP sockets. Shows the number of concurrent TCP connections, often rising/falling with application deployment, VM migration, or service churn. | `node_sockstat_TCP_inuse` | Low |
| `node_sockstat_TCP_orphan` | Orphaned TCP sockets (connections without an owning process). Small blips are normal; spikes warn of resource leaks, failed connection cleanup, or node overload. Large orphan counts risk kernel memory exhaustion and connection stalls for VMs/pods. | `node_sockstat_TCP_orphan` | High |
| `node_sockstat_UDP_mem_bytes` | Kernel memory usage for UDP sockets (bytes). Measures the total memory the Linux kernel has allocated for UDP socket receive buffers. High values typically mean heavy UDP traffic (e.g., DNS, telemetry, log streaming) or application buffer misconfigurations. Excessive UDP buffer use may indicate noisy or overloaded pods/VMs and can lead to packet drops if kernel limits are hit. | `node_sockstat_UDP_mem_bytes` | Low |
| `node_sockstat_UDP_mem` | Total kernel memory in use by UDP sockets. Indicates both volume and health of stateless traffic (e.g., DNS, syslog, DHCP) in the node. Unexpected rises can precede UDP packet loss, memory stress, pod or VM service downtime, or even node OOM. | `node_sockstat_UDP_mem` | Medium |
| `node_softnet_backlog_len` | Current software network interrupt (softnet) queue length. This metric tells you how many packets are waiting in the backlog for processing by the CPU after they arrive from the NIC. If this grows, it signals the node’s CPUs can't keep up with incoming packets—due to CPU saturation, high packet rates, or driver inefficiencies. A persistent backlog means packet delivery slowdowns or VM/pod networking errors are imminent. | `sum by (instance)(node_softnet_backlog_len) ` | High |
| `node_softnet_dropped_total` | Count of packets dropped by the softnet layer. These packets were never delivered to applications because the interrupt-processing queue was full. In dense clusters, spikes here directly cause lost pod/VM packets and degraded service reliability. Alert promptly if this grows. | `sum by(instance)(increase(node_softnet_dropped_total[5m])) > 0` | High |
| `node_textfile_scrape_error` | Indicates if there was an error scraping metrics via the textfile collector.<br><br>**Metric Values**<br>- `1` means an error occurred during the scrape<br>- `0` means the scrape was successful with no issues.<br>  | `node_textfile_scrape_error == 1` | Low |
| `node_thermal_zone_temp` | Zone temperature in Celsius for node hardware components. If temperatures approach critical thresholds on compute nodes running VMs, performance degradation from thermal throttling may occur, causing inconsistent VM performance or unexpected VM migrations. | `max(node_thermal_zone_temp) by (instance) > 80` | High |
| `node_time_seconds` | Current system time in seconds since **Unix epoch.** This is the node’s current wall-clock time, used for troubleshooting time sync across nodes or with external systems.  | `abs(time() - node_time_seconds) > $(time_drift_threshold)` | Medium |
| `node_timex_estimated_error_seconds` | Estimated error of the kernel timekeeping (seconds). Indicates the uncertainty in the local system clock’s accuracy. High error means your NTP/PTP can’t correct drift quickly| `node_timex_estimated_error_seconds > $(timex_error_threshold)` | Medium |
| `node_timex_loop_time_constant` | Phase-locked loop (PLL) time constant of the system clock discipline. This metric controls the responsiveness and stability of the kernel's time synchronization. A higher value means the system clock reacts more slowly to errors and drift, resulting in sluggish time corrections; a lower value means the clock responds quickly, but too low risks introducing instability or oscillation. Values between 2-6 are considered normal while greater than 10 is abnormal|  `node_timex_loop_time_constant > $(timex_loop_time_threshold)`| Low |
| `node_timex_maxerror_seconds` | Maximum kernel clock error tolerated (seconds). This metric shows the worst-case estimated error the kernel attributes to the system clock. When high, it means the clock may have drifted or lost sync, increasing the risk of inconsistent timestamps, event ordering problems, or failures in distributed protocols. Monitoring this helps ensure the trusted foundation of time. Typical values should be low (well under 1s with NTP/chrony configured properly); sudden increases indicate time sync is lost, restarting, or network sources are unreachable. | `node_timex_maxerror_seconds > $(timex_maxerror_threshold)` | Medium |
| `node_timex_offset_seconds` | Current system clock offset from NTP/PTP reference (seconds). This metric reveals how far the local node’s clock has drifted from its reference source (NTP/PTP). Positive or negative values show whether the local clock is running ahead or behind. In OpenShift Virtualization, high offset breaks coordination: authentication tickets may fail, logs cannot be correlated, distributed storage and consensus algorithms (Raft, etcd, etc.) may malfunction. | `abs(node_timex_offset_seconds)` | High |
| `node_timex_pps_stability_exceeded_total` | Count of times PPS (pulse-per-second) signal stability was exceeded. This metric counts every time the hardware clock’s PPS signal goes outside configured stability bounds—essential for nodes using high-precision time sources. Frequent increases mean NTP/PTP hardware is unstable or under stress, endangering precision timekeeping. | `increase(node_timex_pps_stability_exceeded_total[5m]) > 0` | Medium |
| `node_timex_status` | Status bitfield for time synchronization subsystem. Encodes if the kernel thinks the clock is synchronized, in error, or in a special state. **The expected value is 0**. | `node_timex_status != 0` | Medium |
| `node_timex_sync_status` | **Synchronizing status for node time.** 1 typically means in sync, 0 means lost sync ("unsynced"). If not in sync, distributed jobs, storage, and logs are unreliable. | `node_timex_sync_status == 0` | High |
| `node_vmstat_oom_kill` | Count of Out-of-Memory kill events from `/proc/vmstat`. OOM events on nodes running OpenShift Virtualization are catastrophic as they can terminate the virt-launcher pods running VMs, causing immediate VM termination and potential data corruption.  | `increase(node_vmstat_oom_kill[15m]) > 0` | Critical |
| `node_vmstat_pgfault` | Minor page faults (soft paging events). This metric counts how often virtual memory is mapped to physical RAM, but the page is already in memory. Steady or moderate minor page faults are normal for healthy running VMs and pods, but a sharp or persistent rise may signal application memory churn (apps allocating/forking/freeing memory rapidly). While less severe than major page faults, surges still point to less efficient memory usage and can indicate early signs of memory pressure. | `rate(node_vmstat_pgfault[5m]) > $(pgfault_threshold)` | Low |
| `node_vmstat_pgmajfault` | Major page faults (hard paging events). Each increment is a memory access where the needed page wasn't in RAM and had to be loaded from disk (either from swap or the original file). A high or rising major page fault rate almost always means the node is under-resourced (not enough RAM for workload demand), leading to severe VM/pod slowdowns, IO waits, and timeouts. This is a leading indicator for when to add RAM, optimize workloads, or move memory-intensive tenants elsewhere. | `rate(node_vmstat_pgmajfault[5m]) > $(pgmajfault_threshold)` | Critical |
| `node_vmstat_pgpgout` | Number of pages swapped out to disk. A running total of memory pages the OS has moved from RAM to swap device to free up space for active processes. In OpenShift Virtualization, persistent increases mean the node has exhausted RAM and is actively paging VM/pod memory to disk—which is far slower than keeping it in memory. This degrades latency, increases IO load, and causes application stalls or failures. | `rate(node_vmstat_pgpgout[5m])` | Critical |
| `node_vmstat_pswpin` | Pages swapped in from disk. Each increment means a page previously swapped out (see `node_vmstat_pgpgout`) is now brought back into RAM for a running task. High or trending increases usually mean workloads are oscillating between RAM and disk (thrashing), generating extra IO load and causing all VMs and pods to become sluggish. This is especially damaging for performance-sensitive or low-latency workloads. Alert on abnormally high rates, and use with swap and pgmajfault metrics to diagnose memory pressure. | `rate(node_vmstat_pswpin[5m])` | High |
| `node_watchdog_bootstatus` | Hardware watchdog boot status indicator. In virtualization environments, hardware watchdog failures may indicate underlying hardware issues on VM host nodes that could lead to unexpected reboots and VM outages. | `node_watchdog_bootstatus > 0` | Medium |
| `node_watchdog_timeleft_seconds` | Represents seconds remaining before the hardware watchdog timer would trigger a reboot if not reset. In properly functioning systems, this value is typically constant as the watchdog is continuously being reset. **This is NOT an indicator of node health problems when stable at a low value.** If this value remains stable across all nodes, this indicates normal operation where the watchdog is being reset regularly by the kernel. Only a decreasing value without reset would indicate a problem. | `rate(node_watchdog_timeleft_seconds[5m])` | Low |
| `openshift_auth_form_password_count_result` | Count of web console login attempts by result. Multiple failed logins from the same source may indicate attempts to access the virtualization console to manipulate VMs or their configurations. | `sum(rate(openshift_auth_form_password_count_result{result="error"}[15m])) > 5` | Medium |
| `openshift:cpu_usage_cores:sum` | Total CPU usage by OpenShift infrastructure components. During VM migration or creation operations, if this value spikes significantly, platform components may be consuming CPU resources needed for VM operations, causing delays or performance issues. | `openshift:cpu_usage_cores:sum / on() group_left() sum(cluster:capacity_cpu_cores:sum) > 0.3` | Medium |
| `openshift_etcd_operator_signer_expiration_days` | Days until the etcd operator signing certificates expire. Certificate expiration could disrupt etcd, which would affect all VM control operations and potentially make the virtualization platform unavailable. | `openshift_etcd_operator_signer_expiration_days < 14` | High |
| `openshift:memory_usage_bytes:sum` | Total memory usage by OpenShift infrastructure components. If memory usage by platform components is high on nodes intended for VM workloads, VM density will be reduced, and memory-intensive VMs may experience performance degradation or fail to be scheduled. | `openshift:memory_usage_bytes:sum / on() group_left() sum(cluster:capacity_memory_bytes:sum) > 0.3` | Medium |
| `operation:etcd_request_duration_seconds_bucket:rate1m` | Rate of request duration histogram buckets over 1 minute. This shows how frequently requests fall into each duration bucket, giving a short-term performance view. Useful for detecting sudden performance degradation in etcd that could impact VM operations.**NaN:** If no requests in the last minute, may be NaN. | `histogram_quantile(0.95, operation:etcd_request_duration_seconds_bucket:rate1m) > $(request_duration_short_window)` | Medium |
| `operation:etcd_request_duration_seconds_bucket:rate5m` | Rate of request duration histogram buckets over 5 minutes. Similar to the above but for a longer window, smoothing out short-term spikes and showing sustained performance trends. | `histogram_quantile(0.95, operation:etcd_request_duration_seconds_bucket:rate5m) > $(request_duration_long_window)` | Medium |
| `ovn_controller_bridge_mappings` | Status metric for OVN controller bridge mappings configuration. In OpenShift Virtualization, bridge mappings are critical for connecting VMs to physical networks, especially for VMs requiring direct layer 2 connectivity. If bridge mappings are misconfigured or unavailable, VMs with attachments to secondary networks or requiring physical network access may lose connectivity or fail to start. | `ovn_controller_bridge_mappings == 0` | High |
| `ovn_controller_ct_zone_commit_95th_percentile` | 95th percentile time taken to commit connection tracking zones in OVN. If this value is high, VMs with high network traffic may experience connection tracking issues, packet drops, or unexpected connection resets, especially during traffic spikes. | `ovn_controller_ct_zone_commit_95th_percentile > 100` | Medium |
| `ovn_controller_if_status_mgr_run_maximum` | Maximum execution time for the OVN interface status manager. If this value is high, detection and recovery of network interface failures for VMs may be delayed, extending network outages for virtualized workloads. | `ovn_controller_if_status_mgr_run_maximum > 200` | Medium |
| `ovn_controller_lflow_run` | Execution time metrics for logical flow processing in OVN. Logical flows control VM network traffic paths, and slow processing delays network policy and route updates. During network policy updates affecting VMs, slow logical flow processing could delay the application of security rules, temporarily leaving VMs with incorrect network access. | `increase(ovn_controller_lflow_run[5m]) > 1000` | Medium |
| `ovn_controller_monitor_all` | Status of OVN controller monitoring processes. If this metric indicates monitoring is failing, changes to VM network configuration may not be detected or applied, leading to inconsistent network behavior for VMs. | `ovn_controller_monitor_all == 0` | High |
| `ovn_controller_southbound_database_connected` | Binary status indicating if OVN controller is connected to the southbound database . If this value is 0, the node's OVN controller cannot communicate with the central OVN database, preventing VM network configuration updates and potentially isolating VMs on that node. <br><br>**Metric Values**<br>- `1` The binary southbound database is connected<br>- `0` The southbound database is disconnected| `ovn_controller_southbound_database_connected == 0` | Critical |
| `ovn_controller_txn_error` | Count of OVN controller transaction errors. If transaction errors increase during VM creation or migration, the associated network setup may fail, leaving VMs with incorrect or missing network connectivity. | `increase(ovn_controller_txn_error[5m]) > 0` | High |
| `ovn_db_e2e_timestamp` | End-to-end timestamp information for OVN database operations. If the difference between current time and this timestamp increases, it indicates OVN database operation delays that could slow VM network provisioning or policy updates. | `time() - ovn_db_e2e_timestamp > 70`| Medium |
| `ovnkube_clustermanager_allocated_v4_host_subnets` | Number of IPv4 host subnets allocated in the OVN cluster. Determines availability of IPv4 subnets for VM placement. This should be compared against the expected number of subnets per cluster. | `ovnkube_clustermanager_allocated_v4_host_subnets` | Medium |
| `ovnkube_clustermanager_allocated_v6_host_subnets` |  Number of IPv4 host subnets allocated in the OVN cluster. Determines availability of IPv4 subnets for VM placement. This should be compared against the expected number of subnets per cluster. | `ovnkube_clustermanager_allocated_v6_host_subnets` | Medium |
| `ovnkube_controller_ipsec_enabled` | Binary indicator whether IPsec encryption is enabled for OVN network traffic (1=enabled, 0=disabled). Affects security of VM-to-VM traffic across nodes. Critical for multi-tenant virtualization where network traffic isolation is required. If this metric shows 0 when IPsec should be enabled, VM network traffic may be unencrypted, potentially exposing sensitive data in multi-tenant virtualization environments. <br><br>**Metric Values**<br>- `1` IPsec encyrption is enabled<br>- `0` IPsec encryption is **disabled**| `ovnkube_controller_ipsec_enabled == 0` | High |
| `ovnkube_controller_pod_port_binding_chassis_port_binding_up_duration_seconds_bucket` | Histogram of time taken for OVN port bindings to become active. Directly affects how quickly VM network interfaces become available after VM creation or migration. If the 95th percentile exceeds 5 seconds, VM creation operations may take longer than expected, particularly affecting VM migration time and application availability during migrations. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_pod_port_binding_chassis_port_binding_up_duration_seconds_bucket[5m])) by (le)) > 5` | Medium |
| `ovnkube_controller_resource_add_latency_seconds_bucket` | Histogram of latency for adding network resources. Affects how quickly VM network resources (interfaces, IPs) are provisioned, directly impacting VM creation time. If the 95th percentile exceeds 2 seconds, VM network interfaces may be slow to initialize, affecting VM startup time and orchestration workflows. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_resource_add_latency_seconds_bucket[5m])) by (le, resource)) > 2` | Medium |
| `ovnkube_controller_resource_add_latency_seconds_count` | Count of network resource addition operations. High values indicate network resource churn which may affect OVN controller performance for all VM operations. During large-scale VM creation or migration operations, a high rate can indicate high controller load that might impact other network operations. | `rate(ovnkube_controller_resource_add_latency_seconds_count[5m]) > 10` | Low |
| `ovnkube_controller_resource_add_latency_seconds_sum` | Sum of latencies for adding network resources. Used with count to calculate average latency. High values indicate network control plane performance issues. When divided by the count, an increasing average latency for adding VM network resources may indicate control plane bottlenecks affecting VM deployment speed. | `rate(ovnkube_controller_resource_add_latency_seconds_sum[5m]) / rate(ovnkube_controller_resource_add_latency_seconds_count[5m]) > 1` | Medium |
| `ovnkube_controller_resource_delete_latency_seconds_bucket` | Histogram of latency for deleting network resources. Affects how quickly VM network resources are cleaned up after VM deletion or migration. If the 95th percentile exceeds 2 seconds, resources may not be cleaned up quickly, potentially affecting subsequent VM migrations to the same node or IP address reuse. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_resource_delete_latency_seconds_bucket[5m])) by (le, resource)) > 2` | Medium |
| `ovnkube_controller_resource_delete_latency_seconds_count` | Count of network resource deletion operations. High values indicate network resource cleanup activity which may affect controller performance. | `rate(ovnkube_controller_resource_delete_latency_seconds_count[5m]) > 10` | Low |
| `ovnkube_controller_resource_delete_latency_seconds_sum` | Sum of latencies for deleting network resources. Used with count to calculate average latency. High values indicate network control plane cleanup issues. When divided by the count, an increasing average latency for removing VM network resources may indicate control plane bottlenecks affecting resource cleanup. | `rate(ovnkube_controller_resource_delete_latency_seconds_sum[5m]) / rate(ovnkube_controller_resource_delete_latency_seconds_count[5m]) > 1` | Medium |
| `ovnkube_controller_resource_update_latency_seconds_bucket` | Histogram of latency for updating network resources. Affects how quickly VM network configuration changes take effect, including policy updates or interface reconfiguration. If the 95th percentile exceeds 2 seconds, network policy updates for VMs might be delayed, potentially leaving security gaps during policy transitions. | `histogram_quantile(0.95, sum(rate(ovnkube_controller_resource_update_latency_seconds_bucket[5m])) by (le, resource)) > 2` | Medium |
| `ovnkube_master_libovsdb_disconnects_total` | Total count of disconnections from the OVS database. Database disconnections can disrupt VM networking and prevent network configuration changes from being applied. If this metric increases rapidly, the OVN control plane is experiencing database connectivity issues that may prevent VM network changes from being applied correctly. | `increase(ovnkube_master_libovsdb_disconnects_total[15m]) > 0` | High |
| `ovn_northd_nb_connection_status` | Status of the northbound database connection (1=connected, 0=disconnected). If this metric is 0, the OVN northd service cannot communicate with the configuration database, preventing all VM network changes from being processed. <br><br>**Metric Values**<br>- `1` the northbound database is connected<br>- `0` The northbound database is **disconnected**.| `ovn_northd_nb_connection_status == 0` | Critical |
| `ovn_northd_ovnsb_db_run_maximum` | Maximum runtime for OVN Southbound database operations. If this value is high, VM network configuration changes may take longer to propagate, affecting the responsiveness of VM network management operations. | `ovn_northd_ovnsb_db_run_maximum > 1` | Medium |
| `ovn_northd_sb_connection_status` | Status of the southbound database connection (1=connected, 0=disconnected). If this metric is 0, the OVN northd service cannot communicate with the implementation database, preventing VM network configurations from reaching the hosts. | `ovn_northd_sb_connection_status == 0` | Critical |
| `ovn_northd_status` | Overall status of the OVN northd service (1=running, 0=not running). If this metric is 0, the OVN northd service is not running, which will prevent all VM network operations including creation, updates, and policy enforcement. <br><br>**Metric Values**<br>- `1` The northd service is running<br>- `0` The northd service is **down**.| `ovn_northd_status == 0` | Critical |
| `ovs_vswitchd_bridge` | Metric related to OVS bridges in OpenShift, which are foundational for VM networking. In OpenShift, there are typically two critical bridges: `br-int` (handling internal pod/VM traffic) and `br-ex` (gateway for external traffic). These bridges are essential for VM network connectivity - `br-int` connects VM interfaces within the cluster, while `br-ex` enables external access. Bridge failures would isolate VMs from other workloads or external networks. In a healthy OpenShift Virtualization environment, this metric should show both `br-int` and `br-ex` bridges. If either bridge is missing or reports an abnormal state, VMs may have partial or complete network failure, especially affecting external connectivity and inter-node VM communication. | `ovs_vswitchd_bridge{name="br-int"} != 1 or ovs_vswitchd_bridge{name="br-ex"} != 1` | Critical |
| `ovs_vswitchd_interface_tx_dropped_total` | Total number of transmitted packets dropped by OVS interfaces. High drop rates would cause VM applications to experience slow or failed outbound connections, affecting service reliability, especially for VMs serving external clients. | `rate(ovs_vswitchd_interface_tx_dropped_total[5m]) > 100` | High |
| `ovs_vswitchd_ofproto_packet_out` | Count of OpenFlow packet-out operations in OVS. These operations manually inject packets into the switch and abnormal values may indicate OVS controller issues that could affect VM network policy enforcement.| `rate(ovs_vswitchd_ofproto_packet_out[5m]) > 10` | Medium |
| `ovs_vswitchd_packet_in` | Count of packet-in operations where the switch sends packets to the OpenFlow controller for handling. Spikes during VM creation or migration are normal, but sustained high values could indicate network control plane issues that consume resources needed by VMs. | `rate(ovs_vswitchd_packet_in[5m]) > 100` | Medium |
| `ovs_vswitchd_txn_error` | Count of transaction errors in OVS. During network policy updates for VMs, transaction errors could prevent security rules from being applied, leaving VMs with incorrect access controls or connectivity issues. | `increase(ovs_vswitchd_txn_error[15m]) > 0` | High |
| `ovs_vswitchd_upcall_flow_limit_kill` | Count of flows killed due to upcall limits in OVS. If this increases during high VM network activity, it may indicate that the flow table capacity is insufficient for the VM workload, causing connection failures or degraded network performance. | `increase(ovs_vswitchd_upcall_flow_limit_kill[5m]) > 0` | High |
| `prometheus_notifications_dropped_total` | Total count of alert notifications that Prometheus has failed to send to Alertmanager. In OpenShift Virtualization, dropped notifications represent alerts about VM and infrastructure issues that never reach notification systems. Common causes include network issues between Prometheus and Alertmanager, Alertmanager unavailability, or queue overflow due to alert storms.| `increase(prometheus_notifications_dropped_total[15m]) > 0` | High |
| `prometheus_ready` | Binary indicator of whether Prometheus is ready. If this is 0, all VM monitoring would be unavailable, making it impossible to detect issues with VMs, hosts, or networking components, potentially leading to undetected outages. <br><br>**Metric Values**<br>- `1` Prometheus is ready<br>- `0` Prometheus is down.| `prometheus_ready == 0` | Critical |
| `prometheus_rule_evaluation_failures_total` | Count of Prometheus rule evaluation failures. If VM over-commitment rules fail to evaluate, operators may not receive alerts when hosts become overloaded, potentially causing VM performance degradation before manual detection. | `increase(prometheus_rule_evaluation_failures_total[15m]) > 0` | High |
| `prometheus_sd_dns_lookup_failures_total` | Number of DNS lookup failures during service discovery. DNS resolution failures can prevent discovery of virtualization components, causing gaps in monitoring data for VM workloads. | `increase(prometheus_sd_dns_lookup_failures_total[15m]) > 0` | Medium |
| `prometheus_target_scrape_pool_exceeded_target_limit_total` | Count of times when scrape targets exceeded the configured limit. In large clusters with many VMs, hitting target limits could cause monitoring gaps where some virtual machines have no metrics collected, creating blind spots in operations. | `increase(prometheus_target_scrape_pool_exceeded_target_limit_total[15m]) > 0` | Medium |
| `prometheus_target_scrape_pools_failed_total` | Count of scrape pool failures. Failed scrape pools can result in missing metrics from entire classes of virtualization components. | `increase(prometheus_target_scrape_pools_failed_total[15m]) > 0` | High |
| `prometheus_target_sync_failed_total` | Count of failures when syncing targets from discovery results. Sync failures may result in stale or missing monitoring targets for VM components. | `increase(prometheus_target_sync_failed_total[15m]) > 0` | Medium |
| `prometheus_tsdb_head_truncations_failed_total` | Count of failed TSDB head truncations in Prometheus. If TSDB truncations fail, Prometheus may run out of storage space and crash, causing complete loss of monitoring for VMs, preventing alerting for critical VM issues. | `increase(prometheus_tsdb_head_truncations_failed_total[5m]) > 0` | High |
| `prometheus_tsdb_wal_corruptions_total` | Count of WAL (Write-Ahead Log) corruptions. Corruptions can lead to data loss for recent VM metrics and potential monitoring system instability. | `increase(prometheus_tsdb_wal_corruptions_total[15m]) > 0` | High |
| `reconstruct_volume_operations_errors_total` | Count of errors during volume reconstruction operations. Critical for VM persistent storage reliability as failed operations may affect volume availability for VMs. | `increase(reconstruct_volume_operations_errors_total[15m]) > 0` | High |
| `rest_client_request_duration_seconds_bucket` | Histogram of REST client request durations. Slow API responses may delay critical VM operations like live migrations, potentially extending maintenance windows or causing application downtime when VMs need to be moved. | `histogram_quantile(0.95, sum(rate(rest_client_request_duration_seconds_bucket[5m])) by (le)) > 1` | Medium |
| `scheduler_pod_scheduling_sli_duration_seconds_bucket` | Histogram of scheduling latency for pods. If the 95th percentile exceeds 5 seconds, VM creation operations might take longer than expected, affecting user experience and migration performance. | `histogram_quantile(0.95, sum(rate(scheduler_pod_scheduling_sli_duration_seconds_bucket[5m])) by (le)) > 5` | Medium |
| `scheduler_unschedulable_pods` | Count of pods that can't be scheduled. Unschedulable VM pods prevent new VMs from starting or existing VMs from migrating, directly affecting virtualization platform capacity. | `scheduler_unschedulable_pods{namespace="openshift-virtualization"} > 0` | High |
| `service_ca_expiry_time_seconds` | The Unix timestamp (in seconds) when the OpenShift Service CA certificate will expire. If the Service CA certificate expires, internal TLS communication between OpenShift components-including those managing and exposing VM services-will fail. This can break service-to-service communication, API access, and disrupt VM management, migrations, and monitoring. | `service_ca_expiry_time_seconds - time() < 604800` (alerts if expiry is within 7 days) | Critical |
| `thanos_alert_sender_alerts_dropped_total` | Count of alerts dropped by Thanos alert sender.  Dropped alerts may mean missing critical notifications about VM health or performance issues. | `increase(thanos_alert_sender_alerts_dropped_total[15m]) > 0` | High |
| `thanos_rule_alertmanagers_dns_failures_total` | Count of DNS failures when connecting to Alertmanager. If DNS failures occur when trying to reach the Alertmanager, critical VM failure or performance alerts may not be delivered, leading to delayed incident response. | `increase(thanos_rule_alertmanagers_dns_failures_total[15m]) > 0` | High |
| `thanos_sidecar_prometheus_up` | Indicates if Prometheus is reachable via Thanos sidecar. Critical for long-term metric retention of VM performance data. If `0`, VM historical metrics become inaccessible. | `thanos_sidecar_prometheus_up == 0` | Critical |  
| `vector_checksum_errors_total` | Total number of checksum errors detected by Vector when processing log/event data. Indicates data integrity issues during log ingestion or forwarding. High or increasing values suggest that some log entries—potentially including those from VM, hypervisor, or cluster events—were corrupted or tampered with in transit. | `increase(vector_checksum_errors_total[15m]) > 0` | High |   
| `vector_http_server_handler_duration_seconds_bucket` | HTTP request latency for Vector. High latency delays VM log availability during incidents. | `histogram_quantile(0.95, sum(rate(vector_http_server_handler_duration_seconds_bucket[5m])) by (le)) > 2` | Medium |   
| `workload:cpu_usage_cores:sum` | Aggregated CPU usage across VMs. Sustained >85% usage risks VM throttling. | `sum(workload:cpu_usage_cores:sum) / sum(cluster:capacity_cpu_cores:sum) > 0.85` | Critical |  
| `workload:memory_usage_bytes:sum` | Total memory used by VMs. Combined with swap metrics, identifies memory pressure. | `sum(workload:memory_usage_bytes:sum) / sum(cluster:capacity_memory_bytes:sum) > 0.85` | High |  
| `write:apiserver_request_duration_seconds_bucket:rate1m` | API write latency. High values delay VM lifecycle operations. | `histogram_quantile(0.95, write:apiserver_request_duration_seconds_bucket:rate1m) > 1.5` | High | 
